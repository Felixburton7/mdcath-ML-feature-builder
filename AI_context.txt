=======================================
Project Root Directory Information
=======================================
Working Directory: /home/s_felix/mdcath-processor

Project File Structure:
(Excluding: __pycache__, *.egg-info, .git, .vscode, ./outputs)
---------------------------------------------------------
.
├── AI_context.sh
├── AI_context.txt
├── config
│   └── default_config.yaml
├── LICENSE
├── main.py
├── pyproject.toml
├── README.md
├── requirements.txt
├── scripts
│   └── check_environment.py
├── src
│   └── mdcath
│       ├── cli.py
│       ├── config.py
│       ├── exceptions.py
│       ├── features
│       │   ├── builder.py
│       │   └── __init__.py
│       ├── __init__.py
│       ├── io
│       │   ├── hdf5_reader.py
│       │   ├── __init__.py
│       │   └── writers.py
│       ├── metrics
│       │   ├── __init__.py
│       │   └── rmsf.py
│       ├── pipeline
│       │   ├── executor.py
│       │   └── __init__.py
│       ├── structure
│       │   ├── frame_extractor.py
│       │   ├── __init__.py
│       │   ├── pdb_processor.py
│       │   └── properties.py
│       ├── utils
│       │   ├── __init__.py
│       │   ├── logging_config.py
│       │   └── parallel.py
│       ├── visualize
│       │   ├── __init__.py
│       │   └── plots.py
│       └── voxel
│           ├── aposteriori_wrapper.py
│           └── __init__.py
└── tests
    ├── features
    │   ├── __init__.py
    │   └── test_builder.py
    ├── __init__.py
    ├── io
    │   ├── __init__.py
    │   └── test_hdf5_reader.py
    ├── metrics
    │   ├── __init__.py
    │   └── test_rmsf.py
    ├── pipeline
    │   ├── __init__.py
    │   └── test_executor.py
    └── structure
        ├── __init__.py
        ├── test_pdb_processor.py
        └── test_properties.py

18 directories, 45 files


=======================================
Contents of Code, Config, and Metadata Files
(Ignoring binary files and excluded directories)
=======================================

===== FILE: ./config/default_config.yaml =====
# Default configuration for the mdCATH Processor pipeline

# Input Data Configuration
input:
  mdcath_folder: "/mnt/datasets/MD_CATH/data" # *** IMPORTANT: Set this path ***
  domain_ids: ["1a02F00", "16pkA02"]    # List of specific domain IDs to process, or null/empty to process all found HDF5 files in mdcath_folder
                      # Example: ["1a02F00", "16pkA02"]

# Output Configuration
output:
  base_dir: "./outputs" # Root directory for all generated files
  flatten_rmsf_dirs: true # Controls RMSF output structure (True flattens replica/temp folders)

# Processing Parameters
processing:
  # Frame selection from simulation trajectories
  frame_selection:
    method: "rmsd"    # Options: 'regular', 'rmsd', 'gyration', 'random', 'last'
    num_frames: 1     # Number of frames to extract per domain/temperature/replica
    cluster_method: "kmeans" # Method for RMSD-based selection ('kmeans' currently supported)

  # PDB Cleaning options
  pdb_cleaning:
    # Tool selection ('pdbutils' preferred, 'fallback' uses internal logic)
    tool: "pdbutils" # Options: 'pdbutils', 'fallback'
    replace_chain_0_with_A: true
    fix_atom_numbering: true
    correct_unusual_residue_names: true # HSD/HSE/HSP -> HIS
    stop_after_ter: true # Stop reading PDB after the first TER record
    remove_hydrogens: false # Keep hydrogen atoms by default
    remove_solvent_ions: true # Remove common water (TIP, HOH) and ions (SOD, CLA), etc.

  # Structure Properties Calculation (DSSP/SASA etc.)
  properties_calculation:
    # External DSSP executable name (searched in PATH)
    dssp_executable: "dssp" # Can be 'dssp' or 'mkdssp'
    # Fallback method if DSSP executable fails/not found ('biopython' uses ShrakeRupley)
    fallback_method: "biopython"
    # SASA threshold for Core/Exterior classification using Biopython fallback (Å²)
    # Note: The PropertiesCalculator now uses a relative ASA threshold of 0.20 by default
    sasa_threshold_fallback_absolute: 20.0 # Threshold for absolute SASA only if relative calculation fails completely

  # Feature Building options
  feature_building:
    # If true, adds log-transformed RMSF column (e.g., rmsf_log)
    add_rmsf_log: true

  # Voxelization using Aposteriori
  voxelization:
    enabled: true # Set to false to skip voxelization step
    # Path to aposteriori's 'make-frame-dataset' executable (if not in PATH)
    aposteriori_executable: null # Default: search in PATH
    # Parameters for 'make-frame-dataset'
    frame_edge_length: 12.0  # Angstroms
    voxels_per_side: 21
    atom_encoder: "CNOCBCA"  # Options: CNO, CNOCB, CNOCACB
    encode_cb: true
    compression_gzip: true
    voxelise_all_states: false # For NMR structures
    output_name: "mdcath_voxelized" # Base name for the output HDF5 file

# Performance Configuration
performance:
  # Number of parallel processes (0 uses os.cpu_count() - 2, 1 disables parallelism)
  num_cores: 0
  # Optional: Batch size for processing domains in memory-intensive steps (e.g., feature building)
  # null means process domain-by-domain or let the step decide its internal logic.
  batch_size: null # Example: 50

# Logging Configuration
logging:
  # Overall level for logs written to file
  log_level_file: "DEBUG"
  # Overall level for logs printed to console
  log_level_console: "INFO"
  # Path to the log file (relative to output.base_dir/logs)
  log_filename: "pipeline.log"
  # Show tqdm progress bars
  show_progress_bars: true

# Visualization Configuration
visualization:
  enabled: true # Set to false to skip generating plots
  dpi: 300 # Resolution for saved figures
  # Color palette (see seaborn/matplotlib documentation)
  palette: "colorblind" # e.g., "viridis", "colorblind", "deep", "muted"
  # Histogram binning strategy ('auto', or an integer number of bins)
  histogram_bins: 50


--- End of File: ./config/default_config.yaml ---

===== FILE: ./tests/structure/test_pdb_processor.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/structure/test_properties.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/structure/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/io/test_hdf5_reader.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/io/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/features/test_builder.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/features/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/pipeline/test_executor.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/pipeline/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/metrics/test_rmsf.py (Skipped - Detected as non-text) =====

===== FILE: ./tests/metrics/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./LICENSE =====
MIT License

Copyright (c) 2025 [Your Name or Organization]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--- End of File: ./LICENSE ---

===== FILE: ./src/mdcath/structure/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/structure/frame_extractor.py =====
"""
Module for extracting specific simulation frames as PDB files.
"""
import os
import logging
import random
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from mdcath.io.writers import save_string
try:
    from sklearn.cluster import KMeans # Import here if using K-Means method
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.info("scikit-learn not found. KMeans frame selection method will not be available.")


# Helper function to format PDB ATOM/HETATM line (ensure consistency)
def format_pdb_line(record_type, atom_num, atom_name, alt_loc, res_name, chain_id, res_num, icode, x, y, z, occupancy, temp_factor, element):
    """Formats a PDB ATOM/HETATM line string with proper spacing."""
    atom_name_padded = f"{atom_name:<4}"
    if len(atom_name_padded) > 4 : atom_name_padded = atom_name_padded[:4] # Ensure max 4 chars

    # Ensure res_name is string and pad
    res_name_str = str(res_name if res_name is not None else "UNK")
    res_name_padded = f"{res_name_str:>3}" # Right-justify resname

    # Ensure element symbol is string and formatted
    element_str = str(element if element is not None else "")
    element_padded = f"{element_str:>2}" # Right align 2 chars

    # Ensure numeric fields are correctly formatted
    occupancy_val = float(occupancy) if occupancy else 0.0
    temp_factor_val = float(temp_factor) if temp_factor else 0.0


    line = (
        f"{record_type:<6}"              # ATOM or HETATM
        f"{int(atom_num):>5} "           # Atom serial number
        f"{atom_name_padded:4}"          # Atom name
        f"{str(alt_loc):1}"              # Alt loc indicator
        f"{res_name_padded:3} "          # Residue name
        f"{str(chain_id):1}"             # Chain identifier
        f"{int(res_num):>4}"            # Residue sequence number
        f"{str(icode):1}"                # Code for insertion of residues
        f"   "                          # 3 spaces separator
        f"{x:8.3f}"                      # X coordinate
        f"{y:8.3f}"                      # Y coordinate
        f"{z:8.3f}"                      # Z coordinate
        f"{occupancy_val:6.2f}"          # Occupancy
        f"{temp_factor_val:6.2f}      "   # Temperature factor + spaces
        f"{element_padded:>2}  "         # Element symbol + charge placeholder space
        "\n"
    )
    return line

def select_frame_indices(num_available_frames: int,
                         num_frames_to_select: int,
                         method: str,
                         cluster_method: str = 'kmeans',
                         rmsd_data: Optional[np.ndarray] = None,
                         gyration_data: Optional[np.ndarray] = None) -> List[int]:
    """
    Selects frame indices based on the specified method.

    Args:
        num_available_frames (int): Total frames available in the trajectory.
        num_frames_to_select (int): Number of frames desired.
        method (str): Selection method ('regular', 'last', 'random', 'rmsd', 'gyration').
        cluster_method (str): Clustering method if method='rmsd'.
        rmsd_data (Optional[np.ndarray]): RMSD values for each frame (required for 'rmsd').
        gyration_data (Optional[np.ndarray]): Gyration radius values (required for 'gyration').

    Returns:
        List[int]: A list of selected frame indices.
    """
    if num_available_frames <= 0: return []
    if num_frames_to_select <= 0: return []

    # Ensure num_frames_to_select is not greater than available
    num_frames_to_select = min(num_frames_to_select, num_available_frames)

    indices = []
    if method == 'last':
        start_index = max(0, num_available_frames - num_frames_to_select)
        indices = list(range(start_index, num_available_frames))
        if num_frames_to_select > 1:
             logging.debug(f"Method 'last' selected. Selecting last {len(indices)} frames.")

    elif method == 'regular':
        # Generate evenly spaced indices including start and end points if possible
        indices = np.linspace(0, num_available_frames - 1, num_frames_to_select, dtype=int).tolist()

    elif method == 'random':
         indices = random.sample(range(num_available_frames), num_frames_to_select)

    elif method == 'gyration':
        if gyration_data is None or len(gyration_data) != num_available_frames:
            logging.warning("Gyration data missing or invalid for 'gyration' selection. Falling back to 'regular'.")
            return select_frame_indices(num_available_frames, num_frames_to_select, 'regular')
        # Select frames representing min, max, and intermediate gyration values
        sorted_indices = np.argsort(gyration_data)
        indices_of_indices = np.linspace(0, num_available_frames - 1, num_frames_to_select, dtype=int)
        indices = [sorted_indices[i] for i in indices_of_indices]

    elif method == 'rmsd':
        if rmsd_data is None or len(rmsd_data) != num_available_frames:
            logging.warning("RMSD data missing or invalid for 'rmsd' selection. Falling back to 'regular'.")
            return select_frame_indices(num_available_frames, num_frames_to_select, 'regular')

        if cluster_method == 'kmeans' and SKLEARN_AVAILABLE and num_available_frames >= num_frames_to_select > 0:
            try:
                rmsd_reshaped = rmsd_data.reshape(-1, 1)
                n_clusters = num_frames_to_select

                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
                kmeans.fit(rmsd_reshaped)

                indices = []
                # Find frame closest to each centroid
                for i in range(n_clusters):
                     center = kmeans.cluster_centers_[i]
                     cluster_member_indices = np.where(kmeans.labels_ == i)[0]
                     if len(cluster_member_indices) > 0:
                         distances = np.abs(rmsd_reshaped[cluster_member_indices] - center)
                         closest_point_in_cluster_idx = np.argmin(distances)
                         original_frame_index = cluster_member_indices[closest_point_in_cluster_idx]
                         indices.append(original_frame_index)
                     else:
                         logging.warning(f"KMeans cluster {i} has no points for RMSD selection.")
                # If fewer frames selected than requested (e.g. due to identical RMSD values),
                # potentially top up with random/regular samples? For now, return what's found.

            except Exception as e:
                logging.error(f"KMeans clustering for RMSD selection failed: {e}. Falling back to 'regular'.", exc_info=True)
                return select_frame_indices(num_available_frames, num_frames_to_select, 'regular')
        else:
            if not SKLEARN_AVAILABLE:
                logging.warning("scikit-learn not installed. Cannot use 'kmeans' for RMSD selection.")
            else:
                logging.warning(f"Insufficient data or unsupported cluster method '{cluster_method}' for RMSD selection.")
            logging.info("Falling back to 'regular' frame selection.")
            return select_frame_indices(num_available_frames, num_frames_to_select, 'regular')
    else:
        logging.warning(f"Unknown frame selection method '{method}'. Falling back to 'regular'.")
        return select_frame_indices(num_available_frames, num_frames_to_select, 'regular')

    # Return unique, sorted indices
    return sorted(list(set(indices)))


def extract_and_save_frames(domain_id: str,
                            coords_all_frames: np.ndarray, # Shape (F, N, 3)
                            cleaned_pdb_template_path: str,
                            output_dir: str, # Base output dir (e.g., ./outputs)
                            config: Dict[str, Any],
                            rmsd_data: Optional[np.ndarray] = None,
                            gyration_data: Optional[np.ndarray] = None,
                            temperature: Optional[str] = None, # For naming output folder
                            replica: Optional[str] = None      # For naming output folder
                            ) -> bool:
    """
    Extracts specified frames from coordinate data and saves them as PDB files.

    Args:
        domain_id (str): Domain identifier.
        coords_all_frames (np.ndarray): Coordinate data for all frames (F, N, 3).
        cleaned_pdb_template_path (str): Path to the cleaned static PDB file to use as template.
        output_dir (str): Base output directory (e.g., './outputs').
        config (Dict[str, Any]): Pipeline configuration dictionary.
        rmsd_data (Optional[np.ndarray]): RMSD data for selection methods.
        gyration_data (Optional[np.ndarray]): Gyration data for selection methods.
        temperature (Optional[str]): Temperature identifier for output path.
        replica (Optional[str]): Replica identifier for output path.


    Returns:
        bool: True if at least one frame was successfully extracted and saved, False otherwise.
    """
    frame_cfg = config.get('processing', {}).get('frame_selection', {})
    num_frames_to_select = frame_cfg.get('num_frames', 1)
    method = frame_cfg.get('method', 'rmsd')
    cluster_method = frame_cfg.get('cluster_method', 'kmeans')

    if coords_all_frames is None or coords_all_frames.ndim != 3 or coords_all_frames.shape[0] == 0:
        logging.warning(f"No valid multi-frame coordinate data provided for {domain_id}. Skipping frame extraction.")
        return False

    num_available_frames = coords_all_frames.shape[0]
    num_atoms = coords_all_frames.shape[1]

    if not os.path.exists(cleaned_pdb_template_path):
        logging.error(f"Cleaned PDB template not found: {cleaned_pdb_template_path}. Cannot extract frames for {domain_id}.")
        return False

    # Read the template PDB
    try:
        with open(cleaned_pdb_template_path, 'r') as f:
            template_lines = f.readlines()
    except Exception as e:
        logging.error(f"Failed to read PDB template {cleaned_pdb_template_path}: {e}", exc_info=True)
        return False

    # Select frame indices
    selected_indices = select_frame_indices(num_available_frames, num_frames_to_select, method,
                                            cluster_method, rmsd_data, gyration_data)

    if not selected_indices:
        logging.warning(f"No frames selected for extraction for domain {domain_id}.")
        return False

    logging.info(f"Attempting to extract frames {selected_indices} for {domain_id} (Temp: {temperature}, Rep: {replica})")

    # --- Prepare mapping from template atom index to coordinate index ---
    # Assumption: Atom order in cleaned static PDB matches order in HDF5 coords.
    template_atom_lines_info = []
    for line in template_lines:
         if line.startswith(("ATOM", "HETATM")):
              # Store minimal info needed for line regeneration
              try:
                    info = {
                        "record_type": line[0:6].strip(),
                        "atom_num": int(line[6:11].strip()),
                        "atom_name": line[12:16].strip(),
                        "alt_loc": line[16:17].strip(),
                        "res_name": line[17:20].strip(),
                        "chain_id": line[21:22].strip(),
                        "res_num": int(line[22:26].strip()),
                        "icode": line[26:27].strip(),
                        "occupancy": line[54:60].strip(),
                        "temp_factor": line[60:66].strip(),
                        "element": line[76:78].strip() if len(line) >= 78 else line[12:14].strip()[0:1].upper(), # Guess element
                        "original_line": line # Keep original for non-atom lines
                    }
                    template_atom_lines_info.append(info)
              except ValueError as e:
                   logging.warning(f"Could not parse atom line in template {cleaned_pdb_template_path}: {line.strip()} - {e}")
                   template_atom_lines_info.append({"original_line": line}) # Keep line as is


    num_template_atoms = len([info for info in template_atom_lines_info if "record_type" in info])

    if num_template_atoms != num_atoms:
        logging.warning(f"Atom count mismatch for {domain_id}: Template PDB ({num_template_atoms}) vs Coordinates ({num_atoms}). "
                        f"Using minimum count ({min(num_template_atoms, num_atoms)}) for frame extraction.")
        num_atoms_to_process = min(num_template_atoms, num_atoms)
    else:
        num_atoms_to_process = num_atoms

    # --- Extract and save each selected frame ---
    saved_count = 0
    non_atom_header = [info["original_line"] for info in template_atom_lines_info if "record_type" not in info and not info["original_line"].startswith("END")]
    non_atom_footer = [info["original_line"] for info in template_atom_lines_info if "record_type" not in info and info["original_line"].startswith("END")]
    if not non_atom_footer: non_atom_footer = ["END\n"] # Ensure END record

    atom_template_info_to_use = [info for info in template_atom_lines_info if "record_type" in info][:num_atoms_to_process]

    for frame_idx in selected_indices:
        try:
            frame_coords = coords_all_frames[frame_idx, :num_atoms_to_process, :] # Use potentially truncated coords
            new_pdb_lines = list(non_atom_header) # Start with header lines

            for i, atom_info in enumerate(atom_template_info_to_use):
                x, y, z = frame_coords[i]
                new_line = format_pdb_line(
                    atom_info["record_type"], atom_info["atom_num"], atom_info["atom_name"],
                    atom_info["alt_loc"], atom_info["res_name"], atom_info["chain_id"],
                    atom_info["res_num"], atom_info["icode"], x, y, z,
                    atom_info["occupancy"], atom_info["temp_factor"], atom_info["element"]
                )
                new_pdb_lines.append(new_line)

            new_pdb_lines.extend(non_atom_footer) # Add footer lines

            # Define output path
            frame_output_dir = os.path.join(output_dir, "frames")
            if temperature is not None and replica is not None:
                 frame_output_dir = os.path.join(frame_output_dir, str(temperature), f"replica_{replica}")
            elif temperature is not None:
                 frame_output_dir = os.path.join(frame_output_dir, str(temperature))
            # Handle 'average' path if necessary

            output_filename = f"{domain_id}_frame_{frame_idx}.pdb"
            output_path = os.path.join(frame_output_dir, output_filename)

            # Save the new PDB file
            save_string("".join(new_pdb_lines), output_path)
            saved_count += 1
        except Exception as e:
            logging.error(f"Failed to process or save frame {frame_idx} for {domain_id}: {e}", exc_info=True)

    if saved_count > 0:
         logging.info(f"Successfully saved {saved_count} frames for {domain_id} (Temp: {temperature}, Rep: {replica})")
    else:
         logging.warning(f"Failed to save any frames for {domain_id} (Temp: {temperature}, Rep: {replica})")

    return saved_count > 0


--- End of File: ./src/mdcath/structure/frame_extractor.py ---

===== FILE: ./src/mdcath/structure/properties.py =====
"""
Module for calculating structural properties like DSSP, SASA, Core/Exterior.
"""
import os
import logging
import subprocess
import tempfile
import shutil
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, Tuple
from functools import lru_cache

# Attempt to import Biopython
try:
    from Bio.PDB import PDBParser, DSSP, HSExposureCB, NeighborSearch, ResidueDepth
    from Bio.PDB.SASA import ShrakeRupley
    BIOPYTHON_AVAILABLE = True
except ImportError:
    BIOPYTHON_AVAILABLE = False
    logging.error("BioPython library not found. Structural property calculation cannot proceed.")
    # Define dummy classes/functions if needed for program structure but operations will fail
    class PDBParser: pass
    class DSSP: pass
    class ShrakeRupley: pass

# Max size for LRU cache
CACHE_MAX_SIZE = 256

# Approximate maximum ASA values for normalization (from Tien et al. 2013 PLoS ONE, or similar source)
# Use 3-letter codes. Added default for unknown residues.
PDB_MAX_ASA = {
    'ALA': 129.0, 'ARG': 274.0, 'ASN': 195.0, 'ASP': 193.0, 'CYS': 167.0,
    'GLN': 225.0, 'GLU': 223.0, 'GLY': 104.0, 'HIS': 224.0, 'ILE': 197.0,
    'LEU': 201.0, 'LYS': 236.0, 'MET': 224.0, 'PHE': 240.0, 'PRO': 159.0,
    'SER': 155.0, 'THR': 172.0, 'TRP': 285.0, 'TYR': 263.0, 'VAL': 174.0,
    'UNK': 197.0 # Default fallback (e.g., average of ILE/LEU)
}


class PropertiesCalculatorError(Exception):
    """Custom exception for PropertiesCalculator errors."""
    pass

class PropertiesCalculator:
    """
    Calculates structural properties from cleaned PDB files.
    Uses DSSP executable and BioPython. Caches results.
    """
    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the calculator with configuration.

        Args:
            config (Dict[str, Any]): The 'properties_calculation' section of the main config.

        Raises:
            ImportError: If BioPython is not installed.
        """
        if not BIOPYTHON_AVAILABLE:
            raise ImportError("BioPython is required for PropertiesCalculator but not found.")

        self.config = config
        self.dssp_executable = self._find_dssp()
        self.fallback_method = self.config.get('fallback_method', 'biopython') # Currently only 'biopython' fallback supported
        # Use relative ASA threshold by default
        self.relative_asa_core_threshold = self.config.get('relative_asa_core_threshold', 0.20)


        # Initialize BioPython parser and SASA calculator once
        self.parser = PDBParser(QUIET=True)
        self.sr = ShrakeRupley()


    def _find_dssp(self) -> Optional[str]:
        """Finds the DSSP executable path (dssp or mkdssp)."""
        configured_exec = self.config.get('dssp_executable', 'dssp')
        execs_to_try = [configured_exec]
        if configured_exec == 'dssp':
            execs_to_try.append('mkdssp')
        elif configured_exec == 'mkdssp':
             execs_to_try.append('dssp') # Try the other one if primary fails

        for exe in execs_to_try:
            path = shutil.which(exe)
            if path:
                logging.info(f"Found DSSP executable '{exe}' at: {path}")
                return path

        logging.warning(f"DSSP executable ('{configured_exec}' or its alternative) not found in PATH. "
                       f"DSSP calculation will use fallback methods if possible.")
        return None

        # Use file path as cache key. Ensure path is absolute/canonical if needed.
    @lru_cache(maxsize=CACHE_MAX_SIZE)
    def _run_dssp(self, cleaned_pdb_path: str) -> Optional[Dict[Tuple, Tuple]]:
        """Runs DSSP executable via BioPython wrapper and caches results."""
        if not self.dssp_executable:
            logging.debug(f"Skipping DSSP run: No DSSP executable found.")
            return None

        logging.debug(f"Running DSSP on {os.path.basename(cleaned_pdb_path)}...")
        try:
            structure = self.parser.get_structure(os.path.basename(cleaned_pdb_path), cleaned_pdb_path)
            model = structure[0]

            # DSSP execution
            dssp_results = DSSP(model, cleaned_pdb_path, dssp=self.dssp_executable)

            # DSSP object itself acts like a dictionary
            if not dssp_results or len(dssp_results) == 0:
                 logging.warning(f"DSSP returned empty results for {cleaned_pdb_path}.")
                 return None

            # Convert to a standard dict to ensure consistency before returning/caching
            # If iteration below fails, the issue might be here or how DSSP object behaves
            standardized_results = {key: dssp_results[key] for key in dssp_results.keys()}

            logging.debug(f"Successfully ran DSSP on {cleaned_pdb_path}. Found {len(standardized_results)} residues.")
            return standardized_results

        except FileNotFoundError:
             logging.error(f"DSSP executable '{self.dssp_executable}' not found during execution attempt.", exc_info=True)
             self.dssp_executable = None
             return None
        except Exception as e:
            # *** CORRECTED LOGGING CALL ***
            logging.log(logging.WARNING, f"DSSP execution or parsing failed for {cleaned_pdb_path}: {e}", exc_info=True)
            return None

    # Use file path as cache key. Consider if config changes should invalidate cache.
        # Use file path as cache key. Consider if config changes should invalidate cache.
    @lru_cache(maxsize=CACHE_MAX_SIZE)
    def calculate_properties(self, cleaned_pdb_path: str) -> Optional[pd.DataFrame]:
        """
        Calculates DSSP, SASA, Core/Exterior for a cleaned PDB file.
        Results are cached based on the pdb path.

        Args:
            cleaned_pdb_path (str): Path to the cleaned PDB file.

        Returns:
            Optional[pd.DataFrame]: DataFrame with columns
                ['resid', 'chain', 'dssp', 'relative_accessibility', 'core_exterior', 'phi', 'psi'],
                or None if calculations fail completely.

        Raises:
            PropertiesCalculatorError: If file not found or critical calculation fails.
        """
        if not os.path.exists(cleaned_pdb_path):
            raise PropertiesCalculatorError(f"PDB file not found for property calculation: {cleaned_pdb_path}")

        domain_id = os.path.splitext(os.path.basename(cleaned_pdb_path))[0]
        logging.debug(f"Calculating properties for {domain_id} from {cleaned_pdb_path}")

        # Attempt to get DSSP results (from cache or by running)
        dssp_results_dict = self._run_dssp(cleaned_pdb_path) # Returns a dict or None

        # --- Prepare structure for SASA and fallback ---
        try:
            structure = self.parser.get_structure(domain_id, cleaned_pdb_path)
            model = structure[0]
        except Exception as e:
             logging.error(f"Failed to parse PDB {cleaned_pdb_path} with BioPython: {e}", exc_info=True)
             raise PropertiesCalculatorError(f"BioPython parsing failed for {domain_id}") from e

        # --- Process residues ---
        property_list = []
        processed_keys = set() # Track DSSP keys to add fallback info later

        # 1. Process residues found by DSSP
        if dssp_results_dict:
            # *** CORRECTED ITERATION ***
            for key in dssp_results_dict.keys(): # Iterate over keys
                dssp_data = dssp_results_dict[key] # Get data for the key
                try:
                    chain_id = key[0]
                    res_id_tuple = key[1]
                    # Skip HETATM/water records potentially included by DSSP key format
                    if res_id_tuple[0] != ' ':
                         logging.debug(f"Skipping non-standard residue key from DSSP: {key}")
                         continue
                    res_num = res_id_tuple[1]

                    ss_code = dssp_data[2] if len(dssp_data) > 2 and dssp_data[2] not in [' ', '-'] else 'C'
                    phi = dssp_data[4] if len(dssp_data) > 4 and isinstance(dssp_data[4], (int, float)) else 0.0
                    psi = dssp_data[5] if len(dssp_data) > 5 and isinstance(dssp_data[5], (int, float)) else 0.0

                    property_list.append({
                        "key": key, "resid": res_num, "chain": chain_id,
                        "dssp": ss_code, "phi": phi, "psi": psi,
                        "relative_accessibility": np.nan, # Placeholder
                        "core_exterior": "unknown"      # Placeholder
                    })
                    processed_keys.add(key)
                except (IndexError, TypeError, KeyError) as e:
                     logging.warning(f"Could not parse DSSP data for key {key} in {domain_id}: {e}")

        # 2. Calculate SASA using ShrakeRupley for all standard residues in the parsed structure
        sasa_data = {} # Store {key: relative_asa}
        try:
            self.sr.compute(model, level="R") # Compute SASA at residue level
            for chain in model:
                for residue in chain:
                    key = (chain.id, residue.get_id())
                    if key[1][0] == ' ': # Standard residue (' ' flag)
                        resname = residue.get_resname()
                        # Handle non-standard AAs, ensure 3 letters before MAX_ASA lookup
                        if len(resname) != 3:
                            logging.debug(f"Skipping SASA for non-3-letter residue {resname} in {domain_id} {key}")
                            continue
                        if resname not in PDB_MAX_ASA:
                             logging.debug(f"Residue {resname} in {domain_id} {key} not in PDB_MAX_ASA map, using UNK default.")
                             resname_lookup = 'UNK'
                        else:
                             resname_lookup = resname

                        sasa_abs = residue.sasa if hasattr(residue, 'sasa') else 0.0
                        max_asa = PDB_MAX_ASA.get(resname_lookup, PDB_MAX_ASA['UNK'])
                        relative_accessibility = min(1.0, sasa_abs / max_asa) if max_asa > 0 else 0.0
                        sasa_data[key] = relative_accessibility

                        # Add defaults if residue was missed by DSSP run (e.g., if DSSP failed entirely)
                        if key not in processed_keys:
                             logging.debug(f"Adding fallback property data for residue {key} missed by DSSP in {domain_id}")
                             property_list.append({
                                 "key": key, "resid": key[1][1], "chain": key[0],
                                 "dssp": 'C', "phi": 0.0, "psi": 0.0,
                                 "relative_accessibility": np.nan, # Placeholder
                                 "core_exterior": "unknown"      # Placeholder
                             })
                             processed_keys.add(key) # Mark as processed

        except Exception as e:
             logging.error(f"SASA calculation failed for {domain_id}: {e}", exc_info=True)
             # Continue, relative_accessibility will be filled with default later

        # 3. Combine data and finalize
        if not property_list:
            logging.error(f"No property data could be generated for {domain_id}.")
            return None

        final_df = pd.DataFrame(property_list)

        # Fill in SASA and Core/Exterior using the calculated sasa_data map
        final_df['relative_accessibility'] = final_df['key'].map(sasa_data)
        final_df['relative_accessibility'].fillna(0.5, inplace=True) # Use default 0.5 if SASA mapping failed

        # Calculate core_exterior based on the relative accessibility
        final_df['core_exterior'] = np.where(
             final_df['relative_accessibility'] > self.relative_asa_core_threshold,
             'exterior',
             'core'
        )

        # Clean up and type conversion
        final_df = final_df.drop(columns=['key'])
        try:
            # Ensure correct types before returning
            final_df['resid'] = pd.to_numeric(final_df['resid'])
            final_df['relative_accessibility'] = pd.to_numeric(final_df['relative_accessibility'])
            final_df['phi'] = pd.to_numeric(final_df['phi'])
            final_df['psi'] = pd.to_numeric(final_df['psi'])
            final_df['chain'] = final_df['chain'].astype(str)
            final_df['dssp'] = final_df['dssp'].astype(str)
            final_df['core_exterior'] = final_df['core_exterior'].astype(str)

        except Exception as e:
            logging.error(f"Type conversion failed for property DataFrame of {domain_id}: {e}", exc_info=True)
            return None

        final_df = final_df.sort_values(by=['chain', 'resid']).reset_index(drop=True)
        logging.debug(f"Generated properties table with {len(final_df)} residues for {domain_id}")

        # Final check for NaNs in critical columns
        if final_df[['resid', 'chain', 'dssp', 'core_exterior']].isnull().any().any():
             logging.warning(f"Found null values in critical property columns for {domain_id}. Check processing.")

        return final_df


--- End of File: ./src/mdcath/structure/properties.py ---

===== FILE: ./src/mdcath/structure/pdb_processor.py =====
"""
Module for cleaning and processing PDB files.
Uses pdbUtils if available, otherwise provides a fallback.
"""
import os
import logging
import subprocess
import tempfile
from typing import Dict, Any

# Attempt to import pdbUtils
try:
    from pdbUtils import pdbUtils
    PDBUTILS_AVAILABLE = True
    logging.debug("pdbUtils library found. Will be used for PDB processing if configured.")
except ImportError:
    PDBUTILS_AVAILABLE = False
    logging.debug("pdbUtils library not found. Fallback method will be used if pdbUtils is configured.")

class PDBProcessorError(Exception):
    """Custom exception for PDBProcessor errors."""
    pass

class PDBProcessor:
    """
    Cleans and processes PDB files according to configuration.
    """
    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the PDBProcessor with cleaning configuration.

        Args:
            config (Dict[str, Any]): The 'pdb_cleaning' section of the main config.
        """
        self.config = config
        self._setup_tool()

    def _setup_tool(self):
        """Determines which cleaning tool to use based on config and availability."""
        configured_tool = self.config.get("tool", "pdbutils")
        if configured_tool == "pdbutils":
            if PDBUTILS_AVAILABLE:
                self.use_pdbutils = True
                logging.info("Using pdbUtils for PDB cleaning.")
            else:
                self.use_pdbutils = False
                logging.warning("pdbUtils configured but not found. Switching to fallback PDB cleaning.")
        elif configured_tool == "fallback":
            self.use_pdbutils = False
            logging.info("Using fallback method for PDB cleaning.")
        else:
            logging.warning(f"Unknown PDB cleaning tool '{configured_tool}' specified in config. Using fallback.")
            self.use_pdbutils = False

    def clean_pdb_string(self, pdb_string: str, output_pdb_path: str) -> bool:
        """
        Cleans a PDB string and saves the result to a file.

        Args:
            pdb_string (str): The PDB content as a single string.
            output_pdb_path (str): Path where the cleaned PDB file will be saved.

        Returns:
            bool: True if cleaning was successful, False otherwise.
        """
        # Write the input string to a temporary file for processing
        # Use NamedTemporaryFile for automatic cleanup
        try:
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=".pdb", encoding='utf-8') as temp_in:
                temp_in_path = temp_in.name
                temp_in.write(pdb_string)
            # Now process the temporary file
            success = self.clean_pdb_file(temp_in_path, output_pdb_path)
        except Exception as e:
             logging.error(f"Error creating or writing temporary input PDB: {e}", exc_info=True)
             success = False
        finally:
            # Ensure temporary input file is removed
            if 'temp_in_path' in locals() and os.path.exists(temp_in_path):
                try:
                    os.remove(temp_in_path)
                except OSError as e:
                    logging.warning(f"Could not remove temporary input file {temp_in_path}: {e}")
        return success


    def clean_pdb_file(self, input_pdb_path: str, output_pdb_path: str) -> bool:
        """
        Cleans a PDB file using the configured method.

        Args:
            input_pdb_path (str): Path to the input PDB file.
            output_pdb_path (str): Path where the cleaned PDB file will be saved.

        Returns:
            bool: True if cleaning was successful, False otherwise.
        """
        if not os.path.exists(input_pdb_path):
            logging.error(f"Input PDB file not found: {input_pdb_path}")
            return False

        logging.debug(f"Cleaning PDB file: {os.path.basename(input_pdb_path)} -> {os.path.basename(output_pdb_path)}")
        try:
            os.makedirs(os.path.dirname(output_pdb_path), exist_ok=True)
            if self.use_pdbutils:
                return self._clean_with_pdbutils(input_pdb_path, output_pdb_path)
            else:
                return self._clean_with_fallback(input_pdb_path, output_pdb_path)
        except Exception as e:
            logging.error(f"Failed to clean PDB {input_pdb_path}: {e}", exc_info=True)
            return False

    def _clean_with_pdbutils(self, input_path: str, output_path: str) -> bool:
        """Cleans PDB using pdbUtils library."""
        try:
            pdb_df = pdbUtils.pdb2df(input_path)
            initial_atoms = len(pdb_df)

            # --- Apply Filters First ---
            # Remove solvent/ions first to potentially simplify TER handling
            if self.config.get("remove_solvent_ions", True):
                 skip_resnames = {"TIP", "HOH", "WAT", "SOD", "CLA", "K", "MG", "ZN", "CA", "CL-", "NA+", "K+"} # More variants
                 if "RES_NAME" in pdb_df.columns:
                      pdb_df = pdb_df[~pdb_df["RES_NAME"].isin(skip_resnames)]
                 # Consider chain removal if needed (e.g., Chain 'W')
                 # if "CHAIN_ID" in pdb_df.columns:
                 #     pdb_df = pdb_df[pdb_df["CHAIN_ID"] != "W"]

            # Remove hydrogens
            if self.config.get("remove_hydrogens", False):
                 # More robust hydrogen check: element or atom name start
                 if "ELEMENT" in pdb_df.columns:
                      pdb_df = pdb_df[pdb_df["ELEMENT"] != "H"]
                 elif "ATOM_NAME" in pdb_df.columns: # Fallback if element missing
                      # Handle names like '1H', '2H', 'HD1', etc.
                      pdb_df = pdb_df[~pdb_df["ATOM_NAME"].str.strip().str.match(r"^[1-9]?[HD]")]
                 else:
                      logging.warning("Cannot remove hydrogens: Neither ELEMENT nor ATOM_NAME column found.")


            # Stop after TER: Apply filter after other removals
            if self.config.get("stop_after_ter", True):
                 # Find the index of the first non-ATOM/HETATM record after initial filtering
                 # This is complex with DataFrames. Simpler approach: Re-parse after saving temporarily?
                 # Let's assume for now that pdbUtils' df2pdb writes a sensible structure
                 # and subsequent reads won't include much beyond the main chain if solvent etc. removed.
                 # A truly strict TER stop might need the fallback line-by-line approach.
                 logging.debug("pdbUtils TER handling: Assuming removal of solvent/ions suffices or df2pdb handles it.")


            # --- Apply Corrections ---
            # Chain ID '0' -> 'A'
            if self.config.get("replace_chain_0_with_A", True) and "CHAIN_ID" in pdb_df.columns:
                pdb_df["CHAIN_ID"] = pdb_df["CHAIN_ID"].replace("0", "A")

            # Correct residue names (HIS variants)
            if self.config.get("correct_unusual_residue_names", True) and "RES_NAME" in pdb_df.columns:
                his_map = {"HSD": "HIS", "HSE": "HIS", "HSP": "HIS"}
                pdb_df["RES_NAME"] = pdb_df["RES_NAME"].replace(his_map)

            # Fix atom numbering (sequential) AFTER all filtering
            if self.config.get("fix_atom_numbering", True) and "ATOM_NUM" in pdb_df.columns:
                 pdb_df["ATOM_NUM"] = range(1, len(pdb_df) + 1)

            # Ensure essential columns exist for df2pdb, add placeholders if missing?
            # Depends on pdbUtils requirements.

            pdbUtils.df2pdb(pdb_df, output_path)
            final_atoms = len(pdb_df)
            logging.info(f"Cleaned PDB (pdbUtils): {os.path.basename(input_path)} ({initial_atoms} atoms -> {final_atoms} atoms)")
            # Post-process: Ensure CRYST1 if needed (e.g., for DSSP)
            self._ensure_cryst1(output_path)
            return True
        except Exception as e:
            logging.error(f"pdbUtils cleaning failed for {input_path}: {e}", exc_info=True)
            return False

    def _clean_with_fallback(self, input_path: str, output_path: str) -> bool:
        """Cleans PDB using basic line-by-line processing."""
        cleaned_lines = []
        atom_counter = 1
        ter_encountered = False
        last_res_info = "" # To format TER card correctly

        # Read config settings
        stop_after_ter = self.config.get("stop_after_ter", True)
        skip_resnames = {"TIP", "HOH", "WAT", "SOD", "CLA", "K", "MG", "ZN", "CA", "CL-", "NA+", "K+"} if self.config.get("remove_solvent_ions", True) else set()
        remove_h = self.config.get("remove_hydrogens", False)
        correct_res = self.config.get("correct_unusual_residue_names", True)
        fix_chain0 = self.config.get("replace_chain_0_with_A", True)
        fix_numbering = self.config.get("fix_atom_numbering", True)
        his_map = {"HSD": "HIS", "HSE": "HIS", "HSP": "HIS"}

        try:
            with open(input_path, 'r') as infile:
                # Check for CRYST1 early, add if missing and needed
                # Note: This simplistic check won't fix a malformed CRYST1
                infile_content = infile.readlines() # Read all lines first for CRYST1 check
                has_cryst1 = any(line.startswith("CRYST1") for line in infile_content)
                if not has_cryst1:
                    cleaned_lines.append("CRYST1    1.000    1.000    1.000  90.00  90.00  90.00 P 1           1\n")
                    logging.debug("Added default CRYST1 record.")

                infile.seek(0) # Rewind not needed as we iterate over infile_content

                for line_num, line in enumerate(infile_content):
                    line = line.rstrip() # Remove trailing whitespace
                    if not line: continue # Skip empty lines

                    # Check TER condition first
                    if stop_after_ter and ter_encountered:
                        if line.startswith("END"):
                            cleaned_lines.append(line + "\n")
                        continue

                    if line.startswith("TER"):
                        ter_atom_idx = atom_counter # Use the *next* atom number
                        # Try to format TER using last residue info
                        # TER card format: TER serialNumber resName chainID resSeq iCode
                        ter_line = f"TER   {ter_atom_idx:>5}      {last_res_info}\n" # last_res_info is "resName chainID resSeq"
                        cleaned_lines.append(ter_line)
                        ter_encountered = True
                        last_res_info = "" # Reset for next chain if any
                        continue

                    if line.startswith("END"):
                         cleaned_lines.append(line + "\n")
                         break

                    if line.startswith(("ATOM", "HETATM")):
                        # --- Parse line carefully ---
                        try:
                            record_type = line[0:6]
                            atom_name = line[12:16] # Keep spacing
                            alt_loc = line[16:17]
                            res_name = line[17:20].strip()
                            chain_id = line[21:22].strip()
                            res_num_str = line[22:26].strip()
                            icode = line[26:27]
                            x_coord_str = line[30:38].strip()
                            y_coord_str = line[38:46].strip()
                            z_coord_str = line[46:54].strip()
                            occupancy_str = line[54:60].strip()
                            temp_factor_str = line[60:66].strip()
                            element = line[76:78].strip() if len(line) >= 78 else ''
                            # charge = line[78:80].strip() if len(line) >= 80 else ''

                            # Basic validation of parsed numbers
                            res_num = int(res_num_str)
                            x_coord = float(x_coord_str)
                            y_coord = float(y_coord_str)
                            z_coord = float(z_coord_str)
                            occupancy = float(occupancy_str) if occupancy_str else 0.0
                            temp_factor = float(temp_factor_str) if temp_factor_str else 0.0

                        except (ValueError, IndexError) as parse_error:
                            logging.warning(f"Skipping malformed ATOM/HETATM line {line_num+1} in {os.path.basename(input_path)}: {parse_error} -> '{line}'")
                            continue

                        # --- Apply Filters/Modifications ---
                        # Element check for Hydrogen removal
                        if not element: element = atom_name.strip()[0:1].upper() # Simple guess if missing
                        if remove_h and (element == 'H' or atom_name.strip().startswith(('H','D','1','2','3'))): # More robust H check
                            continue

                        # Residue name filter (solvent/ions)
                        if res_name in skip_resnames:
                            continue

                        # --- Apply Corrections ---
                        if correct_res and res_name in his_map: res_name = his_map[res_name]
                        if fix_chain0 and chain_id == '0': chain_id = 'A'

                        # --- Format Output ---
                        current_atom_num = atom_counter if fix_numbering else int(line[6:11].strip())
                        formatted_line = format_pdb_line(
                             record_type, current_atom_num, atom_name.strip(), alt_loc.strip(), res_name,
                             chain_id, res_num, icode.strip(), x_coord, y_coord, z_coord,
                             occupancy, temp_factor, element
                        )
                        cleaned_lines.append(formatted_line)
                        last_res_info = f"{res_name:>3} {chain_id:1}{res_num:>4}{icode:1}".strip() # Store for TER card
                        atom_counter += 1
                    elif not ter_encountered: # Keep other lines only before first TER if stopping
                        cleaned_lines.append(line + "\n") # Append original line (e.g., REMARK, CRYST1)

            # Add END if not present and not stopped early by TER break
            if not cleaned_lines or not cleaned_lines[-1].startswith("END"):
                 cleaned_lines.append("END\n")

            # Write cleaned file
            with open(output_path, 'w') as outfile:
                outfile.writelines(cleaned_lines)

            logging.info(f"Cleaned PDB (Fallback): {os.path.basename(input_path)} ({atom_counter-1} atoms written)")
            # Ensure CRYST1 after writing (in case original was missing)
            self._ensure_cryst1(output_path)
            return True

        except Exception as e:
            logging.error(f"Fallback cleaning failed for {input_path}: {e}", exc_info=True)
            return False

    def _ensure_cryst1(self, pdb_path: str):
        """Checks if a PDB file has a CRYST1 record, adds a default if missing."""
        try:
            with open(pdb_path, 'r') as f:
                lines = f.readlines()
            has_cryst1 = any(line.startswith("CRYST1") for line in lines)
            if not has_cryst1:
                logging.debug(f"Adding default CRYST1 record to {pdb_path}")
                default_cryst1 = "CRYST1    1.000    1.000    1.000  90.00  90.00  90.00 P 1           1\n"
                with open(pdb_path, 'w') as f:
                    f.write(default_cryst1)
                    f.writelines(lines)
        except Exception as e:
            logging.warning(f"Could not ensure CRYST1 record for {pdb_path}: {e}")



--- End of File: ./src/mdcath/structure/pdb_processor.py ---

===== FILE: ./src/mdcath/visualize/plots.py =====
# -*- coding: utf-8 -*-
"""
Enhanced module for generating visualizations of processed mdCATH data.
Includes fixes, stylistic improvements, and additional plots.
"""
import glob
import os
import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
from typing import Dict, Optional, Any, List, Tuple

# Specific imports used in functions
from matplotlib.colors import LinearSegmentedColormap, Normalize
# from scipy.stats import kde # Removed as kdeplot is used directly
from scipy.ndimage import gaussian_filter1d
import h5py # Used in voxel info plot
# Import statsmodels now that it's installed (for lowess)
try:
    import statsmodels.api as sm
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    logging.info("statsmodels not found. Plots using LOWESS smoothing will be skipped.")


# Import the constant from the structure properties module
try:
    from ..structure.properties import PDB_MAX_ASA
except ImportError:
    # Define fallback if run standalone or structure changes
    logging.warning("Could not import PDB_MAX_ASA, using fallback.")
    PDB_MAX_ASA = {
        'ALA': 129.0, 'ARG': 274.0, 'ASN': 195.0, 'ASP': 193.0, 'CYS': 167.0,
        'GLN': 225.0, 'GLU': 223.0, 'GLY': 104.0, 'HIS': 224.0, 'ILE': 197.0,
        'LEU': 201.0, 'LYS': 236.0, 'MET': 224.0, 'PHE': 240.0, 'PRO': 159.0,
        'SER': 155.0, 'THR': 172.0, 'TRP': 285.0, 'TYR': 263.0, 'VAL': 174.0,
        'UNK': 197.0
    }


# --- Plotting Configuration ---
DEFAULT_PALETTE = "colorblind"
DEFAULT_DPI = 300
plt.style.use('seaborn-v0_8-whitegrid') # Use a seaborn style globally

# --- Helper Functions ---
def _setup_plot_style(palette_name: Optional[str] = None):
    """Sets consistent Seaborn style and color palette."""
    # Style is set globally above, just handle palette here
    palette = palette_name or DEFAULT_PALETTE
    try:
        current_palette = sns.color_palette(palette)
        # sns.set_palette(current_palette) # Avoid resetting palette globally within function
        logging.debug(f"Using seaborn palette: {palette}")
        return current_palette
    except ValueError:
        logging.warning(f"Invalid palette '{palette}' specified. Using default '{DEFAULT_PALETTE}'.")
        current_palette = sns.color_palette(DEFAULT_PALETTE)
        # sns.set_palette(current_palette)
        return current_palette

def _save_plot(fig, output_dir: str, filename: str, dpi: int):
    """Handles saving the plot with directory creation."""
    try:
        os.makedirs(output_dir, exist_ok=True)
        path = os.path.join(output_dir, filename)
        fig.savefig(path, dpi=dpi, bbox_inches='tight')
        logging.info(f"Saved visualization: {path}")
        plt.close(fig)
    except Exception as e:
        logging.error(f"Failed to save plot {filename}: {e}", exc_info=logging.getLogger().isEnabledFor(logging.DEBUG))
        if 'fig' in locals() and plt.fignum_exists(fig.number):
             plt.close(fig)

# --- Plotting Functions ---

def create_temperature_summary_heatmap(rmsf_data: Dict[str, pd.DataFrame],
                                     output_dir: str,
                                     viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette = _setup_plot_style(viz_config.get('palette')) # Get palette but might not be used directly

    temps = [temp for temp in rmsf_data.keys() if temp.isdigit()]
    if not temps:
        logging.warning("No numeric temperature data available for heatmap")
        return None

    domain_ids = set()
    for temp in temps:
        if temp in rmsf_data and rmsf_data[temp] is not None and not rmsf_data[temp].empty:
             if 'domain_id' in rmsf_data[temp].columns:
                 domain_ids.update(rmsf_data[temp]["domain_id"].unique())
             else:
                 logging.warning(f"Missing 'domain_id' column in RMSF data for temp {temp}")

    if not domain_ids:
        logging.warning("No domain IDs found across temperature data for heatmap.")
        return None

    domain_ids = sorted(list(domain_ids))
    heatmap_data = []
    expected_rmsf_col_pattern = "rmsf_"

    for domain_id in domain_ids:
        domain_data = {"domain_id": domain_id}
        for temp in temps:
            if temp in rmsf_data and rmsf_data[temp] is not None:
                rmsf_col = f"rmsf_{temp}"
                domain_temp_data = rmsf_data[temp][rmsf_data[temp]["domain_id"] == domain_id]

                if not domain_temp_data.empty and rmsf_col in domain_temp_data.columns:
                    domain_data[temp] = domain_temp_data[rmsf_col].mean()
                else:
                    fallback_cols = [c for c in domain_temp_data.columns if c.startswith(expected_rmsf_col_pattern)]
                    if not domain_temp_data.empty and fallback_cols:
                        fallback_col = fallback_cols[0]
                        logging.warning(f"Column '{rmsf_col}' not found for T={temp}, Domain={domain_id}. Using fallback '{fallback_col}'.")
                        domain_data[temp] = domain_temp_data[fallback_col].mean()
                    else:
                        domain_data[temp] = np.nan

        heatmap_data.append(domain_data)

    if not heatmap_data:
        logging.warning("No data aggregated for heatmap")
        return None

    heatmap_df = pd.DataFrame(heatmap_data)
    heatmap_df.columns = [int(c) if isinstance(c, str) and c.isdigit() else c for c in heatmap_df.columns]
    temp_cols = sorted([c for c in heatmap_df.columns if isinstance(c, int)])
    if not temp_cols:
        logging.warning("No numeric temperature columns found for heatmap pivot.")
        return None
    heatmap_pivot = heatmap_df.set_index("domain_id")[temp_cols]

    if heatmap_pivot.empty or heatmap_pivot.isnull().all().all():
         logging.warning("Heatmap pivot table is empty or all NaN.")
         return None

    fig_height = max(8, len(domain_ids) * 0.1)
    fig_width = max(10, len(temp_cols) * 0.8)
    fig, ax = plt.subplots(figsize=(fig_width, fig_height))

    sns.heatmap(heatmap_pivot, annot=False, cmap="viridis", ax=ax, cbar_kws={'label': 'Mean RMSF (nm)'})
    ax.set_title("Average RMSF by Domain and Temperature")
    ax.set_xlabel("Temperature (K)")
    ax.set_ylabel(f"Domains (n={len(domain_ids)})")

    if len(domain_ids) > 50:
        ax.set_yticks([])
    else:
        ax.tick_params(axis='y', labelsize=min(10, 400 / len(domain_ids)))

    ax.text(0.01, 0.01, f"Domains: {len(domain_ids)}\nTemps: {', '.join(map(str, temp_cols))}",
            transform=ax.transAxes, fontsize=8, verticalalignment='bottom',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.tight_layout()
    output_path = os.path.join(vis_dir, "temperature_summary_heatmap.png")
    _save_plot(fig, vis_dir, "temperature_summary_heatmap.png", dpi)
    return output_path


def create_temperature_average_summary(feature_df_average: pd.DataFrame,
                                     output_dir: str,
                                     viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette = _setup_plot_style(viz_config.get('palette'))

    if feature_df_average is None or feature_df_average.empty or 'rmsf_average' not in feature_df_average.columns:
        logging.warning("No temperature average data available for summary plot (expected in feature_df_average)")
        return None

    fig = plt.figure(figsize=(15, 10))
    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[2, 1])

    ax_density = plt.subplot(gs[0, 0])
    domain_stats = feature_df_average.groupby("domain_id")["rmsf_average"].mean().reset_index()
    domain_stats = domain_stats.rename(columns={'rmsf_average': 'mean_domain_rmsf'})

    if domain_stats.empty:
        logging.warning("No domain stats calculated for density plot.")
        plt.close(fig)
        return None

    rmsf_mean_q1, rmsf_mean_q3 = domain_stats["mean_domain_rmsf"].quantile([0.01, 0.99])
    density_plot_data = domain_stats[
        (domain_stats["mean_domain_rmsf"] >= rmsf_mean_q1) & (domain_stats["mean_domain_rmsf"] <= rmsf_mean_q3)
    ]["mean_domain_rmsf"]

    sns.kdeplot(density_plot_data, fill=True, color=palette[0], ax=ax_density, bw_adjust=0.5) # Use palette color
    sns.rugplot(density_plot_data, ax=ax_density, height=0.05, color=palette[1], alpha=0.5) # Use palette color

    quartiles = np.percentile(domain_stats["mean_domain_rmsf"], [25, 50, 75])
    labels = ["Q1", "Median", "Q3"]
    q_colors = ["green", "red", "green"]
    for q, label, color in zip(quartiles, labels, q_colors):
        ax_density.axvline(q, color=color, linestyle="--", alpha=0.7, linewidth=1.5)
        ax_density.text(q, ax_density.get_ylim()[1] * 0.9, f"{label}: {q:.4f}",
                     ha="center", va="top", fontsize=9,
                     bbox=dict(facecolor="white", alpha=0.7, boxstyle="round"))

    stats_text = (
        f"Mean Domain Avg RMSF: {domain_stats['mean_domain_rmsf'].mean():.4f} nm\n"
        f"Std Dev: {domain_stats['mean_domain_rmsf'].std():.4f} nm\n"
        f"Range: {domain_stats['mean_domain_rmsf'].min():.4f} - {domain_stats['mean_domain_rmsf'].max():.4f} nm\n"
        f"Number of domains: {len(domain_stats)}"
    )
    ax_density.text(0.02, 0.98, stats_text, transform=ax_density.transAxes,
                 fontsize=10, va="top", ha="left",
                 bbox=dict(facecolor="white", alpha=0.7, boxstyle="round"))
    ax_density.set_title("Distribution of Mean RMSF Across Domains (Avg Temp)", fontsize=14)
    ax_density.set_xlabel("Mean RMSF per Domain (nm)", fontsize=12)
    ax_density.set_ylabel("Density", fontsize=12)
    ax_density.set_xlim(left=max(0, rmsf_mean_q1 * 0.9), right=rmsf_mean_q3 * 1.1)

    ax_box = plt.subplot(gs[0, 1])
    rmsf_all_q1, rmsf_all_q3 = feature_df_average["rmsf_average"].quantile([0.01, 0.99])
    boxplot_data = feature_df_average[
        (feature_df_average["rmsf_average"] >= rmsf_all_q1) & (feature_df_average["rmsf_average"] <= rmsf_all_q3)
    ]
    sns.boxplot(y="rmsf_average", data=boxplot_data, ax=ax_box, color=palette[0], showfliers=False)
    stripplot_data = feature_df_average.sample(min(1000, len(feature_df_average)))
    sns.stripplot(y="rmsf_average", data=stripplot_data,
                  ax=ax_box, alpha=0.2, size=2, color=palette[1])
    ax_box.set_title("Overall Residue RMSF Distribution", fontsize=14)
    ax_box.set_ylabel("RMSF (nm)", fontsize=12)
    ax_box.set_xlabel("")
    ax_box.set_ylim(bottom=max(0, rmsf_all_q1 * 0.9), top=rmsf_all_q3 * 1.1)

    ax_size = plt.subplot(gs[1, 0])
    if "protein_size" in feature_df_average.columns:
        domain_size_info = feature_df_average[['domain_id', 'protein_size']].drop_duplicates()
        domain_size_rmsf = pd.merge(domain_stats, domain_size_info, on="domain_id", how="left")

        if not domain_size_rmsf.empty and 'protein_size' in domain_size_rmsf.columns and domain_size_rmsf['protein_size'].notna().any():
            scatter = ax_size.scatter(domain_size_rmsf["protein_size"],
                                   domain_size_rmsf["mean_domain_rmsf"],
                                   alpha=0.6, s=30, c=domain_size_rmsf["mean_domain_rmsf"],
                                   cmap="viridis")
            cbar = plt.colorbar(scatter, ax=ax_size)
            cbar.set_label("Mean Domain RMSF (nm)")

            try:
                valid_fit_data = domain_size_rmsf[['protein_size', 'mean_domain_rmsf']].dropna()
                x = valid_fit_data["protein_size"].astype(float)
                y = valid_fit_data["mean_domain_rmsf"].astype(float)
                if len(x) > 1:
                    z = np.polyfit(x, y, 1)
                    p = np.poly1d(z)
                    x_range = np.linspace(x.min(), x.max(), 100)
                    ax_size.plot(x_range, p(x_range), "r--", linewidth=1.5, alpha=0.7)
                    corr = np.corrcoef(x, y)[0, 1]
                    ax_size.text(0.05, 0.95, f"r = {corr:.3f}", transform=ax_size.transAxes,
                              fontsize=10, va="top", ha="left",
                              bbox=dict(facecolor="white", alpha=0.7, boxstyle="round"))
            except Exception as e:
                logging.warning(f"Failed to create trend line for size vs RMSF: {e}")

            ax_size.set_title("Mean Domain RMSF vs Protein Size", fontsize=14)
            ax_size.set_xlabel("Number of Residues", fontsize=12)
            ax_size.set_ylabel("Mean Domain RMSF (nm)", fontsize=12)
        else:
             ax_size.text(0.5, 0.5, "Protein size data processed incorrectly or missing", ha="center", va="center", fontsize=12)
             ax_size.axis("off")
    else:
        ax_size.text(0.5, 0.5, "Protein size data not available in feature_df_average", ha="center", va="center", fontsize=12)
        ax_size.axis("off")

    ax_var = plt.subplot(gs[1, 1])
    domain_variability = feature_df_average.groupby("domain_id")["rmsf_average"].std().reset_index()
    domain_variability = domain_variability.rename(columns={'rmsf_average': 'rmsf_std'})

    if not domain_variability.empty and 'rmsf_std' in domain_variability.columns and domain_variability['rmsf_std'].notna().any():
        valid_std_data = domain_variability["rmsf_std"].dropna()
        std_q1, std_q3 = valid_std_data.quantile([0.01, 0.99])
        hist_std_data = valid_std_data[(valid_std_data >= std_q1) & (valid_std_data <= std_q3)]

        sns.histplot(hist_std_data, kde=True, ax=ax_var, color=palette[2], bins=30) # Use palette color
        median_std = valid_std_data.median()
        ax_var.axvline(median_std, color="red", linestyle="--", linewidth=1.5)
        ax_var.text(median_std, ax_var.get_ylim()[1] * 0.9, f"Median Std: {median_std:.4f}",
                  ha="center", va="top", fontsize=9,
                  bbox=dict(facecolor="white", alpha=0.7, boxstyle="round"))
        ax_var.set_title("RMSF Variability Within Domains", fontsize=14)
        ax_var.set_xlabel("Std Dev of RMSF per Domain (nm)", fontsize=12)
        ax_var.set_ylabel("Count", fontsize=12)
        ax_var.set_xlim(left=max(0, std_q1*0.9), right=std_q3*1.1)
    else:
         ax_var.text(0.5, 0.5, "RMSF variability data not available", ha="center", va='center', fontsize=12)
         ax_var.axis("off")

    plt.tight_layout()
    output_path = os.path.join(vis_dir, "temperature_average_summary.png")
    _save_plot(fig, vis_dir, "temperature_average_summary.png", dpi)
    return output_path


def create_rmsf_distribution_plots(replica_avg_data: Dict[str, pd.DataFrame],
                                  overall_avg_data: Optional[pd.DataFrame],
                                  output_dir: str,
                                  viz_config: Dict[str, Any]) -> List[Optional[str]]:
    """
    Create distribution plots (violin, separate histograms) for RMSF by temperature.

    Args:
        replica_avg_data: Dict {temp: avg_replica_df}.
        overall_avg_data: DataFrame with overall average RMSF.
        output_dir: Base output directory.
        viz_config: Visualization config section.

    Returns:
        List[Optional[str]]: Paths to the saved figures (violin, separated histograms).
    """
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE)
    palette = _setup_plot_style(palette_name)
    histogram_bins = viz_config.get('histogram_bins', 50) # Use configured bins

    saved_paths = []

    temps = sorted([temp for temp in replica_avg_data.keys() if temp.isdigit()], key=int)
    if not temps:
        logging.warning("No numeric temperature data available for RMSF distribution plots")
        return [None, None]

    # Prepare data for plotting
    dist_data = []
    for temp in temps:
        if temp in replica_avg_data and replica_avg_data[temp] is not None:
            temp_df = replica_avg_data[temp]
            rmsf_col = f"rmsf_{temp}"
            if rmsf_col in temp_df.columns:
                # Append data with temperature label
                temp_subset = temp_df[[rmsf_col]].copy()
                temp_subset['Temperature'] = temp # Keep as int for sorting
                temp_subset.rename(columns={rmsf_col: "RMSF"}, inplace=True)
                dist_data.append(temp_subset)

    if not dist_data:
        logging.warning("No valid RMSF data collected for distribution plots")
        return [None, None]

    dist_df = pd.concat(dist_data, ignore_index=True)
    # Ensure Temperature is treated categorically for plotting if needed
    dist_df['Temperature_Str'] = dist_df['Temperature'].astype(str) + "K"
    temp_order_str = [str(t)+"K" for t in temps]

    # 1. Create violin plot
    try:
        fig_violin, ax_violin = plt.subplots(figsize=(max(8, len(temps)*1.2), 6))
        violin_palette = sns.color_palette(palette_name, n_colors=len(temps))
        sns.violinplot(x="Temperature_Str", y="RMSF", data=dist_df, order=temp_order_str,
                       palette=violin_palette, ax=ax_violin, hue="Temperature_Str", legend=False)
        ax_violin.set_title("RMSF Distribution by Temperature")
        ax_violin.set_xlabel("Temperature (K)")
        ax_violin.set_ylabel("RMSF (nm)")
        ax_violin.tick_params(axis='x', rotation=45)
        plt.tight_layout()
        violin_path = os.path.join(vis_dir, "rmsf_violin_plot.png")
        _save_plot(fig_violin, vis_dir, "rmsf_violin_plot.png", dpi)
        saved_paths.append(violin_path)
    except Exception as e:
        logging.error(f"Failed to create RMSF violin plot: {e}", exc_info=True)
        saved_paths.append(None)

    # 2. Create separate histograms
    try:
        # Determine grid size
        num_plots = len(temps) + (1 if overall_avg_data is not None and not overall_avg_data.empty else 0)
        ncols = 3
        nrows = (num_plots + ncols - 1) // ncols
        fig_hist, axs = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 4), sharex=True, sharey=True)
        axs = axs.flatten()

        hist_palette = sns.color_palette(palette_name, n_colors=len(temps))

        for i, temp in enumerate(temps):
            ax = axs[i]
            temp_data = dist_df[dist_df["Temperature"] == temp]["RMSF"].dropna()
            if not temp_data.empty:
                sns.histplot(temp_data, kde=True, bins=histogram_bins, ax=ax, color=hist_palette[i], stat="density")
                ax.set_title(f"Temperature {temp}K")
                ax.set_xlabel("RMSF (nm)")
                ax.set_ylabel("Density" if i % ncols == 0 else "") # Label only first column y-axis

                mean_val = temp_data.mean()
                std_val = temp_data.std()
                ax.text(0.95, 0.95, f"Mean: {mean_val:.4f}\nStd: {std_val:.4f}",
                        transform=ax.transAxes, fontsize=9, # Smaller font
                        verticalalignment='top', horizontalalignment='right',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
                ax.axvline(mean_val, color='k', linestyle='--', linewidth=1) # Use black for mean line
            else:
                 ax.text(0.5, 0.5, "No Data", ha='center', va='center')
                 ax.axis('off')


        # Add overall average histogram if available
        if overall_avg_data is not None and not overall_avg_data.empty and 'rmsf_average' in overall_avg_data.columns:
            ax = axs[len(temps)]
            avg_data = overall_avg_data["rmsf_average"].dropna()
            if not avg_data.empty:
                sns.histplot(avg_data, kde=True, bins=histogram_bins, ax=ax, color="grey", stat="density") # Neutral color
                ax.set_title("Average RMSF (All Temps)")
                ax.set_xlabel("RMSF (nm)")
                ax.set_ylabel("Density" if len(temps) % ncols == 0 else "") # Label y-axis if it starts a new row

                mean_val = avg_data.mean()
                std_val = avg_data.std()
                ax.text(0.95, 0.95, f"Mean: {mean_val:.4f}\nStd: {std_val:.4f}",
                        transform=ax.transAxes, fontsize=9,
                        verticalalignment='top', horizontalalignment='right',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
                ax.axvline(mean_val, color='k', linestyle='--', linewidth=1)
            else:
                ax.text(0.5, 0.5, "No Data", ha='center', va='center')
                ax.axis('off')


        # Hide unused axes
        for j in range(num_plots, nrows * ncols):
            axs[j].axis('off')

        plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout
        fig_hist.suptitle("RMSF Distributions by Temperature", fontsize=16, y=0.99)
        hist_path = os.path.join(vis_dir, "rmsf_histogram_separated.png") # Changed filename
        _save_plot(fig_hist, vis_dir, "rmsf_histogram_separated.png", dpi)
        saved_paths.append(hist_path)

    except Exception as e:
        logging.error(f"Failed to create RMSF separated histograms: {e}", exc_info=True)
        saved_paths.append(None)

    return saved_paths


def create_amino_acid_rmsf_plot(feature_df_average: pd.DataFrame,
                              output_dir: str,
                              viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE)
    _setup_plot_style(palette_name)

    if feature_df_average is None or feature_df_average.empty or 'rmsf_average' not in feature_df_average.columns or 'resname' not in feature_df_average.columns:
        logging.warning("No average feature data available for amino acid plot")
        return None

    plot_data = feature_df_average[['resname', 'rmsf_average']].copy()
    plot_data['resname'] = plot_data['resname'].replace({'HSD': 'HIS', 'HSE': 'HIS', 'HSP': 'HIS'})
    standard_aa = sorted(list(PDB_MAX_ASA.keys()))
    if 'UNK' in standard_aa: standard_aa.remove('UNK')
    plot_data = plot_data[plot_data['resname'].isin(standard_aa)]

    if plot_data.empty:
         logging.error("No standard amino acid data left for RMSF plot.")
         return None

    order = sorted(plot_data['resname'].unique())

    fig, ax = plt.subplots(figsize=(16, 7))
    num_colors_needed = len(order)
    plot_palette = sns.color_palette(palette_name, n_colors=num_colors_needed)

    sns.violinplot(data=plot_data, x='resname', y='rmsf_average', ax=ax,
                   hue='resname',
                   palette=plot_palette, order=order,
                   density_norm='width', cut=0, legend=False)

    sns.pointplot(data=plot_data, x='resname', y='rmsf_average', order=order, ax=ax,
                  color='black', markers='.', linestyle='none',
                  errorbar=None)

    ax.set_title('Average RMSF Distribution by Amino Acid Type')
    ax.set_xlabel('Amino Acid')
    ax.set_ylabel('Average RMSF (nm)')
    ax.tick_params(axis='x', rotation=45, labelsize=9)
    plt.tight_layout()

    output_path = os.path.join(vis_dir, "amino_acid_rmsf_violin_plot.png")
    _save_plot(fig, vis_dir, "amino_acid_rmsf_violin_plot.png", dpi)
    return output_path


def create_amino_acid_rmsf_plot_colored(feature_df_average: pd.DataFrame,
                                         output_dir: str,
                                         viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    _setup_plot_style(viz_config.get('palette'))

    if feature_df_average is None or feature_df_average.empty or 'rmsf_average' not in feature_df_average.columns or 'resname' not in feature_df_average.columns:
        logging.warning("No average feature data available for colored amino acid plot")
        return None

    avg_df = feature_df_average.copy()
    avg_df["resname"] = avg_df["resname"].apply(lambda x: "HIS" if x in ["HSE", "HSP", "HSD"] else x)

    aa_properties = {
        "ALA": {"color": "salmon", "type": "hydrophobic"}, "ARG": {"color": "royalblue", "type": "basic"},
        "ASN": {"color": "mediumseagreen", "type": "polar"}, "ASP": {"color": "crimson", "type": "acidic"},
        "CYS": {"color": "gold", "type": "special"}, "GLN": {"color": "mediumseagreen", "type": "polar"},
        "GLU": {"color": "crimson", "type": "acidic"}, "GLY": {"color": "lightgray", "type": "special"},
        "HIS": {"color": "cornflowerblue", "type": "basic"}, "ILE": {"color": "darksalmon", "type": "hydrophobic"},
        "LEU": {"color": "darksalmon", "type": "hydrophobic"}, "LYS": {"color": "royalblue", "type": "basic"},
        "MET": {"color": "orange", "type": "hydrophobic"}, "PHE": {"color": "chocolate", "type": "aromatic"},
        "PRO": {"color": "greenyellow", "type": "special"}, "SER": {"color": "mediumseagreen", "type": "polar"},
        "THR": {"color": "mediumseagreen", "type": "polar"}, "TRP": {"color": "chocolate", "type": "aromatic"},
        "TYR": {"color": "chocolate", "type": "aromatic"}, "VAL": {"color": "darksalmon", "type": "hydrophobic"}
    }
    standard_aa_props = list(aa_properties.keys())
    aa_df = avg_df[avg_df['resname'].isin(standard_aa_props)][['resname', 'rmsf_average']].copy()

    if aa_df.empty:
        logging.warning("No standard AA data remaining for colored plot.")
        return None

    all_residues_present = sorted(aa_df["resname"].unique())
    aa_stats = aa_df.groupby("resname")["rmsf_average"].agg(['mean', 'std', 'count']).reset_index()
    colors_dict = {aa: aa_properties.get(aa, {"color": "gray"})["color"] for aa in all_residues_present}

    fig, ax = plt.subplots(figsize=(16, 10))
    sns.violinplot(x="resname", y="rmsf_average", data=aa_df, order=all_residues_present,
                   hue="resname", palette=colors_dict, inner="box", ax=ax, legend=False)

    for i, aa in enumerate(all_residues_present):
        stats = aa_stats[aa_stats["resname"] == aa]
        if not stats.empty:
            mean_val = stats.iloc[0]['mean']
            ax.scatter(i, mean_val, color='black', s=30, zorder=10)
            count = stats.iloc[0]['count']
            ax.annotate(f"n={int(count):,}", xy=(i, -0.05), xycoords=('data', 'axes fraction'),
                         ha='center', va='top', fontsize=8, rotation=90)


    type_colors = {
        "hydrophobic": "darksalmon", "polar": "mediumseagreen", "acidic": "crimson",
        "basic": "royalblue", "aromatic": "chocolate", "special": "gold"
    }
    legend_elements = [plt.Line2D([0], [0], color=color, marker='o', linestyle='', markersize=10, label=type_name)
                     for type_name, color in type_colors.items()]
    ax.legend(handles=legend_elements, title="Amino Acid Types", loc='upper right')

    plt.title("RMSF Distribution by Amino Acid Type (Colored by Property)", fontsize=14)
    plt.xlabel("Amino Acid")
    plt.ylabel("RMSF (nm)")
    plt.xticks(rotation=45)
    plt.tight_layout(rect=[0, 0.05, 1, 0.95])

    colored_output_path = os.path.join(vis_dir, "amino_acid_rmsf_colored.png")
    _save_plot(fig, vis_dir, "amino_acid_rmsf_colored.png", dpi)
    return colored_output_path


def create_replica_variance_plot(combined_rmsf_data: Dict[str, Dict[str, pd.DataFrame]],
                               output_dir: str,
                               viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE)
    palette = _setup_plot_style(palette_name)

    variance_data = []
    temps = list(combined_rmsf_data.keys())
    if not temps:
        logging.warning("No combined RMSF data available for replica variance plot")
        return None

    for temp_str in temps:
        replica_dict = combined_rmsf_data.get(temp_str, {})
        if not replica_dict or len(replica_dict) < 2:
            logging.debug(f"Skipping T={temp_str} for variance plot: Not enough replicas ({len(replica_dict)}).")
            continue

        all_reps_df_list = []
        rmsf_col = f"rmsf_{temp_str}"
        for rep_str, df in replica_dict.items():
            if df is not None and not df.empty:
                 if rmsf_col in df.columns:
                      rep_df = df[['domain_id', 'resid', rmsf_col]].copy()
                      rep_df['replica'] = rep_str
                      all_reps_df_list.append(rep_df)
                 else:
                      logging.warning(f"RMSF column '{rmsf_col}' not found in replica data for {temp_str}, {rep_str}")

        if not all_reps_df_list: continue
        temp_combined_df = pd.concat(all_reps_df_list, ignore_index=True)

        grouped = temp_combined_df.groupby(['domain_id', 'resid'])
        stats = grouped[rmsf_col].agg(['mean', 'var', 'std', 'count'])
        stats = stats[stats['count'] > 1].reset_index()

        if stats.empty: continue

        stats['Temperature'] = temp_str
        stats['CV'] = (stats['std'] / stats['mean'].replace(0, np.nan)).fillna(0) * 100
        variance_data.append(stats)


    if not variance_data:
        logging.warning("No variance data calculated across replicas.")
        return None

    variance_df = pd.concat(variance_data, ignore_index=True)
    variance_df = variance_df.rename(columns={'mean': 'Mean_RMSF', 'var': 'Variance'})

    fig = plt.figure(figsize=(14, 10))
    gs = gridspec.GridSpec(2, 2, height_ratios=[3, 1], width_ratios=[3, 1])
    ax_main = plt.subplot(gs[0, 0])
    colors = ["blue", "green", "yellow", "red"]
    cmap_name = "cv_colormap"
    cm = LinearSegmentedColormap.from_list(cmap_name, colors)

    plot_x = variance_df["Mean_RMSF"].replace([np.inf, -np.inf], np.nan).dropna()
    plot_y = variance_df["Variance"].replace([np.inf, -np.inf], np.nan).fillna(0).dropna()
    if plot_x.empty or plot_y.empty or len(plot_x) != len(plot_y):
         logging.warning("Insufficient valid data for hist2d in replica variance plot.")
         plt.close(fig)
         return None

    h = ax_main.hist2d(plot_x, plot_y, bins=50, cmap="Blues", alpha=0.8, cmin=1)
    plt.colorbar(h[3], ax=ax_main, label="Number of residues")

    high_cv_threshold = np.percentile(variance_df['CV'].dropna(), 90)
    high_cv = variance_df[(variance_df["CV"] > high_cv_threshold) & variance_df["CV"].notna()]
    if not high_cv.empty:
        scatter = ax_main.scatter(high_cv["Mean_RMSF"], high_cv["Variance"],
                                c=high_cv["CV"], cmap=cm, alpha=0.7, s=20, edgecolor='k', vmin=0, vmax=max(100, high_cv["CV"].max()))
        cb = plt.colorbar(scatter, ax=ax_main, label="CV (%) [Top 10%]")
    else:
         logging.info("No high CV outliers found for scatter overlay.")

    ax_main.set_title("RMSF Variance vs Mean RMSF (with density)")
    ax_main.set_xlabel("Mean RMSF (nm)")
    ax_main.set_ylabel("Variance of RMSF (nm²)")
    xlim_right = np.percentile(plot_x, 99.5) if not plot_x.empty else 1
    ylim_top = np.percentile(plot_y[plot_y > 0], 99.5) if any(plot_y > 0) else 0.1
    ax_main.set_xlim(left=0, right=xlim_right)
    ax_main.set_ylim(bottom=0, top=ylim_top)

    ax_right = plt.subplot(gs[0, 1], sharey=ax_main)
    ax_right.hist(plot_y, bins=50, orientation='horizontal', color=palette[0], alpha=0.7, range=(0, ax_main.get_ylim()[1])) # Use palette
    ax_right.set_xlabel("Count")
    ax_right.set_title("Variance Dist.")
    plt.setp(ax_right.get_yticklabels(), visible=False)

    ax_bottom = plt.subplot(gs[1, 0], sharex=ax_main)
    ax_bottom.hist(plot_x, bins=50, color=palette[0], alpha=0.7, range=ax_main.get_xlim()) # Use palette
    ax_bottom.set_ylabel("Count")
    ax_bottom.set_title("Mean RMSF Dist.")
    plt.setp(ax_bottom.get_xticklabels(), visible=False)

    ax_temp = plt.subplot(gs[1, 1])
    try:
        variance_df['Temperature_Num'] = pd.to_numeric(variance_df['Temperature'])
        temp_order = sorted(variance_df['Temperature'].unique(), key=lambda x: int(x))
    except ValueError:
        temp_order = sorted(variance_df['Temperature'].unique())

    n_temps = len(temp_order)
    temp_palette = sns.color_palette(palette_name, n_colors=n_temps)

    sns.boxplot(x="Temperature", y="Variance", data=variance_df, ax=ax_temp, order=temp_order,
                hue="Temperature", palette=temp_palette,
                showfliers=False, legend=False)

    ax_temp.set_title("Variance by Temperature")
    ax_temp.set_xlabel("Temperature (K)")
    ax_temp.set_ylabel("Variance (nm²)")
    ax_temp.tick_params(axis='x', rotation=45)
    ax_temp.set_ylim(bottom=0, top=ylim_top)

    total_residues = len(variance_df)
    high_cv_pct = len(high_cv) / total_residues * 100 if total_residues > 0 and not high_cv.empty else 0
    mean_cv_val = variance_df['CV'].mean()
    ax_main.text(0.01, 0.99,
               f"Total residues (var calculated): {total_residues:,}\n"
               f"High CV outliers (>{high_cv_threshold:.1f}%): {len(high_cv):,} ({high_cv_pct:.1f}%)\n"
               f"Mean CV: {mean_cv_val:.2f}%" if pd.notna(mean_cv_val) else "Mean CV: N/A",
               transform=ax_main.transAxes, fontsize=9, verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.tight_layout()
    output_path = os.path.join(vis_dir, "replica_variance_plot.png")
    _save_plot(fig, vis_dir, "replica_variance_plot.png", dpi)
    return output_path


def create_dssp_rmsf_correlation_plot(feature_df_average: pd.DataFrame,
                                    output_dir: str,
                                    viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE)
    palette = _setup_plot_style(palette_name)

    if feature_df_average is None or feature_df_average.empty:
        logging.warning("No average feature data available for DSSP correlation plot")
        return None
    required_cols = ["dssp", "rmsf_average"]
    if not all(col in feature_df_average.columns for col in required_cols):
         logging.warning(f"DSSP correlation plot requires columns: {required_cols}. Found: {feature_df_average.columns.tolist()}")
         return None

    avg_df = feature_df_average.copy()

    dssp_category_map = {
        'H': 'Alpha Helix', 'G': 'Alpha Helix', 'I': 'Alpha Helix',
        'E': 'Beta Sheet', 'B': 'Beta Sheet',
        'T': 'Turn/Bend', 'S': 'Turn/Bend',
        'C': 'Coil/Loop', ' ': 'Coil/Loop', '-': 'Coil/Loop'
    }
    avg_df['ss_category'] = avg_df['dssp'].apply(
        lambda x: dssp_category_map.get(str(x).upper(), 'Coil/Loop')
    ).astype('category')

    category_order = ['Alpha Helix', 'Beta Sheet', 'Turn/Bend', 'Coil/Loop']
    present_categories = [cat for cat in category_order if cat in avg_df['ss_category'].cat.categories]
    if not present_categories:
         logging.warning("No recognized secondary structure categories found for DSSP plot.")
         return None
    avg_df['ss_category'] = avg_df['ss_category'].cat.set_categories(present_categories)


    fig = plt.figure(figsize=(18, 12))
    gs = gridspec.GridSpec(2, 3, height_ratios=[1, 1.5])

    ax1 = plt.subplot(gs[0, 0:2])
    category_palette = {'Alpha Helix': 'crimson', 'Beta Sheet': 'royalblue', 'Turn/Bend': 'gold', 'Coil/Loop': 'mediumseagreen'}
    plot_palette = {k: v for k, v in category_palette.items() if k in present_categories}

    sns.violinplot(x='ss_category', y='rmsf_average', data=avg_df,
                 order=present_categories, ax=ax1, inner='quartile',
                 hue='ss_category', palette=plot_palette, legend=False)

    stats = avg_df.groupby('ss_category', observed=False)['rmsf_average'].agg(['mean', 'std', 'count'])
    stats = stats.reindex(present_categories)

    y_min, y_max = ax1.get_ylim()
    text_y_base = y_max * 0.95

    for i, cat in enumerate(present_categories):
        if cat in stats.index and pd.notna(stats.loc[cat, 'count']):
            row = stats.loc[cat]
            text_content = f"n={int(row['count']):,}\nMean={row['mean']:.4f}\nStd={row['std']:.4f}"
            ax1.text(i, text_y_base , text_content,
                   ha='center', va='top', bbox=dict(facecolor='white', alpha=0.7, boxstyle='round'), fontsize=8)
        else:
            ax1.text(i, text_y_base, "No Data", ha='center', va='top', fontsize=8)


    ax1.set_title("RMSF Distribution by Secondary Structure Category", fontsize=14)
    ax1.set_xlabel("Secondary Structure Type", fontsize=12)
    ax1.set_ylabel("RMSF (nm)", fontsize=12)

    ax2 = plt.subplot(gs[0, 2])
    category_counts = avg_df['ss_category'].value_counts().reindex(present_categories).fillna(0)
    bar_colors = [plot_palette.get(cat, 'gray') for cat in category_counts.index]
    bars = ax2.bar(range(len(category_counts)), category_counts, color=bar_colors)
    ax2.set_xticks(range(len(category_counts)))
    ax2.set_xticklabels(category_counts.index, rotation=45, ha='right')
    ax2.set_title("Residue Distribution by Structure Type", fontsize=14)
    ax2.set_ylabel("Number of Residues", fontsize=12)

    total = category_counts.sum()
    if total > 0:
        ax2.bar_label(bars, fmt=lambda x: f'{x/total:.1%}' if total > 0 else '0%', label_type='edge', fontsize=8, padding=3)

    ax3 = plt.subplot(gs[1, 0])
    valid_dssp_codes = avg_df['dssp'].replace([' ', '-'], np.nan).dropna()
    top_dssp = []
    dssp_subset = pd.DataFrame()
    if not valid_dssp_codes.empty:
        top_dssp = valid_dssp_codes.value_counts().head(10).index.tolist()
        dssp_subset = avg_df[avg_df['dssp'].isin(top_dssp)].copy()

    if not dssp_subset.empty:
        medians = dssp_subset.groupby('dssp')['rmsf_average'].median().sort_values(ascending=False)
        sorted_dssp = medians.index.tolist()

        dssp_colors = {
            'H': '#FF0000', 'G': '#FFA500', 'I': '#FFC0CB', 'E': '#0000FF', 'B': '#ADD8E6',
            'T': '#008000', 'S': '#FFFF00', 'C': '#808080', ' ': '#808080', '-': '#808080'
        }
        color_map_dict = {code: dssp_colors.get(code, 'gray') for code in sorted_dssp}

        sns.boxplot(x='dssp', y='rmsf_average', data=dssp_subset,
                    order=sorted_dssp, hue='dssp', palette=color_map_dict,
                    ax=ax3, showfliers=False, legend=False)

        ax3.set_title("RMSF by Specific DSSP Code (Top 10)", fontsize=14)
        ax3.set_xlabel("DSSP Code", fontsize=12)
        ax3.set_ylabel("RMSF (nm)", fontsize=12)
    else:
        ax3.text(0.5, 0.5, "No valid top DSSP codes found.", ha='center', va='center', fontsize=12)
        ax3.axis('off')

    ax4 = plt.subplot(gs[1, 1:])
    if "normalized_resid" in avg_df.columns:
        plot_data_heatmap = avg_df[['normalized_resid', 'ss_category', 'rmsf_average']].copy()
        try:
             plot_data_heatmap['position_bin'] = pd.qcut(plot_data_heatmap['normalized_resid'], 20,
                                                          labels=False, duplicates='drop')
             pivot_data = plot_data_heatmap.groupby(['position_bin', 'ss_category'], observed=False)['rmsf_average'].mean().reset_index()
             if not pivot_data.empty:
                 pivot_table = pivot_data.pivot(index='position_bin', columns='ss_category', values='rmsf_average')
                 pivot_table = pivot_table.reindex(columns=present_categories)

                 sns.heatmap(pivot_table, cmap="YlOrRd", annot=True, fmt=".3f", linewidths=.5,
                           cbar_kws={'label': 'Mean RMSF (nm)'}, ax=ax4, annot_kws={"size": 7})
                 ax4.set_title("RMSF by Structure Type and Relative Position", fontsize=14)
                 ax4.set_xlabel("Secondary Structure Type", fontsize=12)
                 ax4.set_ylabel("Normalized Residue Position (bins)", fontsize=12)
                 ax4.tick_params(axis='y', labelsize=8)
             else:
                  logging.warning("Pivot table for position vs structure heatmap is empty.")
                  ax4.text(0.5, 0.5, "Pivot table empty", ha='center', va='center', fontsize=12)
                  ax4.axis('off')

        except Exception as e:
             logging.warning(f"Could not generate position vs structure heatmap: {e}")
             ax4.text(0.5, 0.5, "Could not generate heatmap", ha='center', va='center', fontsize=12)
             ax4.axis('off')
    else:
        ax4.text(0.5, 0.5, "Normalized position data missing", ha='center', va='center', fontsize=12)
        ax4.axis('off')


    dssp_descriptions = {
        'H': 'α-helix', 'G': '3₁₀-helix', 'I': 'π-helix', 'E': 'β-strand', 'B': 'β-bridge',
        'T': 'Turn', 'S': 'Bend', 'C': 'Coil', ' ': 'Undefined', '-': 'Undefined'
    }
    codes_in_plot = set(top_dssp)
    description = "DSSP Codes:\n" + "\n".join([f"• {k}: {v}" for k, v in dssp_descriptions.items() if k in codes_in_plot or k in [' ', '-','C']])
    fig.text(0.5, 0.01, description, ha='center', va='bottom', fontsize=8, wrap=True,
               bbox=dict(facecolor='white', alpha=0.9, boxstyle='round'))

    plt.tight_layout(rect=[0, 0.05, 1, 0.97])
    plt.subplots_adjust(hspace=0.4, wspace=0.3)

    output_path = os.path.join(vis_dir, "dssp_rmsf_correlation_plot.png")
    _save_plot(fig, vis_dir, "dssp_rmsf_correlation_plot.png", dpi)
    return output_path


def create_feature_correlation_plot(feature_df_average: pd.DataFrame,
                                  output_dir: str,
                                  viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    _setup_plot_style(viz_config.get('palette'))

    if feature_df_average is None or feature_df_average.empty:
        logging.warning("No average feature data available for feature correlation plot")
        return None

    numerical_cols = []
    potential_cols = ['rmsf_average', 'rmsf_log', 'protein_size', 'normalized_resid',
                      'relative_accessibility', 'phi_norm', 'psi_norm',
                      'core_exterior_encoded', 'secondary_structure_encoded', 'resname_encoded']
    for col in potential_cols:
        if col in feature_df_average.columns and pd.api.types.is_numeric_dtype(feature_df_average[col]):
             numerical_cols.append(col)

    if len(numerical_cols) < 2:
        logging.warning(f"Not enough numerical feature columns ({len(numerical_cols)}) found for correlation plot")
        return None

    plot_data = feature_df_average[numerical_cols].copy()
    plot_data.replace([np.inf, -np.inf], np.nan, inplace=True)
    plot_data.dropna(inplace=True)

    if len(plot_data) < 2:
        logging.warning("Not enough valid data points for correlation plot after handling NaNs.")
        return None

    corr_df = plot_data.corr(method='spearman')

    fig, ax = plt.subplots(figsize=(max(10, len(numerical_cols)*0.8), max(8, len(numerical_cols)*0.7)))
    sns.heatmap(corr_df, annot=True, cmap="coolwarm", fmt=".2f",
               vmin=-1, vmax=1, center=0, ax=ax, linewidths=.5, annot_kws={"size": 8})
    ax.set_title("Spearman Correlation Between Features (Average Dataset)")
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()

    output_path = os.path.join(vis_dir, "feature_correlation_plot.png")
    _save_plot(fig, vis_dir, "feature_correlation_plot.png", dpi)
    return output_path


def create_frames_visualization(pdb_results: Dict[str, Any], config: Dict[str, Any],
                              # domain_results: Dict[str, Dict[str, Any]], # Not used here
                              output_dir: str,
                              viz_config: Dict[str, Any]) -> Optional[str]:
    logging.warning("Frame visualization currently uses SIMULATED/PLACEHOLDER data. Needs adaptation for real data.")
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette = _setup_plot_style(viz_config.get('palette'))

    frame_selection = config.get("processing", {}).get("frame_selection", {})
    method = frame_selection.get("method", "rmsd")
    num_frames = frame_selection.get("num_frames", 1)
    cluster_method = frame_selection.get("cluster_method", "kmeans")

    fig = plt.figure(figsize=(14, 14))
    gs = gridspec.GridSpec(3, 2, height_ratios=[1, 2, 2])

    ax_meta = plt.subplot(gs[0, :])
    selection_info = (
        f"Frame Selection Configuration:\n"
        f"• Method: {method}\n"
        f"• Frames per domain/temp/rep: {num_frames}\n"
        f"• Clustering (RMSD): {cluster_method}\n"
    )
    frame_base_dir = os.path.join(output_dir, "frames")
    total_domains = len(pdb_results) if pdb_results else 0 # Placeholder count
    saved_frame_files = glob.glob(os.path.join(frame_base_dir, "*", "*", "*.pdb"), recursive=True)
    domains_with_frames = set()
    if saved_frame_files:
        for f_path in saved_frame_files:
            fname = os.path.basename(f_path)
            parts = fname.split('_')
            if len(parts) >= 3 and parts[-1].endswith('.pdb'):
                domains_with_frames.add(parts[0])

    num_domains_with_frames = len(domains_with_frames)
    frame_percentage = (num_domains_with_frames / total_domains * 100) if total_domains > 0 else 0
    selection_info += f"• Domains processed (Est.): {total_domains}\n" # Mark as estimate
    selection_info += f"• Domains with saved frames: {num_domains_with_frames} ({frame_percentage:.1f}%)\n"
    selection_info += f"• Total frames saved: {len(saved_frame_files):,}"

    ax_meta.text(0.5, 0.5, selection_info, ha='center', va='center', fontsize=10,
               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.2))
    ax_meta.set_title("Frame Selection Metadata", fontsize=14)
    ax_meta.axis('off')

    # Placeholder Panels
    for i in range(4):
        row, col = divmod(i, 2)
        ax = plt.subplot(gs[row + 1, col])
        ax.text(0.5, 0.5, f"Placeholder Panel {i+2}\n(Requires Real Data Analysis)",
                ha='center', va='center', fontsize=10, color='gray')
        ax.set_title(f"Analysis Panel {i+2}")
        ax.set_xticks([])
        ax.set_yticks([])

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    fig.suptitle("Frame Analysis Summary (Placeholder Data)", fontsize=16, y=0.99)

    output_path = os.path.join(vis_dir, "frames_analysis.png")
    _save_plot(fig, vis_dir, "frames_analysis.png", dpi)
    return output_path


def create_ml_features_plot(feature_df_average: pd.DataFrame,
                          output_dir: str,
                          viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE)
    palette = _setup_plot_style(palette_name)

    if feature_df_average is None or feature_df_average.empty:
        logging.warning("No average feature data available for ML features plot")
        return None

    avg_df = feature_df_average.copy()

    fig = plt.figure(figsize=(16, 14))
    gs = gridspec.GridSpec(3, 3, height_ratios=[1, 1.5, 1.5])

    ax_summary = plt.subplot(gs[0, :])
    feature_info = [
        f"ML Feature Dataset Overview (Avg Temp):",
        f"• Total residues: {len(avg_df):,}",
        f"• Unique domains: {avg_df['domain_id'].nunique()}",
        f"• Features: {', '.join([col for col in avg_df.columns if col not in ['domain_id', 'resid']])}"
    ]
    if "secondary_structure_encoded" in avg_df.columns:
        ss_dist = avg_df["secondary_structure_encoded"].value_counts(normalize=True)
        feature_info.append(f"• SS Dist: Helix {ss_dist.get(0, 0):.1%}, Sheet {ss_dist.get(1, 0):.1%}, Coil {ss_dist.get(2, 0):.1%}")
    if "core_exterior_encoded" in avg_df.columns:
        ce_dist = avg_df["core_exterior_encoded"].value_counts(normalize=True)
        feature_info.append(f"• Core/Ext Dist: Core {ce_dist.get(0, 0):.1%}, Exterior {ce_dist.get(1, 0):.1%}")
    ax_summary.text(0.5, 0.5, "\n".join(feature_info), ha='center', va='center', fontsize=10,
                  bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.2))
    ax_summary.set_title("ML Features Overview (Average)", fontsize=14)
    ax_summary.axis('off')

    ax_corr = plt.subplot(gs[1, 0:2])
    corr_features = []
    potential_corr_cols = ["rmsf_average", "relative_accessibility", "normalized_resid",
                   "secondary_structure_encoded", "core_exterior_encoded", "protein_size",
                   "rmsf_log", "phi_norm", "psi_norm", "resname_encoded"]
    for col in potential_corr_cols:
        if col in avg_df.columns and pd.api.types.is_numeric_dtype(avg_df[col]):
             corr_features.append(col)

    if len(corr_features) > 1:
        plot_data_corr = avg_df[corr_features].copy()
        plot_data_corr.replace([np.inf, -np.inf], np.nan, inplace=True)
        plot_data_corr.dropna(inplace=True)
        if len(plot_data_corr) > 1:
             corr_matrix = plot_data_corr.corr(method='spearman')
             sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", ax=ax_corr, annot_kws={"size": 7})
             ax_corr.set_title("Feature Correlation (Spearman)")
             ax_corr.tick_params(axis='x', rotation=45, labelsize=8)
             ax_corr.tick_params(axis='y', labelsize=8)
        else:
             ax_corr.text(0.5, 0.5, "Not enough valid data for correlation", ha='center', va='center')
             ax_corr.axis('off')
    else:
        ax_corr.text(0.5, 0.5, "Not enough numeric features for correlation", ha='center', va='center')
        ax_corr.axis('off')

    ax_access = plt.subplot(gs[1, 2])
    if "relative_accessibility" in avg_df.columns and "rmsf_average" in avg_df.columns:
        sample_size = min(5000, len(avg_df))
        sample_df = avg_df.sample(sample_size, random_state=42)
        if "core_exterior" in sample_df.columns:
            hue_order = sorted([h for h in sample_df['core_exterior'].unique() if pd.notna(h)])
            plot_palette_dict = {"core": palette[0], "exterior": palette[1]}

            if all(item in plot_palette_dict for item in hue_order):
                sns.scatterplot(data=sample_df, x="relative_accessibility", y="rmsf_average",
                              hue="core_exterior", palette=plot_palette_dict, hue_order=hue_order,
                              alpha=0.4, s=10, ax=ax_access)
                handles, labels = ax_access.get_legend_handles_labels()
                if handles: ax_access.legend(handles=handles, labels=labels, title="Location", fontsize=8)
            else:
                 logging.warning("Core/Exterior values mismatch palette keys. Plotting without hue.")
                 sns.scatterplot(data=sample_df, x="relative_accessibility", y="rmsf_average",
                               alpha=0.4, s=10, ax=ax_access, color=palette[0])
        else:
             sns.scatterplot(data=sample_df, x="relative_accessibility", y="rmsf_average",
                           alpha=0.4, s=10, ax=ax_access, color=palette[0])

        ax_access.set_title(f"RMSF vs Rel. Accessibility")
        ax_access.set_xlabel("Relative Accessibility")
        ax_access.set_ylabel("RMSF (nm)")
        ax_access.set_xlim(left=0, right=1.0)
    else:
        ax_access.text(0.5, 0.5, "RMSF/Access. data missing", ha='center', va='center')
        ax_access.axis('off')

    ax_pos = plt.subplot(gs[2, 0])
    if "normalized_resid" in avg_df.columns and "rmsf_average" in avg_df.columns:
        sample_size = min(5000, len(avg_df))
        sample_df = avg_df.sample(sample_size, random_state=42)
        sns.scatterplot(data=sample_df, x="normalized_resid", y="rmsf_average",
                      alpha=0.4, s=10, ax=ax_pos, color=palette[2])
        if STATSMODELS_AVAILABLE:
            try:
                 sns.regplot(data=sample_df, x="normalized_resid", y="rmsf_average",
                             scatter=False, lowess=True, ax=ax_pos, line_kws={'color': 'red', 'lw': 1.5})
            except Exception as e:
                 logging.warning(f"Could not add LOWESS smoothed line to pos vs RMSF: {e}")
        else:
             logging.debug("statsmodels not found, skipping LOWESS smoothing.")
        ax_pos.set_title(f"RMSF vs Norm. Position")
        ax_pos.set_xlabel("Normalized Residue Position")
        ax_pos.set_ylabel("RMSF (nm)")
        ax_pos.set_xlim(left=-0.05, right=1.05)
    else:
        ax_pos.text(0.5, 0.5, "RMSF/Pos. data missing", ha='center', va='center')
        ax_pos.axis('off')

    ax_ss = plt.subplot(gs[2, 1])
    if "secondary_structure_encoded" in avg_df.columns and "rmsf_average" in avg_df.columns:
        ss_map = {0: "Helix", 1: "Sheet", 2: "Coil"}
        plot_data_ss = avg_df[['secondary_structure_encoded', 'rmsf_average']].copy()
        plot_data_ss["SS_Type"] = plot_data_ss["secondary_structure_encoded"].map(ss_map)
        ss_order = [ss_map[i] for i in sorted(ss_map.keys()) if ss_map[i] in plot_data_ss['SS_Type'].unique()]
        ss_palette_dict = {"Helix": palette[3], "Sheet": palette[4], "Coil": palette[5]}
        plot_ss_palette = {k: v for k, v in ss_palette_dict.items() if k in ss_order}

        if ss_order:
            sns.boxplot(data=plot_data_ss, x="SS_Type", y="rmsf_average", ax=ax_ss, order=ss_order,
                        hue="SS_Type", palette=plot_ss_palette,
                        showfliers=False, legend=False)
            ax_ss.set_title("RMSF Dist by SS Type")
            ax_ss.set_xlabel("Secondary Structure Type")
            ax_ss.set_ylabel("RMSF (nm)")
        else:
             ax_ss.text(0.5, 0.5, "No SS data to plot", ha='center', va='center')
             ax_ss.axis('off')
    else:
        ax_ss.text(0.5, 0.5, "RMSF/SS data missing", ha='center', va='center')
        ax_ss.axis('off')

    ax_ce = plt.subplot(gs[2, 2])
    if "core_exterior_encoded" in avg_df.columns and "rmsf_average" in avg_df.columns:
        ce_map = {0: "Core", 1: "Exterior"}
        ce_plot_map = {"Core": "core", "Exterior": "exterior"}
        plot_data_ce = avg_df[['core_exterior_encoded', 'rmsf_average']].copy()
        plot_data_ce["Location_Full"] = plot_data_ce["core_exterior_encoded"].map(ce_map)
        plot_data_ce["Location"] = plot_data_ce["Location_Full"].map(ce_plot_map)
        loc_order = [loc for loc in ["core", "exterior"] if loc in plot_data_ce['Location'].unique()]
        ce_palette_dict = {"core": palette[0], "exterior": palette[1]}
        plot_ce_palette = {k: v for k, v in ce_palette_dict.items() if k in loc_order}

        if loc_order:
            sns.boxplot(data=plot_data_ce, x="Location", y="rmsf_average", ax=ax_ce, order=loc_order,
                        hue="Location", palette=plot_ce_palette,
                        showfliers=False, legend=False)
            ax_ce.set_title("RMSF Dist by Location")
            ax_ce.set_xlabel("Residue Location")
            ax_ce.set_ylabel("RMSF (nm)")
        else:
            ax_ce.text(0.5, 0.5, "No Core/Ext data to plot", ha='center', va='center')
            ax_ce.axis('off')
    else:
        ax_ce.text(0.5, 0.5, "RMSF/CoreExt data missing", ha='center', va='center')
        ax_ce.axis('off')

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    fig.suptitle("ML Features Analysis (Average Dataset)", fontsize=16, y=0.99)

    output_path = os.path.join(vis_dir, "ml_features_analysis.png")
    _save_plot(fig, vis_dir, "ml_features_analysis.png", dpi)
    return output_path


def create_summary_plot(replica_avg_data: Dict[str, pd.DataFrame],
                      feature_df_average: pd.DataFrame,
                      domain_status: Dict[str, str],
                      output_dir: str,
                      viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE)
    palette = _setup_plot_style(palette_name)

    has_avg_feature_data = feature_df_average is not None and not feature_df_average.empty
    has_replica_avg_data = replica_avg_data is not None and bool(replica_avg_data)
    has_status_data = domain_status is not None and bool(domain_status)

    fig = plt.figure(figsize=(16, 12))
    gs = gridspec.GridSpec(3, 3, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1])
    fig.suptitle("Pipeline Summary Report", fontsize=16, y=0.99)

    # Panel 1: Processing Status
    ax_status = plt.subplot(gs[0, 0])
    ax_status.set_title("Processing Status", fontsize=12)
    if has_status_data:
        status_series = pd.Series(domain_status)
        status_counts = status_series.value_counts()
        defined_order = ['Success', 'Failed HDF5 Read/Access', 'Failed PDB Read', 'Failed PDB Clean',
                         'Failed Properties Calc', 'Failed RMSF Proc', 'Failed Unexpected', 'Failed Component Init']
        present_statuses = status_counts.index.tolist()
        status_order = [s for s in defined_order if s in present_statuses] + \
                       [s for s in present_statuses if s not in defined_order]
        color_map = {
            'Success': 'mediumseagreen', 'Failed HDF5 Read/Access': 'tomato',
            'Failed PDB Read': 'tomato', 'Failed PDB Clean': 'tomato',
            'Failed Properties Calc': 'tomato', 'Failed RMSF Proc': 'tomato',
            'Failed Unexpected': 'darkred', 'Failed Component Init': 'salmon'
        }
        status_colors = [color_map.get(s, 'lightgray') for s in status_order]

        ax_status.pie(status_counts[status_order], labels=status_order, autopct='%1.1f%%',
                      startangle=90, colors=status_colors, textprops={'fontsize': 8}, pctdistance=0.85)
        ax_status.axis('equal')
    else:
        ax_status.text(0.5, 0.5, "No Status Data", ha='center', va='center')
        ax_status.axis('off')

    # Panel 2: RMSF Distribution (Overall Average)
    ax_rmsf_hist = plt.subplot(gs[0, 1])
    ax_rmsf_hist.set_title("Avg RMSF Distribution", fontsize=12)
    if has_avg_feature_data and 'rmsf_average' in feature_df_average.columns:
        avg_rmsf = feature_df_average["rmsf_average"].dropna()
        if not avg_rmsf.empty:
            sns.histplot(avg_rmsf, bins=30, kde=True, ax=ax_rmsf_hist, color=palette[0])
            mean_rmsf = avg_rmsf.mean()
            ax_rmsf_hist.axvline(mean_rmsf, color='r', linestyle='--', lw=1, label=f'Mean: {mean_rmsf:.3f}')
            ax_rmsf_hist.legend(fontsize=8)
            ax_rmsf_hist.set_xlabel("RMSF (nm)", fontsize=9)
            ax_rmsf_hist.set_ylabel("Count", fontsize=9)
        else:
             ax_rmsf_hist.text(0.5, 0.5, "No RMSF Values", ha='center', va='center')
             ax_rmsf_hist.axis('off')
    else:
        ax_rmsf_hist.text(0.5, 0.5, "No Avg RMSF Data", ha='center', va='center')
        ax_rmsf_hist.axis('off')

    # Panel 3: RMSF Temperature Trend
    ax_temp_trend = plt.subplot(gs[0, 2])
    ax_temp_trend.set_title("RMSF vs Temperature", fontsize=12)
    if has_replica_avg_data:
        temps = sorted([t for t in replica_avg_data.keys() if t.isdigit()], key=int)
        temp_rmsf_means = []
        temp_rmsf_stds = []
        valid_temps_for_plot = []
        for temp in temps:
            rmsf_col = f"rmsf_{temp}"
            if temp in replica_avg_data and replica_avg_data[temp] is not None and rmsf_col in replica_avg_data[temp].columns:
                 rmsf_vals = replica_avg_data[temp][rmsf_col].dropna()
                 if not rmsf_vals.empty:
                      temp_rmsf_means.append(rmsf_vals.mean())
                      temp_rmsf_stds.append(rmsf_vals.std())
                      valid_temps_for_plot.append(str(temp)) # Keep as string

        if temp_rmsf_means:
            ax_temp_trend.errorbar(valid_temps_for_plot, temp_rmsf_means, yerr=temp_rmsf_stds,
                                fmt='o-', capsize=3, color=palette[1], markersize=4, elinewidth=1, alpha=0.8)
            ax_temp_trend.set_xlabel("Temperature (K)", fontsize=9)
            ax_temp_trend.set_ylabel("Mean RMSF (nm)", fontsize=9)
            ax_temp_trend.tick_params(axis='x', rotation=45)
        else:
             ax_temp_trend.text(0.5, 0.5, "No Temp RMSF Data Calculated", ha='center', va='center')
             ax_temp_trend.axis('off')
    else:
        ax_temp_trend.text(0.5, 0.5, "No Temp RMSF Data Found", ha='center', va='center')
        ax_temp_trend.axis('off')

    # Panel 4: Secondary Structure Distribution
    ax_ss_pie = plt.subplot(gs[1, 0])
    ax_ss_pie.set_title("Secondary Structure", fontsize=12)
    if has_avg_feature_data and "secondary_structure_encoded" in feature_df_average.columns:
        ss_map = {0: "Helix", 1: "Sheet", 2: "Coil"}
        ss_counts = feature_df_average["secondary_structure_encoded"].map(ss_map).value_counts()
        ss_colors = {'Helix': palette[2], 'Sheet': palette[3], 'Coil': palette[4]}
        pie_colors = [ss_colors.get(lbl, 'grey') for lbl in ss_counts.index]
        ax_ss_pie.pie(ss_counts, labels=ss_counts.index, autopct='%1.1f%%',
                   startangle=90, colors=pie_colors,
                   textprops={'fontsize': 8})
        ax_ss_pie.axis('equal')
    else:
        ax_ss_pie.text(0.5, 0.5, "No SS Data", ha='center', va='center')
        ax_ss_pie.axis('off')

    # Panel 5: Core/Exterior Distribution
    ax_ce_pie = plt.subplot(gs[1, 1])
    ax_ce_pie.set_title("Core vs Exterior", fontsize=12)
    if has_avg_feature_data and "core_exterior_encoded" in feature_df_average.columns:
        ce_map = {0: "Core", 1: "Exterior"}
        plot_data_ce_pie = feature_df_average["core_exterior_encoded"].map(ce_map)
        ce_counts = plot_data_ce_pie.value_counts()
        ce_colors = {'Core': palette[5], 'Exterior': palette[6]}
        pie_colors_ce = [ce_colors.get(lbl, 'grey') for lbl in ce_counts.index]
        ax_ce_pie.pie(ce_counts, labels=ce_counts.index, autopct='%1.1f%%',
                   startangle=90, colors=pie_colors_ce,
                   textprops={'fontsize': 8})
        ax_ce_pie.axis('equal')
    else:
        ax_ce_pie.text(0.5, 0.5, "No Core/Ext Data", ha='center', va='center')
        ax_ce_pie.axis('off')

    # Panel 6: RMSF vs Accessibility Scatter
    ax_rmsf_access = plt.subplot(gs[1, 2])
    ax_rmsf_access.set_title("RMSF vs Access.", fontsize=12)
    if has_avg_feature_data and "rmsf_average" in feature_df_average.columns and "relative_accessibility" in feature_df_average.columns:
        sample_size = min(2000, len(feature_df_average))
        sample_df = feature_df_average.sample(sample_size, random_state=42)
        sns.scatterplot(data=sample_df, x="relative_accessibility", y="rmsf_average",
                      alpha=0.3, s=5, ax=ax_rmsf_access, color=palette[7])
        ax_rmsf_access.set_xlabel("Rel. Accessibility", fontsize=9)
        ax_rmsf_access.set_ylabel("RMSF (nm)", fontsize=9)
        ax_rmsf_access.set_xlim(0, 1)
    else:
        ax_rmsf_access.text(0.5, 0.5, "No RMSF/Access. Data", ha='center', va='center')
        ax_rmsf_access.axis('off')

    # Panel 7: Top Amino Acids Bar Chart
    ax_aa_bar = plt.subplot(gs[2, 0])
    ax_aa_bar.set_title("Top 5 Amino Acids", fontsize=12)
    if has_avg_feature_data and "resname" in feature_df_average.columns:
        aa_counts = feature_df_average["resname"].value_counts().head(5)
        bar_colors_aa = palette[:len(aa_counts)]
        # *** Corrected bar plot and label call ***
        bars_aa = ax_aa_bar.bar(aa_counts.index, aa_counts.values, color=bar_colors_aa)
        ax_aa_bar.bar_label(bars_aa, fmt='{:,.0f}') # Use the container returned by ax.bar
        ax_aa_bar.set_xlabel("Amino Acid", fontsize=9)
        ax_aa_bar.set_ylabel("Count", fontsize=9)
        ax_aa_bar.tick_params(axis='x', labelsize=8)
    else:
        ax_aa_bar.text(0.5, 0.5, "No Resname Data", ha='center', va='center')
        ax_aa_bar.axis('off')

    # Panel 8: RMSF by SS Type Bar Chart
    ax_ss_rmsf = plt.subplot(gs[2, 1])
    ax_ss_rmsf.set_title("RMSF by SS Type", fontsize=12)
    if has_avg_feature_data and "secondary_structure_encoded" in feature_df_average.columns and "rmsf_average" in feature_df_average.columns:
        ss_map = {0: "Helix", 1: "Sheet", 2: "Coil"}
        plot_data_ss = feature_df_average[['secondary_structure_encoded', 'rmsf_average']].copy()
        plot_data_ss["SS_Type"] = plot_data_ss["secondary_structure_encoded"].map(ss_map)
        ss_rmsf_means = plot_data_ss.groupby("SS_Type", observed=False)['rmsf_average'].mean().reindex(["Helix", "Sheet", "Coil"]).fillna(0)
        ss_colors = {'Helix': palette[2], 'Sheet': palette[3], 'Coil': palette[4]}
        bar_colors = [ss_colors.get(idx, 'grey') for idx in ss_rmsf_means.index]
        # *** Corrected bar plot and label call ***
        bars_ss = ax_ss_rmsf.bar(ss_rmsf_means.index, ss_rmsf_means.values, color=bar_colors)
        ax_ss_rmsf.bar_label(bars_ss, fmt='%.4f', fontsize=8)
        ax_ss_rmsf.set_xlabel("Secondary Structure", fontsize=9)
        ax_ss_rmsf.set_ylabel("Mean RMSF (nm)", fontsize=9)
        ax_ss_rmsf.tick_params(axis='x', rotation=0, labelsize=8)
    else:
        ax_ss_rmsf.text(0.5, 0.5, "No RMSF/SS Data", ha='center', va='center')
        ax_ss_rmsf.axis('off')

    # Panel 9: RMSF by Core/Exterior Bar Chart
    ax_ce_rmsf = plt.subplot(gs[2, 2])
    ax_ce_rmsf.set_title("RMSF by Location", fontsize=12)
    if has_avg_feature_data and "core_exterior" in feature_df_average.columns and "rmsf_average" in feature_df_average.columns:
        plot_data_ce = feature_df_average[['core_exterior', 'rmsf_average']].copy()
        ce_rmsf_means = plot_data_ce.groupby("core_exterior", observed=False)['rmsf_average'].mean().reindex(["core", "exterior"]).fillna(0)
        ce_colors = {'core': palette[5], 'exterior': palette[6]}
        bar_colors_ce = [ce_colors.get(idx, 'grey') for idx in ce_rmsf_means.index]
        # *** Corrected bar plot and label call ***
        bars_ce = ax_ce_rmsf.bar(ce_rmsf_means.index, ce_rmsf_means.values, color=bar_colors_ce)
        ax_ce_rmsf.bar_label(bars_ce, fmt='%.4f', fontsize=8)
        ax_ce_rmsf.set_xlabel("Location", fontsize=9)
        ax_ce_rmsf.set_ylabel("Mean RMSF (nm)", fontsize=9)
        ax_ce_rmsf.tick_params(axis='x', rotation=0, labelsize=8)
    else:
        ax_ce_rmsf.text(0.5, 0.5, "No RMSF/CoreExt Data", ha='center', va='center')
        ax_ce_rmsf.axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    output_path = os.path.join(vis_dir, "pipeline_summary_report.png")
    _save_plot(fig, vis_dir, "pipeline_summary_report.png", dpi)
    return output_path


def create_voxel_info_plot(config: Dict[str, Any],
                           voxel_output_file: Optional[str],
                           output_dir: str,
                           viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    _setup_plot_style(viz_config.get('palette'))

    voxel_config = config.get("processing", {}).get("voxelization", {})
    if not voxel_config.get("enabled", True):
        logging.info("Voxelization was disabled, skipping parameter summary plot.")
        return None

    fig = plt.figure(figsize=(10, 7))
    gs = gridspec.GridSpec(2, 1, height_ratios=[1.2, 1])

    ax_config = plt.subplot(gs[0])
    summary_lines = ["Voxelization Parameters (Aposteriori):"]
    try:
         resolution = f"{voxel_config.get('frame_edge_length', 0) / voxel_config.get('voxels_per_side', 1):.2f} Å/voxel"
    except (TypeError, ValueError, ZeroDivisionError):
         resolution = "N/A"

    params = [
        ('Enabled', voxel_config.get("enabled", False)),
        ('Executable Used', voxel_config.get("aposteriori_executable") or "make-frame-dataset (PATH)"),
        ('Output File', os.path.basename(voxel_output_file) if voxel_output_file and os.path.exists(voxel_output_file) else "Not Generated/Found"),
        ('Frame Edge Length', f"{voxel_config.get('frame_edge_length', 'N/A')} Å"),
        ('Voxels Per Side', voxel_config.get('voxels_per_side', 'N/A')),
        ('Resolution', resolution),
        ('Atom Encoder', voxel_config.get('atom_encoder', 'N/A')),
        ('Encode CB', voxel_config.get('encode_cb', 'N/A')),
        ('Compression (Gzip)', voxel_config.get('compression_gzip', 'N/A')),
        ('Voxelise All NMR States', voxel_config.get('voxelise_all_states', 'N/A')),
    ]
    for name, value in params:
        summary_lines.append(f"• {name}: {value}")

    if voxel_output_file and os.path.exists(voxel_output_file):
         try:
             size_mb = os.path.getsize(voxel_output_file) / (1024 * 1024)
             summary_lines.append(f"\n• Output File Size: {size_mb:.2f} MB")
             try:
                 with h5py.File(voxel_output_file, 'r') as f:
                     num_domains = len(list(f.keys()))
                     num_residues = 0
                     for domain_key in f:
                         if isinstance(f[domain_key], h5py.Group):
                             for chain_key in f[domain_key]:
                                  if isinstance(f[f"{domain_key}/{chain_key}"], h5py.Group):
                                       if 'residue_idx' in f[f"{domain_key}/{chain_key}"]:
                                            num_residues += len(f[f"{domain_key}/{chain_key}/residue_idx"][:])
                                       else:
                                             num_residues += len([k for k in f[f"{domain_key}/{chain_key}"] if k.isdigit()])
                     summary_lines.append(f"• Domains in Output: {num_domains}")
                     summary_lines.append(f"• Residues in Output: {num_residues:,}")
             except ImportError:
                 summary_lines.append("• (h5py not installed - cannot read content stats)")
             except Exception as h5_err:
                 summary_lines.append(f"• (Error reading HDF5 content: {h5_err})")
         except Exception as e:
              summary_lines.append(f"• (Could not get file stats: {e})")

    summary_text = "\n".join(summary_lines)
    ax_config.text(0.05, 0.95, summary_text, transform=ax_config.transAxes, fontsize=9,
                   verticalalignment='top', family='monospace')
    ax_config.axis('off')
    ax_config.set_title('Voxelization Parameters & Output', fontsize=12)

    ax_slice = plt.subplot(gs[1])
    ax_slice.set_title("Illustrative Voxel Slice (Example)", fontsize=12)
    try:
        grid_size = int(voxel_config.get('voxels_per_side', 21))
        fake_slice = np.zeros((grid_size, grid_size))
        center = grid_size // 2
        if center > 0:
            fake_slice[center, center] = 1
            if center+1 < grid_size and center-1 >= 0: fake_slice[center+1, center-1] = 0.8
            if center-1 >= 0: fake_slice[center-1, center-1] = 0.7
            if center+2 < grid_size: fake_slice[center, center+2] = 0.6
        if grid_size > 5:
            fake_slice = gaussian_filter1d(fake_slice, sigma=1.0)

        im = ax_slice.imshow(fake_slice, cmap='viridis', origin='lower')
        plt.colorbar(im, ax=ax_slice, label='Atom Density (Illustrative)')
        step = max(1, grid_size // 5)
        ax_slice.set_xticks(np.arange(-.5, grid_size-.5, step))
        ax_slice.set_yticks(np.arange(-.5, grid_size-.5, step))
        ax_slice.set_xticklabels(np.arange(0, grid_size, step))
        ax_slice.set_yticklabels(np.arange(0, grid_size, step))
        ax_slice.grid(color='white', linestyle='-', linewidth=0.5, alpha=0.3)
        ax_slice.set_xlabel("Voxel Index (X)")
        ax_slice.set_ylabel("Voxel Index (Y)")
    except Exception as plot_err:
         logging.warning(f"Could not generate illustrative voxel slice: {plot_err}")
         ax_slice.text(0.5, 0.5, "Could not generate slice", ha='center', va='center')
         ax_slice.axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.96])
    output_path = os.path.join(vis_dir, "voxelization_info.png")
    _save_plot(fig, vis_dir, "voxelization_info.png", dpi)
    return output_path


def create_additional_ml_features_plot(feature_df_average: pd.DataFrame,
                                     output_dir: str,
                                     viz_config: Dict[str, Any]) -> Optional[str]:
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE)
    palette = _setup_plot_style(palette_name)

    if feature_df_average is None or feature_df_average.empty:
        logging.warning("No average feature data available for additional ML features plot")
        return None

    avg_df = feature_df_average.copy()

    fig = plt.figure(figsize=(16, 14))
    gs = gridspec.GridSpec(3, 3, height_ratios=[1, 1.5, 1.5])
    fig.suptitle("Additional Features Analysis (Average Dataset)", fontsize=16, y=0.99)

    ax_importance = plt.subplot(gs[0, :])
    ax_importance.set_title("Feature Correlation with RMSF (Spearman)", fontsize=12)
    features_to_correlate = []
    potential_features = [col for col in avg_df.columns
                          if col not in ['domain_id', 'resid', 'resname', 'chain', 'dssp', 'core_exterior',
                                         'rmsf_average', 'rmsf_log']]
    for col in potential_features:
         if col in avg_df.columns and pd.api.types.is_numeric_dtype(avg_df[col]):
             features_to_correlate.append(col)

    if "rmsf_average" in avg_df.columns and features_to_correlate:
        corrs = []
        temp_df_corr = avg_df[['rmsf_average'] + features_to_correlate].copy()
        temp_df_corr.replace([np.inf, -np.inf], np.nan, inplace=True)
        temp_df_corr.dropna(inplace=True)

        if len(temp_df_corr) > 1:
            for feat in features_to_correlate:
                try:
                    corr = temp_df_corr[feat].corr(temp_df_corr["rmsf_average"], method='spearman')
                    corrs.append((feat, corr))
                except Exception as corr_err:
                     logging.debug(f"Could not calculate Spearman correlation for {feat}: {corr_err}")
                     corrs.append((feat, np.nan))

            corrs = sorted([c for c in corrs if pd.notna(c[1])], key=lambda x: abs(x[1]), reverse=True)

            if corrs:
                feat_names = [name.replace("_encoded", "").replace("_norm", "").replace("_", " ").title() for name, _ in corrs]
                corr_values = [corr for _, corr in corrs]
                colors = [palette[1] if c < 0 else palette[0] for c in corr_values]
                y_pos = range(len(feat_names))
                ax_importance.barh(y_pos, corr_values, color=colors, align='center')
                ax_importance.set_yticks(y_pos)
                ax_importance.set_yticklabels(feat_names, fontsize=8)
                ax_importance.axvline(0, color='black', linestyle='-', lw=0.5, alpha=0.5)
                ax_importance.set_xlabel("Spearman Correlation Coefficient", fontsize=9)
                ax_importance.invert_yaxis()
            else:
                ax_importance.text(0.5, 0.5, "No valid correlations calculated.", ha='center', va='center')
                ax_importance.axis('off')
        else:
             ax_importance.text(0.5, 0.5, "Not enough valid data for correlations.", ha='center', va='center')
             ax_importance.axis('off')
    else:
        ax_importance.text(0.5, 0.5, "RMSF or numeric features missing.", ha='center', va='center')
        ax_importance.axis('off')

    ax_ss_ce = plt.subplot(gs[1, 0:2])
    ax_ss_ce.set_title("Mean RMSF by SS Type & Location", fontsize=12)
    if ("secondary_structure_encoded" in avg_df.columns and
        "core_exterior" in avg_df.columns and
        "rmsf_average" in avg_df.columns):

        ss_map = {0: "Helix", 1: "Sheet", 2: "Coil"}
        plot_data_ssce = avg_df.copy()
        plot_data_ssce["SS_Type"] = plot_data_ssce["secondary_structure_encoded"].map(ss_map)
        plot_data_ssce["Location"] = plot_data_ssce["core_exterior"]

        plot_data_ssce["SS_Type"] = pd.Categorical(plot_data_ssce["SS_Type"], categories=["Helix", "Sheet", "Coil"], ordered=True)
        plot_data_ssce["Location"] = pd.Categorical(plot_data_ssce["Location"], categories=["core", "exterior"], ordered=True)
        plot_data_ssce.dropna(subset=["SS_Type", "Location", "rmsf_average"], inplace=True)

        if not plot_data_ssce.empty:
             pivot_data = plot_data_ssce.groupby(["SS_Type", "Location"], observed=False)["rmsf_average"].agg(["mean", "count"])
             pivot_mean = pivot_data['mean'].unstack(level='Location').reindex(["Helix", "Sheet", "Coil"]).reindex(columns=["core", "exterior"])
             pivot_count = pivot_data['count'].unstack(level='Location').reindex(["Helix", "Sheet", "Coil"]).reindex(columns=["core", "exterior"])

             sns.heatmap(pivot_mean, annot=True, fmt=".4f", cmap="viridis", ax=ax_ss_ce,
                       linewidths=.5, cbar_kws={'label': 'Mean RMSF (nm)'}, annot_kws={"size": 8})
             for i, ss_type in enumerate(pivot_mean.index):
                 for j, location in enumerate(pivot_mean.columns):
                     if ss_type in pivot_count.index and location in pivot_count.columns:
                          count = pivot_count.loc[ss_type, location]
                          mean_val = pivot_mean.loc[ss_type, location]
                          if pd.notna(count) and pd.notna(mean_val):
                               text_color = 'white' if mean_val > pivot_mean.values[~np.isnan(pivot_mean.values)].mean() else 'black'
                               ax_ss_ce.text(j + 0.5, i + 0.7, f"n={int(count):,}",
                                            ha='center', va='center', fontsize=7, color=text_color)

             ax_ss_ce.set_xlabel("Location", fontsize=9)
             ax_ss_ce.set_ylabel("Secondary Structure", fontsize=9)
        else:
             ax_ss_ce.text(0.5, 0.5, "No valid SS/Location data.", ha='center', va='center')
             ax_ss_ce.axis('off')
    else:
        ax_ss_ce.text(0.5, 0.5, "Required columns missing (SS_enc, core_ext, RMSF).", ha='center', va='center')
        ax_ss_ce.axis('off')

    ax_pos_access = plt.subplot(gs[1, 2])
    ax_pos_access.set_title("Accessibility vs Norm. Position", fontsize=12)
    if ("normalized_resid" in avg_df.columns and "relative_accessibility" in avg_df.columns):
        try:
             sample_df = avg_df.sample(min(5000, len(avg_df)), random_state=42)
             sns.scatterplot(data=sample_df, x="normalized_resid", y="relative_accessibility",
                             alpha=0.4, s=10, ax=ax_pos_access, color=palette[3])
             if STATSMODELS_AVAILABLE:
                 sns.regplot(data=sample_df, x="normalized_resid", y="relative_accessibility",
                             scatter=False, lowess=True,
                             ax=ax_pos_access, line_kws={'color': 'red', 'lw': 1})
             ax_pos_access.set_xlabel("Norm. Residue Position", fontsize=9)
             ax_pos_access.set_ylabel("Rel. Accessibility", fontsize=9)
             ax_pos_access.set_xlim(-0.05, 1.05)
             ax_pos_access.set_ylim(-0.05, 1.05)

        except Exception as e:
             logging.warning(f"Scatter/Regplot failed for Pos vs Access: {e}")
             ax_pos_access.text(0.5, 0.5, "Plotting failed.", ha='center', va='center')
             ax_pos_access.axis('off')
    else:
        ax_pos_access.text(0.5, 0.5, "Required columns missing (norm_resid, rel_access).", ha='center', va='center')
        ax_pos_access.axis('off')

    ax_phi_psi = plt.subplot(gs[2, 0:2])
    ax_phi_psi.set_title("RMSF vs Torsion Angles (Density)", fontsize=12)
    if ("phi_norm" in avg_df.columns and "psi_norm" in avg_df.columns and "rmsf_average" in avg_df.columns):
        plot_data_torsion = avg_df[['phi_norm', 'psi_norm', 'rmsf_average']].copy()
        plot_data_torsion.dropna(inplace=True)
        if len(plot_data_torsion) > 100:
            try:
                sns.kdeplot(data=plot_data_torsion, x="phi_norm", y="psi_norm",
                          fill=True, thresh=0.05, levels=5, cmap="viridis",
                          cbar=True, cbar_kws={'label': 'Density'}, ax=ax_phi_psi)
                ax_phi_psi.set_xlabel("Normalized Phi Angle (-1 to 1)", fontsize=9)
                ax_phi_psi.set_ylabel("Normalized Psi Angle (-1 to 1)", fontsize=9)
                ax_phi_psi.set_xlim(-1, 1)
                ax_phi_psi.set_ylim(-1, 1)
                ax_phi_psi.axhline(0, color='k', lw=0.5, ls=':')
                ax_phi_psi.axvline(0, color='k', lw=0.5, ls=':')
            except Exception as e:
                 logging.warning(f"KDE plot failed for Phi/Psi: {e}")
                 ax_phi_psi.text(0.5, 0.5, "KDE Plot Failed", ha='center', va='center')
                 ax_phi_psi.axis('off')
        else:
             ax_phi_psi.text(0.5, 0.5, "Not enough data points", ha='center', va='center')
             ax_phi_psi.axis('off')
    else:
        ax_phi_psi.text(0.5, 0.5, "Phi/Psi/RMSF data missing.", ha='center', va='center')
        ax_phi_psi.axis('off')

    ax_size_hist = plt.subplot(gs[2, 2])
    ax_size_hist.set_title("Protein Size Distribution", fontsize=12)
    if "protein_size" in avg_df.columns:
        protein_sizes = avg_df[['domain_id', 'protein_size']].drop_duplicates()['protein_size'].dropna()
        if not protein_sizes.empty:
            sns.histplot(protein_sizes, bins=30, kde=True, ax=ax_size_hist, color=palette[5])
            ax_size_hist.set_xlabel("Number of Residues", fontsize=9)
            ax_size_hist.set_ylabel("Number of Domains", fontsize=9)
        else:
             ax_size_hist.text(0.5, 0.5, "No Size Data", ha='center', va='center')
             ax_size_hist.axis('off')
    else:
        ax_size_hist.text(0.5, 0.5, "Protein size column missing.", ha='center', va='center')
        ax_size_hist.axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    output_path = os.path.join(vis_dir, "ml_features_additional_analysis.png")
    _save_plot(fig, vis_dir, "ml_features_additional_analysis.png", dpi)
    return output_path

# --- NEW PLOTS ---

def create_rmsf_density_plots(feature_df_average: pd.DataFrame,
                              output_dir: str,
                              viz_config: Dict[str, Any]) -> Optional[str]:
    """
    Create 2D density plots showing RMSF distribution against other features.

    Args:
        feature_df_average: DataFrame containing average features.
        output_dir: Base output directory.
        viz_config: Visualization config section.

    Returns:
        Path to the saved figure or None if creation fails.
    """
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE) # Use configured palette for cmap if desired
    _setup_plot_style(palette_name) # Set base style

    if feature_df_average is None or feature_df_average.empty:
        logging.warning("No average feature data available for RMSF density plots")
        return None

    required_cols = ["rmsf_average", "relative_accessibility", "normalized_resid"]
    if not all(col in feature_df_average.columns for col in required_cols):
        logging.warning(f"RMSF density plot requires columns: {required_cols}. Skipping.")
        return None

    avg_df = feature_df_average.copy()
    avg_df.dropna(subset=required_cols, inplace=True)

    if len(avg_df) < 50: # Need sufficient points for density estimation
        logging.warning("Not enough data points for RMSF density plots after dropping NaNs.")
        return None

    # Sample data for performance if dataframe is very large
    sample_df = avg_df.sample(min(10000, len(avg_df)), random_state=42)

    fig, axs = plt.subplots(1, 2, figsize=(16, 7))
    fig.suptitle("RMSF Density Analysis", fontsize=16, y=1.02)

    # Define a sequential colormap (e.g., viridis, plasma, magma)
    cmap = plt.get_cmap("viridis")

    # Panel 1: RMSF vs Relative Accessibility (Density colored by RMSF)
    try:
        # Use hexbin for density visualization colored by RMSF mean
        hb1 = axs[0].hexbin(sample_df["relative_accessibility"], sample_df["rmsf_average"],
                           C=sample_df["rmsf_average"], reduce_C_function=np.mean,
                           gridsize=40, cmap=cmap, mincnt=1) # mincnt=1 ensures bins with one point are shown
        cb1 = plt.colorbar(hb1, ax=axs[0])
        cb1.set_label('Mean RMSF (nm) in Bin')
        axs[0].set_title("RMSF vs Relative Accessibility (Density)")
        axs[0].set_xlabel("Relative Accessibility")
        axs[0].set_ylabel("RMSF (nm)")
        axs[0].set_xlim(0, 1)
        # Set ylim based on RMSF quantiles to avoid extreme outliers
        rmsf_q01, rmsf_q99 = sample_df["rmsf_average"].quantile([0.01, 0.99])
        axs[0].set_ylim(max(0, rmsf_q01 * 0.9), rmsf_q99 * 1.1)
    except Exception as e:
        logging.error(f"Failed to create RMSF vs Accessibility density plot: {e}")
        axs[0].text(0.5, 0.5, "Plot Failed", ha='center', va='center')
        axs[0].axis('off')

    # Panel 2: RMSF vs Normalized Position (Density colored by RMSF)
    try:
        hb2 = axs[1].hexbin(sample_df["normalized_resid"], sample_df["rmsf_average"],
                           C=sample_df["rmsf_average"], reduce_C_function=np.mean,
                           gridsize=40, cmap=cmap, mincnt=1)
        cb2 = plt.colorbar(hb2, ax=axs[1])
        cb2.set_label('Mean RMSF (nm) in Bin')
        axs[1].set_title("RMSF vs Normalized Position (Density)")
        axs[1].set_xlabel("Normalized Residue Position (N->C)")
        axs[1].set_ylabel("RMSF (nm)")
        axs[1].set_xlim(-0.05, 1.05)
        axs[1].set_ylim(max(0, rmsf_q01 * 0.9), rmsf_q99 * 1.1) # Use same y-lim as panel 1
    except Exception as e:
        logging.error(f"Failed to create RMSF vs Position density plot: {e}")
        axs[1].text(0.5, 0.5, "Plot Failed", ha='center', va='center')
        axs[1].axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.96])
    output_path = os.path.join(vis_dir, "rmsf_density_plots.png")
    _save_plot(fig, vis_dir, "rmsf_density_plots.png", dpi)
    return output_path


def create_rmsf_by_aa_ss_density(feature_df_average: pd.DataFrame,
                                 output_dir: str,
                                 viz_config: Dict[str, Any]) -> Optional[str]:
    """
    Create a ridgeline plot showing RMSF density distribution for major amino acids,
    separated by secondary structure type.

    Args:
        feature_df_average: DataFrame containing average features.
        output_dir: Base output directory.
        viz_config: Visualization config section.

    Returns:
        Path to the saved figure or None if creation fails.
    """
    vis_dir = os.path.join(output_dir, "visualizations")
    dpi = viz_config.get('dpi', DEFAULT_DPI)
    palette_name = viz_config.get('palette', DEFAULT_PALETTE)
    palette = _setup_plot_style(palette_name)

    if feature_df_average is None or feature_df_average.empty:
        logging.warning("No average feature data available for RMSF AA/SS density plot")
        return None

    required_cols = ["rmsf_average", "resname", "secondary_structure_encoded"]
    if not all(col in feature_df_average.columns for col in required_cols):
        logging.warning(f"RMSF AA/SS density plot requires columns: {required_cols}. Skipping.")
        return None

    avg_df = feature_df_average.copy()

    # Prepare data: Standardize HIS, map SS encoding, filter top AAs
    avg_df['resname'] = avg_df['resname'].replace({'HSD': 'HIS', 'HSE': 'HIS', 'HSP': 'HIS'})
    ss_map = {0: "Helix", 1: "Sheet", 2: "Coil"}
    avg_df['SS_Type'] = avg_df["secondary_structure_encoded"].map(ss_map).astype('category')

    # Get Top N amino acids (e.g., top 12)
    top_n_aa = 12
    top_aa = avg_df["resname"].value_counts().nlargest(top_n_aa).index.tolist()
    plot_df = avg_df[avg_df['resname'].isin(top_aa)].copy()

    # Set order for SS types and AAs (by overall mean RMSF)
    ss_order = ["Helix", "Sheet", "Coil"]
    plot_df['SS_Type'] = pd.Categorical(plot_df['SS_Type'], categories=ss_order, ordered=True)
    aa_order = plot_df.groupby('resname')['rmsf_average'].mean().sort_values().index.tolist()
    plot_df['resname'] = pd.Categorical(plot_df['resname'], categories=aa_order, ordered=True)

    plot_df.dropna(subset=['RMSF', 'resname', 'SS_Type'], inplace=True)

    if plot_df.empty:
        logging.warning("No data remaining for RMSF AA/SS density plot after filtering.")
        return None

    # Use FacetGrid for Ridgeline plot (requires seaborn >= 0.11 for ridgeplot)
    try:
        # Define a color palette for SS types
        ss_colors = {"Helix": palette[0], "Sheet": palette[1], "Coil": palette[2]}

        g = sns.FacetGrid(plot_df, row="resname", hue="SS_Type", aspect=5, height=1.2,
                          row_order=aa_order, hue_order=ss_order, palette=ss_colors)

        # Draw the densities in a few steps
        g.map(sns.kdeplot, "rmsf_average", bw_adjust=.5, clip_on=False, fill=True, alpha=0.7, linewidth=1.5)
        # Draw reference lines for each facet
        g.map(plt.axhline, y=0, linewidth=1, linestyle="-", color=None, clip_on=False)

        # Define and use a simple function to label the plot in axes coordinates
        def label(x, color, label):
            ax = plt.gca()
            ax.text(0.02, .2, label, fontweight="bold", color=color,
                    ha="left", va="center", transform=ax.transAxes)

        g.map(label, "rmsf_average")

        # Set the subplots to overlap
        g.figure.subplots_adjust(hspace=-.5)

        # Remove axes details that don't play well with overlap
        g.set_titles("")
        g.set(yticks=[], ylabel="")
        g.despine(bottom=True, left=True)

        g.fig.suptitle(f'RMSF Density Distribution by Secondary Structure (Top {top_n_aa} Amino Acids)', y=1.02)
        g.set_xlabels("RMSF (nm)")
        g.add_legend(title="SS Type")

        output_path = os.path.join(vis_dir, "rmsf_density_by_aa_ss.png")
        # Save using the FacetGrid figure
        g.figure.savefig(output_path, dpi=dpi, bbox_inches='tight')
        plt.close(g.figure)
        logging.info(f"Saved RMSF density by AA/SS plot: {output_path}")
        return output_path

    except Exception as e:
        logging.error(f"Failed to create RMSF AA/SS density plot: {e}", exc_info=True)
        # Clean up potentially open figure
        if 'g' in locals() and hasattr(g, 'figure'):
            plt.close(g.figure)
        return None

# Ensure all functions intended to be called by the executor exist:
# create_temperature_summary_heatmap
# create_temperature_average_summary
# create_rmsf_distribution_plots (violin + separated histograms)
# create_amino_acid_rmsf_plot
# create_amino_acid_rmsf_plot_colored
# create_replica_variance_plot
# create_dssp_rmsf_correlation_plot
# create_feature_correlation_plot
# create_frames_visualization (Placeholder)
# create_ml_features_plot
# create_summary_plot
# create_voxel_info_plot
# create_additional_ml_features_plot
# create_rmsf_density_plots (New)
# create_rmsf_by_aa_ss_density (New)
--- End of File: ./src/mdcath/visualize/plots.py ---

===== FILE: ./src/mdcath/visualize/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/cli.py =====
"""
Command-Line Interface (CLI) for the mdCATH processor pipeline.
Uses argparse to handle command-line arguments.
"""

import argparse
import logging
import os
import sys
import traceback # Import traceback
from .config import load_config, DEFAULT_CONFIG_PATH, ConfigurationError
from .pipeline.executor import PipelineExecutor, PipelineExecutionError
from .utils.logging_config import setup_logging

def parse_arguments() -> argparse.Namespace:
    """Parses command-line arguments."""
    parser = argparse.ArgumentParser(
        description="mdCATH Dataset Processor: Extracts features and generates outputs for ML.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(
        "-c", "--config",
        type=str,
        default=None,
        help="Path to the pipeline configuration YAML file."
    )
    parser.add_argument(
        "-o", "--output-dir",
        type=str,
        default=None,
        help="Path to the base output directory (overrides config)."
    )
    parser.add_argument(
        "-d", "--domains",
        nargs='*',
        default=None,
        help="List of specific domain IDs to process (overrides config)."
    )
    parser.add_argument(
        "--num-cores",
        type=int,
        default=None,
        help="Number of CPU cores for parallel processing (0=auto, 1=sequential, overrides config)."
    )
    parser.add_argument(
        "-v", "--verbose",
        action="count",
        default=0,
        help="Increase console logging verbosity (-v for INFO, -vv for DEBUG)."
    )

    return parser.parse_args()

def main():
    """Main CLI entry point."""
    args = parse_arguments()

    config = None # Initialize config to None
    try:
        # Load configuration first
        config = load_config(args.config)

        # --- Setup Logging based on args and config ---
        log_cfg = config.setdefault('logging', {})
        if args.verbose == 1:
            log_cfg['log_level_console'] = 'INFO'
        elif args.verbose >= 2:
            log_cfg['log_level_console'] = 'DEBUG'
        # setup_logging will clear existing handlers
        setup_logging(config=config)

        # --- Override config with CLI arguments ---
        if args.output_dir:
            config['output']['base_dir'] = args.output_dir
            logging.info(f"Overriding output directory with CLI argument: {args.output_dir}")
        # Check if args.domains is explicitly provided (not None)
        if args.domains is not None:
             # Handle case where no domains are given on CLI (empty list []) vs not using the arg (None)
             config['input']['domain_ids'] = args.domains if args.domains else [] # Use empty list if args.domains is []
             logging.info(f"Overriding domain list with CLI arguments: {config['input']['domain_ids'] or 'All Found'}")
        if args.num_cores is not None:
            config['performance']['num_cores'] = args.num_cores
            logging.info(f"Overriding number of cores with CLI argument: {args.num_cores}")

        # --- Execute Pipeline ---
        executor = PipelineExecutor(config_dict=config)
        executor.run()

    except PipelineExecutionError as e:
        logging.error(f"Pipeline execution failed: {e}")
        # WORKAROUND for logging TypeError
        tb_str = traceback.format_exc()
        logging.error(f"Traceback for PipelineExecutionError:\n{tb_str}")
        sys.exit(1)
    except ConfigurationError as e: # Catch config errors specifically
        # Config error might happen before logging is fully set up
        print(f"ERROR: Configuration Error - {e}", file=sys.stderr)
        # Log traceback if logging *might* be available and level is DEBUG
        if logging.getLogger().isEnabledFor(logging.DEBUG):
             tb_str = traceback.format_exc()
             logging.debug(f"Traceback for ConfigurationError:\n{tb_str}")
        sys.exit(1)
    except Exception as e:
        # WORKAROUND for logging TypeError
        logging.error(f"An unexpected error occurred: {e}")
        tb_str = traceback.format_exc()
        logging.error(f"Traceback for unexpected error:\n{tb_str}")
        sys.exit(1)
--- End of File: ./src/mdcath/cli.py ---

===== FILE: ./src/mdcath/io/writers.py =====
"""
Helper functions for writing processed data to files (CSV, PDB, etc.).
"""
import os
import pandas as pd
import logging

def save_dataframe_csv(df: pd.DataFrame, path: str, **kwargs):
    """
    Saves a Pandas DataFrame to a CSV file. Creates directory if needed.

    Args:
        df (pd.DataFrame): DataFrame to save.
        path (str): Full path to the output CSV file.
        **kwargs: Additional keyword arguments passed to df.to_csv().
    """
    if df is None or df.empty:
        logging.warning(f"Attempted to save an empty DataFrame to {path}. Skipping.")
        return
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        if 'index' not in kwargs:
            kwargs['index'] = False
        df.to_csv(path, **kwargs)
        logging.debug(f"Successfully saved DataFrame ({len(df)} rows) to {path}")
    except Exception as e:
        logging.error(f"Failed to save DataFrame to {path}: {e}", exc_info=True)
        # Do not re-raise here, allow pipeline to potentially continue

def save_string(text: str, path: str):
    """
    Saves a string to a text file. Creates directory if needed.

    Args:
        text (str): String content to save.
        path (str): Full path to the output text file.
    """
    if not text:
         logging.warning(f"Attempted to save empty string to {path}. Skipping.")
         return
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w') as f:
            f.write(text)
        logging.debug(f"Successfully saved string ({len(text)} chars) to {path}")
    except Exception as e:
        logging.error(f"Failed to save string to {path}: {e}", exc_info=True)
        # Do not re-raise here


# Updated path construction based on prompt refinement
def get_rmsf_output_path(output_base: str, type: str, flatten: bool, **kwargs) -> str:
    """
    Constructs the output path for RMSF files based on flattening config.

    Args:
        output_base (str): Base output directory (e.g., './outputs').
        type (str): 'replica' or 'average'.
        flatten (bool): Whether to flatten the directory structure within type.
        **kwargs: Must contain 'temperature', 'replica' for type='replica'.
                  Must contain 'temperature' for type='average' temp-specific file.
                  Can be empty for type='average' overall file.

    Returns:
        str: The constructed file path.
    """
    rmsf_root = os.path.join(output_base, "RMSF")

    if type == 'replica':
        if 'replica' not in kwargs or 'temperature' not in kwargs:
             raise ValueError("Missing 'replica' or 'temperature' for RMSF type 'replica'")
        replica = kwargs['replica']
        temp = kwargs['temperature']
        filename = f"rmsf_replica{replica}_temperature{temp}.csv"
        # Prompt specified flattening *within* replica folder
        # outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature320.csv
        # outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature348.csv
        replica_dir = os.path.join(rmsf_root, "replicas", f"replica_{replica}")
        path = os.path.join(replica_dir, filename)
        return path

    elif type == 'average':
        temp = kwargs.get('temperature')
        avg_dir = os.path.join(rmsf_root, "replica_average")
        if temp: # Temperature-specific average
            filename = f"rmsf_replica_average_temperature{temp}.csv"
             # Prompt specified flattening directly into replica_average/
             # outputs/RMSF/replica_average/rmsf_replica_average_temperature320.csv
            path = os.path.join(avg_dir, filename)
        else: # Overall average
            filename = "rmsf_all_temperatures_all_replicas.csv"
            # outputs/RMSF/replica_average/rmsf_all_temperatures_all_replicas.csv
            path = os.path.join(avg_dir, filename)
        return path
    else:
        raise ValueError(f"Invalid RMSF type specified: {type}")


--- End of File: ./src/mdcath/io/writers.py ---

===== FILE: ./src/mdcath/io/hdf5_reader.py =====
"""
Module for reading data selectively from mdCATH HDF5 files.
"""
import h5py
import logging
import os
import numpy as np
import pandas as pd
from typing import Optional, List, Tuple, Any, Dict

class HDF5ReaderError(Exception):
    """Custom exception for HDF5Reader errors."""
    pass

class HDF5Reader:
    """
    Reads data from a single mdCATH HDF5 file.

    Args:
        h5_path (str): Path to the HDF5 file.

    Raises:
        FileNotFoundError: If the HDF5 file does not exist.
        HDF5ReaderError: If the file is invalid or key datasets are missing.
    """
    def __init__(self, h5_path: str):
        if not os.path.exists(h5_path):
            raise FileNotFoundError(f"HDF5 file not found: {h5_path}")
        self.h5_path = h5_path
        self.domain_id = self._extract_domain_id()
        self._validate_file() # Basic validation on init

    def _extract_domain_id(self) -> str:
        """Extracts domain ID from the filename."""
        basename = os.path.basename(self.h5_path)
        if basename.startswith("mdcath_dataset_") and basename.endswith(".h5"):
            return basename[len("mdcath_dataset_"):-len(".h5")]
        else:
            logging.warning(f"Could not parse standard domain ID from filename: {basename}. Using filename root.")
            return os.path.splitext(basename)[0]

    def _validate_file(self):
        """Performs basic validation of the HDF5 file structure."""
        try:
            with h5py.File(self.h5_path, 'r') as f:
                if self.domain_id not in f:
                    raise HDF5ReaderError(f"Domain ID group '{self.domain_id}' not found in {self.h5_path}")
                # Check for essential base datasets
                required_base = ['resid', 'resname', 'pdb']
                for req in required_base:
                    if req not in f[self.domain_id]:
                         raise HDF5ReaderError(f"Required base dataset '{req}' not found for domain {self.domain_id}")

                # Check for at least one temperature group (basic check)
                temp_groups = [key for key in f[self.domain_id].keys() if key.isdigit()]
                if not temp_groups:
                     logging.warning(f"No temperature groups found for domain {self.domain_id} in {self.h5_path}")
                else:
                     # Check structure within first temp/replica found
                     first_temp = temp_groups[0]
                     rep_groups = [key for key in f[f"{self.domain_id}/{first_temp}"].keys() if key.isdigit()]
                     if not rep_groups:
                          logging.warning(f"No replica groups found under temp {first_temp} for domain {self.domain_id}")
                     else:
                          first_rep = rep_groups[0]
                          rep_path = f"{self.domain_id}/{first_temp}/{first_rep}"
                          required_rep_data = ['coords', 'rmsf'] # Check essential dynamics data
                          for req_data in required_rep_data:
                              if req_data not in f[rep_path]:
                                   logging.warning(f"Required dataset '{req_data}' not found under {rep_path}. Processing may fail.")


        except HDF5ReaderError:
             raise # Propagate validation errors
        except Exception as e:
            logging.error(f"Failed to open or validate HDF5 file {self.h5_path}: {e}")
            raise HDF5ReaderError(f"HDF5 file access error: {e}") from e


    def get_pdb_string(self) -> Optional[str]:
        """Reads the PDB string from the HDF5 file."""
        path = f"{self.domain_id}/pdb"
        try:
            with h5py.File(self.h5_path, 'r') as f:
                if path not in f:
                     logging.error(f"'pdb' dataset not found for domain {self.domain_id} in {self.h5_path}")
                     return None
                pdb_dataset = f[path]
                pdb_data = pdb_dataset[()] # Read the scalar dataset
                if isinstance(pdb_data, bytes):
                    return pdb_data.decode('utf-8')
                elif isinstance(pdb_data, str):
                    return pdb_data
                else:
                     logging.warning(f"Unexpected data type for PDB string in {self.domain_id}: {type(pdb_data)}")
                     return str(pdb_data) # Attempt conversion
        except Exception as e:
            logging.error(f"Error reading PDB string for {self.domain_id}: {e}")
            return None

    def get_residue_info(self) -> Optional[pd.DataFrame]:
        """Reads unique residue number and name info."""
        try:
            with h5py.File(self.h5_path, 'r') as f:
                domain_group = f[self.domain_id]
                if 'resid' not in domain_group or 'resname' not in domain_group:
                     logging.error(f"Required residue datasets ('resid', 'resname') not found for {self.domain_id}")
                     return None

                resids = domain_group['resid'][:]
                resnames_raw = domain_group['resname'][:]

                # Create a minimal per-atom DataFrame to find unique residues
                # Assumption: first occurrence of a resid corresponds to its resname for the whole residue
                temp_df = pd.DataFrame({'resid': resids, 'resname_raw': resnames_raw})
                unique_residues_df = temp_df.drop_duplicates(subset='resid', keep='first').reset_index(drop=True)

                # Decode bytes if necessary
                unique_residues_df['resname'] = unique_residues_df['resname_raw'].apply(
                    lambda rn: rn.decode('utf-8') if isinstance(rn, bytes) else str(rn)
                )
                unique_residues_df = unique_residues_df[['resid', 'resname']] # Keep only needed columns

                # Get numResidues attribute if available for cross-check
                num_residues_attr = domain_group.attrs.get('numResidues')
                if num_residues_attr is not None and len(unique_residues_df) != num_residues_attr:
                     logging.warning(f"Number of unique residues found ({len(unique_residues_df)}) "
                                     f"does not match '.numResidues' attribute ({num_residues_attr}) for {self.domain_id}.")

                if unique_residues_df.empty:
                     logging.error(f"No unique residues found for domain {self.domain_id}.")
                     return None

                # Ensure resid is numeric
                unique_residues_df['resid'] = pd.to_numeric(unique_residues_df['resid'])

                return unique_residues_df

        except Exception as e:
            logging.error(f"Error reading residue info for {self.domain_id}: {e}", exc_info=True)
            return None

    def get_rmsf(self, temperature: int, replica: int) -> Optional[np.ndarray]:
        """Reads RMSF data for a specific temperature and replica."""
        path = f"{self.domain_id}/{temperature}/{replica}/rmsf"
        try:
            with h5py.File(self.h5_path, 'r') as f:
                if path not in f:
                     logging.warning(f"RMSF data not found at path: {path} in {self.h5_path}")
                     return None
                rmsf_data = f[path][:]
                # Basic shape check: should be 1D
                if rmsf_data.ndim != 1:
                     logging.warning(f"RMSF data at {path} is not 1D (shape: {rmsf_data.shape}). Attempting to flatten.")
                     rmsf_data = rmsf_data.flatten()
                return rmsf_data
        except Exception as e:
            logging.error(f"Error reading RMSF from {path} in {self.h5_path}: {e}")
            return None

    def get_coordinates(self, temperature: int, replica: int, frame_index: int = -1) -> Optional[np.ndarray]:
        """
        Reads coordinate data for a specific temperature, replica, and frame.

        Args:
            temperature (int): Simulation temperature.
            replica (int): Replica index.
            frame_index (int): Frame index to read (-1 for the last frame).
                                Use -999 to read ALL frames.

        Returns:
            Optional[np.ndarray]: Coordinates array (n_atoms, 3) for a single frame,
                                  or (n_frames, n_atoms, 3) if frame_index is -999,
                                  or None if reading fails.
        """
        path = f"{self.domain_id}/{temperature}/{replica}/coords"
        try:
            with h5py.File(self.h5_path, 'r') as f:
                if path not in f:
                    logging.warning(f"Coordinates data not found at path: {path} in {self.h5_path}")
                    return None

                coords_dataset = f[path]
                if coords_dataset.ndim != 3 or coords_dataset.shape[1] == 0 or coords_dataset.shape[2] != 3:
                     logging.error(f"Invalid coordinates shape {coords_dataset.shape} at {path}")
                     return None

                num_frames = coords_dataset.shape[0]
                if num_frames == 0:
                    logging.warning(f"Empty coordinates dataset at path: {path}")
                    return None

                if frame_index == -999: # Read all frames
                    coords_data = coords_dataset[:]
                else:
                    actual_index = frame_index if frame_index >= 0 else num_frames + frame_index
                    if 0 <= actual_index < num_frames:
                        coords_data = coords_dataset[actual_index]
                    else:
                        logging.error(f"Frame index {frame_index} (resolved to {actual_index}) out of bounds "
                                      f"(0-{num_frames-1}) for path {path}")
                        return None
                return coords_data
        except Exception as e:
            logging.error(f"Error reading coordinates from {path} in {self.h5_path}: {e}")
            return None

    def get_scalar_traj(self, temperature: int, replica: int, dataset_name: str) -> Optional[np.ndarray]:
        """Reads scalar trajectory data like 'rmsd' or 'gyrationRadius'."""
        path = f"{self.domain_id}/{temperature}/{replica}/{dataset_name}"
        try:
            with h5py.File(self.h5_path, 'r') as f:
                if path not in f:
                     logging.warning(f"Dataset '{dataset_name}' not found at path: {path} in {self.h5_path}")
                     return None
                data = f[path][:]
                # Ensure it's reasonably 1D
                if data.ndim > 1 and not (data.ndim == 2 and data.shape[1] == 1):
                     logging.warning(f"Expected scalar trajectory for '{dataset_name}' but got shape {data.shape}. Flattening.")
                     data = data.flatten()
                elif data.ndim == 0: # Handle scalar case if possible
                     data = np.array([data])

                # Basic check for expected length based on coords if possible
                # num_frames = self.get_num_frames(temperature, replica)
                # if num_frames is not None and len(data) != num_frames:
                #      logging.warning(f"Length mismatch for '{dataset_name}' ({len(data)}) vs num_frames ({num_frames}) at {path}")

                return data
        except Exception as e:
            logging.error(f"Error reading dataset {dataset_name} from {path} in {self.h5_path}: {e}")
            return None

    def get_num_frames(self, temperature: int, replica: int) -> Optional[int]:
         """Gets the number of frames attribute for a specific replica."""
         path = f"{self.domain_id}/{temperature}/{replica}"
         try:
             with h5py.File(self.h5_path, 'r') as f:
                 if path not in f:
                     logging.warning(f"Replica group not found at path: {path} in {self.h5_path}")
                     return None
                 # Try reading attribute '.numFrames'
                 num_frames_attr = f[path].attrs.get('.numFrames')
                 if num_frames_attr is not None:
                      return int(num_frames_attr)
                 else:
                      # Fallback: read shape of coords dataset if attribute missing
                      coords_path = f"{path}/coords"
                      if coords_path in f and f[coords_path].ndim == 3:
                           return f[coords_path].shape[0]
                      else:
                           logging.warning(f"Could not determine number of frames for {path}")
                           return None
         except Exception as e:
            logging.error(f"Error getting numFrames from {path} in {self.h5_path}: {e}")
            return None


    def get_available_temperatures(self) -> List[int]:
        """Returns a list of available temperatures (as integers) found in the file."""
        temps = []
        try:
            with h5py.File(self.h5_path, 'r') as f:
                if self.domain_id in f:
                    for key in f[self.domain_id].keys():
                        if key.isdigit():
                            temps.append(int(key))
            return sorted(temps)
        except Exception as e:
            logging.error(f"Error listing temperatures for {self.domain_id}: {e}")
            return []

    def get_available_replicas(self, temperature: int) -> List[int]:
         """Returns a list of available replicas (as integers) for a given temperature."""
         reps = []
         temp_path = f"{self.domain_id}/{temperature}"
         try:
             with h5py.File(self.h5_path, 'r') as f:
                 if temp_path in f:
                      for key in f[temp_path].keys():
                           if key.isdigit():
                                reps.append(int(key))
             return sorted(reps)
         except Exception as e:
            logging.error(f"Error listing replicas for {self.domain_id}, temp {temperature}: {e}")
            return []


--- End of File: ./src/mdcath/io/hdf5_reader.py ---

===== FILE: ./src/mdcath/io/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/utils/logging_config.py =====
"""
Logging configuration for the mdCATH processor.
"""
import logging
import os
import sys
from typing import Optional, Dict, Any

def setup_logging(log_level_console: str = 'INFO',
                  log_level_file: str = 'DEBUG',
                  log_file: Optional[str] = None,
                  config: Optional[Dict[str, Any]] = None):
    """
    Sets up logging for the application.

    Args:
        log_level_console (str): Logging level for console output (e.g., 'INFO', 'DEBUG').
        log_level_file (str): Logging level for file output.
        log_file (Optional[str]): Path to the log file. If None, file logging is disabled.
        config (Optional[Dict[str, Any]]): If provided, overrides default levels and file path
                                            using keys from the 'logging' section.
    """
    # Get settings from config if provided
    log_filename_from_config = 'pipeline.log' # Default filename
    if config and 'logging' in config:
        log_cfg = config['logging']
        log_level_console = log_cfg.get('log_level_console', log_level_console).upper()
        log_level_file = log_cfg.get('log_level_file', log_level_file).upper()
        log_filename_from_config = log_cfg.get('log_filename', log_filename_from_config) # Use configured name
        # Construct log file path relative to output base directory
        output_base_dir = config.get('output', {}).get('base_dir', '.')
        log_dir = os.path.join(output_base_dir, 'logs')
        log_file = os.path.join(log_dir, log_filename_from_config)
    else:
        # Convert default levels to uppercase
        log_level_console = log_level_console.upper()
        log_level_file = log_level_file.upper()
        # If no config and no log_file path provided, disable file logging
        if log_file:
             # Assume log_file path is absolute or relative to current dir if no config
             log_dir = os.path.dirname(log_file)
        else:
             log_dir = None # Disable file logging

    # --- Configure Root Logger ---
    # Get the root logger
    root_logger = logging.getLogger()

    # *** Forcefully clear existing handlers to prevent conflicts/duplicates ***
    if root_logger.hasHandlers():
        # print("DEBUG: Clearing existing logging handlers.") # Optional: for debugging setup
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
            handler.close() # Close handler to release file locks etc.

    # Set root logger level - set to lowest level desired across all handlers
    min_level_val = min(getattr(logging, log_level_console, logging.INFO),
                        getattr(logging, log_level_file, logging.DEBUG))
    root_logger.setLevel(min_level_val) # Capture everything up to the lowest level needed

    # Basic formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    date_formatter = logging.Formatter('%(levelname)s: %(message)s') # Even simpler format for console

    # --- Console Handler ---
    console_handler = logging.StreamHandler(sys.stdout)
    try:
        console_level_val = getattr(logging, log_level_console, logging.INFO)
    except AttributeError:
        print(f"Warning: Invalid console log level '{log_level_console}'. Defaulting to INFO.")
        console_level_val = logging.INFO
    console_handler.setLevel(console_level_val)
    console_handler.setFormatter(date_formatter) # Use simpler format for console
    root_logger.addHandler(console_handler)

    # --- File Handler ---
    if log_file and log_dir: # Ensure both log_file and log_dir are set
        try:
            os.makedirs(log_dir, exist_ok=True)
            # Use 'a' for append or 'w' for overwrite
            file_handler = logging.FileHandler(log_file, mode='w') # Overwrite log each run
            try:
                file_level_val = getattr(logging, log_level_file, logging.DEBUG)
            except AttributeError:
                 print(f"Warning: Invalid file log level '{log_level_file}'. Defaulting to DEBUG.")
                 file_level_val = logging.DEBUG
            file_handler.setLevel(file_level_val)
            file_handler.setFormatter(formatter) # Use more detailed format for file
            root_logger.addHandler(file_handler)
            # Use print for this initial message as logging might not be fully ready
            print(f"Logging to file: {log_file}")
        except Exception as e:
            # Use print here as logging might be failing
            print(f"ERROR: Failed to set up file handler for {log_file}: {e}")
            # Optionally, print traceback if needed for debugging setup itself
            # import traceback
            # print(traceback.format_exc())
            # Continue with console logging only
    else:
        print("File logging disabled (no log file path configured).")

    # print("DEBUG: Logging setup complete.") # Optional: for debugging setup
--- End of File: ./src/mdcath/utils/logging_config.py ---

===== FILE: ./src/mdcath/utils/parallel.py =====
"""
Parallel processing helper functions.
Includes robust error logging for pickling issues.
"""
import logging
import concurrent.futures
from tqdm import tqdm
from typing import Callable, Iterable, List, Any, Optional
import traceback # Import traceback module

def parallel_map(func: Callable,
                 items: Iterable,
                 num_cores: int,
                 use_progress_bar: bool = True,
                 desc: Optional[str] = None,
                 chunksize: int = 1) -> List[Any]:
    """
    Applies a function to items in parallel using ProcessPoolExecutor.

    Args:
        func (Callable): The function to apply (must be pickleable).
        items (Iterable): The iterable of items to process.
        num_cores (int): The number of worker processes. If 1, runs sequentially.
        use_progress_bar (bool): Display tqdm progress bar.
        desc (Optional[str]): Description for progress bar.
        chunksize (int): Chunksize for ProcessPoolExecutor.map.

    Returns:
        List[Any]: List of results or raises RuntimeError on failure.
    """
    if num_cores < 1:
        logging.warning("parallel_map called with num_cores < 1. Running sequentially.")
        num_cores = 1

    results = []
    item_list = []
    try:
         item_list = list(items)
         total_items = len(item_list)
    except Exception as e:
         logging.error(f"Input 'items' must be a finite iterable (like a list): {e}")
         raise TypeError("Input 'items' must be a finite iterable") from e

    if total_items == 0: return [] # Nothing to process

    effective_cores = min(num_cores, total_items)

    if effective_cores == 1:
        logging.info("Running sequentially (num_cores=1 or only 1 item).")
        progress_iterator = tqdm(item_list, desc=desc or "Processing", disable=not use_progress_bar, total=total_items)
        try:
            for item in progress_iterator:
                results.append(func(item)) # Call the function directly
        except Exception as e:
             # Apply robust logging fix for sequential mode error
             logging.error(f"Error during sequential processing in parallel_map helper: {e}")
             if logging.getLogger().isEnabledFor(logging.DEBUG):
                  logging.debug(f"Traceback:\n{traceback.format_exc()}")
             raise RuntimeError("Sequential processing failed") from e # Re-raise
    else:
        logging.info(f"Running in parallel with {effective_cores} cores (chunksize={chunksize}).")
        try:
            with concurrent.futures.ProcessPoolExecutor(max_workers=effective_cores) as executor:
                future_iterator = executor.map(func, item_list, chunksize=chunksize)
                progress_iterator = tqdm(future_iterator, desc=desc or "Processing", total=total_items, disable=not use_progress_bar)
                results = list(progress_iterator) # Collect results

        except Exception as e:
            # Apply robust logging fix for parallel mode error (like PicklingError)
            logging.error(f"Error during parallel processing in parallel_map helper: {e}")
            if logging.getLogger().isEnabledFor(logging.DEBUG):
                 # Log traceback explicitly using traceback module
                 logging.debug(f"Traceback:\n{traceback.format_exc()}")
            # Re-raise a more informative error
            raise RuntimeError(f"Parallel processing failed: {e}") from e

    return results
--- End of File: ./src/mdcath/utils/parallel.py ---

===== FILE: ./src/mdcath/utils/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/config.py =====
"""
Configuration loading and validation for the pipeline.
"""
import yaml
import os
import logging
from typing import Dict, Any, Optional

# Define potential default config locations
DEFAULT_CONFIG_NAME = 'default_config.yaml'
# Assume config dir is sibling to src dir if run from root, or relative to this file
CONFIG_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'config')
DEFAULT_CONFIG_PATH = os.path.join(CONFIG_DIR, DEFAULT_CONFIG_NAME)


class ConfigurationError(Exception):
    """Custom exception for configuration errors."""
    pass


def _find_config_file(config_path: Optional[str] = None) -> Optional[str]:
    """Finds the configuration file path."""
    if config_path:
        if os.path.exists(config_path):
            return config_path
        else:
            raise ConfigurationError(f"Specified config file not found: {config_path}")
    elif os.path.exists(DEFAULT_CONFIG_PATH):
         return DEFAULT_CONFIG_PATH
    elif os.path.exists(os.path.join('config', DEFAULT_CONFIG_NAME)): # Check relative to current dir
         return os.path.join('config', DEFAULT_CONFIG_NAME)
    else:
         return None # Let caller handle missing default


def load_config(config_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Loads the YAML configuration file.

    Args:
        config_path (Optional[str]): Path to the config file. If None, tries default locations.

    Returns:
        Dict[str, Any]: The loaded configuration dictionary.

    Raises:
        ConfigurationError: If the config file cannot be found or parsed, or fails validation.
    """
    actual_config_path = _find_config_file(config_path)
    if not actual_config_path:
        raise ConfigurationError("Configuration file not found. Provide path via --config or place default_config.yaml in ./config/")

    logging.info(f"Loading configuration from: {actual_config_path}")
    try:
        with open(actual_config_path, 'r') as f:
            config_data = yaml.safe_load(f)
        if not isinstance(config_data, dict):
            raise ConfigurationError("Configuration file is not a valid YAML dictionary.")

        _validate_config(config_data, actual_config_path) # Basic validation
        return config_data
    except yaml.YAMLError as e:
        raise ConfigurationError(f"Error parsing YAML file {actual_config_path}: {e}") from e
    except Exception as e:
         # Catch other potential errors like permissions
         raise ConfigurationError(f"Error reading configuration file {actual_config_path}: {e}") from e

def _validate_config(config: Dict[str, Any], path: str):
    """Performs basic validation checks on the loaded config."""
    logging.debug("Validating configuration...")
    # 1. Check required top-level keys
    required_keys = ['input', 'output', 'processing', 'performance', 'logging', 'visualization']
    for key in required_keys:
        if key not in config:
            raise ConfigurationError(f"Missing required top-level key '{key}' in config file: {path}")

    # 2. Check 'input.mdcath_folder' existence (if not using explicit domain list)
    input_cfg = config['input']
    if not input_cfg.get('domain_ids'): # If domain_ids is null or empty list
        folder = input_cfg.get('mdcath_folder')
        if not folder or not os.path.isdir(folder):
             raise ConfigurationError(f"'input.mdcath_folder' ({folder}) is not specified or not a valid directory in config: {path}, "
                                     "and 'input.domain_ids' is not provided.")

    # 3. Check 'output.base_dir'
    output_dir = config['output'].get('base_dir')
    if not output_dir:
         raise ConfigurationError(f"Missing required key 'output.base_dir' in config file: {path}")
    # Check if parent dir is writable? Might be too strict. Let makedirs handle it later.


    # Add more specific checks as needed (e.g., valid enum values for methods)
    proc_cfg = config['processing']
    frame_method = proc_cfg.get('frame_selection', {}).get('method')
    valid_frame_methods = ['regular', 'rmsd', 'gyration', 'random', 'last']
    if frame_method not in valid_frame_methods:
         logging.warning(f"Invalid 'processing.frame_selection.method': {frame_method}. Should be one of {valid_frame_methods}. Defaulting behavior might occur.")

    logging.debug("Configuration validation passed (basic checks).")


# Modify PipelineExecutor to accept config dict directly
# Add this method or modify __init__ in pipeline/executor.py
def __init__(self, config_dict: Dict[str, Any]):
        """
        Initializes the executor with a configuration dictionary.

        Args:
            config_dict (Dict[str, Any]): The loaded configuration dictionary.
        """
        try:
            self.config = config_dict # Store the loaded config
            self.output_dir = self.config.get('output', {}).get('base_dir', './outputs')
            # ... rest of initialization as before ...

        except Exception as e:
            logging.exception(f"Failed to initialize PipelineExecutor with config dict: {e}")
            raise PipelineExecutionError(f"Initialization failed: {e}") from e

# Make sure PipelineExecutor.__init__ in executor.py reflects this change
# Update the call in cli.py: executor = PipelineExecutor(config_dict=config)


--- End of File: ./src/mdcath/config.py ---

===== FILE: ./src/mdcath/voxel/aposteriori_wrapper.py =====
"""
Module to wrap and execute the external 'aposteriori' (make-frame-dataset) tool.
"""
import os
import logging
import subprocess
import sys
from typing import Dict, Any, List, Optional

class VoxelizationError(Exception):
    """Custom exception for Voxelization errors."""
    pass

def run_aposteriori(config: Dict[str, Any], output_base_dir: str, cleaned_pdb_dir: str) -> bool:
    """
    Runs the 'make-frame-dataset' command from aposteriori.

    Args:
        config (Dict[str, Any]): The 'voxelization' section of the main config.
        output_base_dir (str): The base output directory (e.g., './outputs').
        cleaned_pdb_dir (str): Path to the directory containing cleaned PDB files.

    Returns:
        bool: True if execution was successful (or skipped), False on failure.

    Raises:
        VoxelizationError: If configuration is invalid or execution fails critically.
    """
    if not config.get("enabled", True):
        logging.info("Voxelization step is disabled in the configuration. Skipping.")
        return True

    # --- Check Inputs ---
    if not os.path.isdir(cleaned_pdb_dir) or not os.listdir(cleaned_pdb_dir):
         logging.warning(f"Cleaned PDB directory '{cleaned_pdb_dir}' is empty or not found. Cannot run voxelization.")
         # Consider if this should be an error or just a skippable condition
         return True # Skip if no input PDBs

    # --- Find Executable ---
    aposteriori_cmd = config.get("aposteriori_executable") or "make-frame-dataset"
    aposteriori_path = shutil.which(aposteriori_cmd)
    if not aposteriori_path:
         raise VoxelizationError(f"Aposteriori command '{aposteriori_cmd}' not found in PATH. "
                                f"Install aposteriori ('pip install aposteriori') or provide full path in config.")
    logging.info(f"Using aposteriori executable: {aposteriori_path}")

    # --- Prepare Arguments ---
    voxel_output_dir = os.path.join(output_base_dir, "voxelized")
    os.makedirs(voxel_output_dir, exist_ok=True)

    output_name = config.get("output_name", "mdcath_voxelized")
    # Output path for aposteriori (it appends .hdf5 itself)
    output_path_base = os.path.join(voxel_output_dir, output_name)
    final_output_hdf5 = output_path_base + ".hdf5" # Expected final file

    cmd_args = [
        aposteriori_path,
        "-o", voxel_output_dir,
        "-n", output_name,
        "-e", ".pdb", # Target cleaned PDB files
        "--frame-edge-length", str(config.get("frame_edge_length", 12.0)),
        "--voxels-per-side", str(config.get("voxels_per_side", 21)),
        # Atom encoder is required by aposteriori
        "-ae", config.get("atom_encoder", "CNOCBCA"),
    ]

    # Add boolean flags carefully (aposteriori expects 'true'/'false' strings often)
    if config.get("encode_cb", True): cmd_args.extend(["-cb", "true"])
    else: cmd_args.extend(["-cb", "false"])

    if config.get("compression_gzip", True): cmd_args.extend(["-comp", "true"])
    else: cmd_args.extend(["-comp", "false"])

    if config.get("voxelise_all_states", False): cmd_args.extend(["-vas", "true"])
    else: cmd_args.extend(["-vas", "false"])

    # Add verbosity based on logging level? Simple -v for now.
    cmd_args.append("-v")

    # Add the input directory LAST
    cmd_args.append(cleaned_pdb_dir)

    # --- Execute Command ---
    logging.info(f"Running Aposteriori: {' '.join(cmd_args)}")
    try:
        # Aposteriori asks for confirmation 'y/n'
        # Use Popen to send 'y\n' to stdin
        process = subprocess.Popen(
            cmd_args,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding='utf-8'
        )

        # Send 'y' followed by newline to confirm
        stdout, stderr = process.communicate(input='y\n', timeout=3600) # Add timeout (e.g., 1 hour)

        logging.debug(f"Aposteriori stdout:\n{stdout}")
        if stderr:
             # Log stderr as warning or error depending on content/return code
             if process.returncode != 0:
                  logging.error(f"Aposteriori stderr:\n{stderr}")
             else:
                  logging.warning(f"Aposteriori stderr:\n{stderr}") # Log as warning if exit code is 0


        if process.returncode != 0:
            raise VoxelizationError(f"Aposteriori command failed with exit code {process.returncode}. See logs for details.")

        # --- Verify Output ---
        if not os.path.exists(final_output_hdf5):
            # Sometimes aposteriori might name it slightly differently? Check common variations.
            # Or just rely on the error code above.
            raise VoxelizationError(f"Aposteriori finished but expected output file '{final_output_hdf5}' was not found.")

        logging.info(f"Aposteriori voxelization completed successfully. Output: {final_output_hdf5}")
        return True

    except subprocess.TimeoutExpired:
         logging.error("Aposteriori command timed out.")
         process.kill()
         stdout, stderr = process.communicate()
         logging.error(f"Aposteriori stdout (on timeout):\n{stdout}")
         logging.error(f"Aposteriori stderr (on timeout):\n{stderr}")
         return False
    except Exception as e:
        logging.error(f"An error occurred during aposteriori execution: {e}", exc_info=True)
        # Ensure process is terminated if it exists and is running
        if 'process' in locals() and process.poll() is None:
             process.kill()
        return False

import shutil # Import missing module



--- End of File: ./src/mdcath/voxel/aposteriori_wrapper.py ---

===== FILE: ./src/mdcath/voxel/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/exceptions.py =====
"""
Custom exception classes for the mdCATH processor package.
Allows for more specific error handling.
"""

class MdCathProcessorError(Exception):
    """Base exception for errors in the mdcath-processor package."""
    pass

class ConfigurationError(MdCathProcessorError):
    """Exception raised for errors in configuration loading or validation."""
    pass

class HDF5ReaderError(MdCathProcessorError):
    """Custom exception for HDF5Reader errors."""
    pass

class PDBProcessorError(MdCathProcessorError):
    """Custom exception for PDBProcessor errors."""
    pass

class PropertiesCalculatorError(MdCathProcessorError):
    """Custom exception for PropertiesCalculator errors."""
    pass

class RmsfProcessingError(MdCathProcessorError):
    """Custom exception for RMSF processing errors."""
    pass

class FeatureBuilderError(MdCathProcessorError):
    """Custom exception for FeatureBuilder errors."""
    pass

class VoxelizationError(MdCathProcessorError):
    """Custom exception for Voxelization errors."""
    pass

class VisualizationError(MdCathProcessorError):
    """Custom exception for Visualization errors."""
    pass

class PipelineExecutionError(MdCathProcessorError):
    """Exception raised for errors during the main pipeline execution flow."""
    pass

--- End of File: ./src/mdcath/exceptions.py ---

===== FILE: ./src/mdcath/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/features/builder.py =====
"""
Module for building the final ML-ready feature set.
Combines RMSF, structural properties, and calculates derived features.
Performs critical alignment validation.
"""
import logging
import pandas as pd
import numpy as np
from typing import Dict, Optional, Any, Tuple

class FeatureBuilderError(Exception):
    """Custom exception for FeatureBuilder errors."""
    pass

class FeatureBuilder:
    """
    Builds ML feature DataFrames by combining processed data sources.

    Args:
        config (Dict[str, Any]): The 'feature_building' section of the main config.
        replica_avg_rmsf (Dict[str, pd.DataFrame]): Dict mapping temp_str to DataFrame
                                                    containing domain_id, resid, resname, rmsf_{temp}.
        overall_avg_rmsf (Optional[pd.DataFrame]): DataFrame with domain_id, resid,
                                                    resname, rmsf_average.
        structure_properties (Dict[str, pd.DataFrame]): Dict mapping domain_id to DataFrame
                                                        from PropertiesCalculator.
    """
    def __init__(self,
                 config: Dict[str, Any],
                 replica_avg_rmsf: Dict[str, pd.DataFrame],
                 overall_avg_rmsf: Optional[pd.DataFrame],
                 structure_properties: Dict[str, pd.DataFrame]):

        self.config = config
        self.replica_avg_rmsf = replica_avg_rmsf
        self.overall_avg_rmsf = overall_avg_rmsf
        self.structure_properties = structure_properties
        self.add_rmsf_log = self.config.get("add_rmsf_log", True)

        if not replica_avg_rmsf and overall_avg_rmsf is None:
             raise FeatureBuilderError("Cannot build features: No RMSF data provided.")
        if not structure_properties:
             raise FeatureBuilderError("Cannot build features: No structure properties data provided.")

        # Pre-process structure properties for faster lookup (optional)
        # Ensure 'resid' is numeric in properties dict values
        for df in self.structure_properties.values():
            if 'resid' in df.columns:
                df['resid'] = pd.to_numeric(df['resid'], errors='coerce')


    def build_features(self, temperature: Optional[str] = None) -> Optional[pd.DataFrame]:
        """
        Builds the feature DataFrame for a specific temperature or the overall average.

        Args:
            temperature (Optional[str]): Temperature string (e.g., "320") or None to build
                                         the overall average feature set.

        Returns:
            Optional[pd.DataFrame]: The final feature DataFrame, or None if building fails.
        """
        if temperature:
            if temperature not in self.replica_avg_rmsf:
                logging.error(f"Cannot build features for T={temperature}: Missing average RMSF data.")
                return None
            base_rmsf_df = self.replica_avg_rmsf[temperature].copy()
            rmsf_col = f"rmsf_{temperature}"
            log_rmsf_col = f"rmsf_log_{temperature}"
            logging.info(f"Building features for temperature: {temperature}")
        else:
            if self.overall_avg_rmsf is None or self.overall_avg_rmsf.empty:
                logging.error("Cannot build average features: Missing overall average RMSF data.")
                return None
            base_rmsf_df = self.overall_avg_rmsf.copy()
            # Ensure the average column name exists and is correct
            if 'rmsf_average' not in base_rmsf_df.columns:
                 # Attempt to find/calculate if missing (should ideally be present from rmsf processing)
                 avg_cols = [c for c in base_rmsf_df if c.startswith("rmsf_avg_at_")]
                 if avg_cols: base_rmsf_df['rmsf_average'] = base_rmsf_df[avg_cols].mean(axis=1)
                 else:
                      logging.error("Cannot build average features: 'rmsf_average' column missing and cannot be calculated.")
                      return None
            rmsf_col = "rmsf_average"
            log_rmsf_col = "rmsf_log" # Generic name for log of average
            logging.info("Building features for overall average.")


        # --- Combine with Structure Properties ---
        all_features_list = []
        processed_domains = base_rmsf_df['domain_id'].unique()
        validation_failed_domains = set()

        for domain_id in processed_domains:
            domain_rmsf_df = base_rmsf_df[base_rmsf_df['domain_id'] == domain_id].copy()
            if domain_id not in self.structure_properties:
                logging.warning(f"Skipping domain {domain_id} for T={temperature or 'average'}: Missing structure properties.")
                validation_failed_domains.add(domain_id)
                continue

            domain_props_df = self.structure_properties[domain_id]
            if domain_props_df is None or domain_props_df.empty:
                 logging.warning(f"Skipping domain {domain_id} for T={temperature or 'average'}: Empty structure properties.")
                 validation_failed_domains.add(domain_id)
                 continue

            # Ensure 'resid' is numeric in both for merging
            try:
                domain_rmsf_df['resid'] = pd.to_numeric(domain_rmsf_df['resid'])
                # Properties 'resid' conversion done in __init__
            except Exception as e:
                 logging.error(f"Resid type conversion failed for domain {domain_id}: {e}. Skipping.")
                 validation_failed_domains.add(domain_id)
                 continue

            # --- CRITICAL VALIDATION: RESIDUE ALIGNMENT ---
            rmsf_resids = set(domain_rmsf_df['resid'])
            props_resids = set(domain_props_df['resid'])

            if len(domain_rmsf_df) != len(domain_props_df) or rmsf_resids != props_resids:
                logging.error(f"Residue mismatch for domain {domain_id}! "
                              f"RMSF count: {len(domain_rmsf_df)}, Properties count: {len(domain_props_df)}. "
                              f"RMSF resids unique: {len(rmsf_resids)}, Props resids unique: {len(props_resids)}. "
                              f" Skipping this domain for T={temperature or 'average'}.")
                # Log details of mismatch if needed (e.g., symmetric difference)
                # diff = rmsf_resids.symmetric_difference(props_resids)
                # logging.debug(f"Residue difference: {diff}")
                validation_failed_domains.add(domain_id)
                continue

            # --- Merge Data ---
            try:
                # Merge on domain_id (implicitly done by processing per domain) and resid
                # Select necessary columns from properties DF to avoid duplication
                props_cols_to_merge = ['resid', 'chain', 'dssp', 'relative_accessibility', 'core_exterior', 'phi', 'psi']
                merged_df = pd.merge(domain_rmsf_df, domain_props_df[props_cols_to_merge], on='resid', how='inner')

                # Double check length after merge (should be identical if validation passed)
                if len(merged_df) != len(domain_rmsf_df):
                    logging.error(f"Length mismatch after merging for domain {domain_id}! "
                                  f"RMSF: {len(domain_rmsf_df)}, Merged: {len(merged_df)}. Skipping.")
                    validation_failed_domains.add(domain_id)
                    continue

                all_features_list.append(merged_df)

            except Exception as e:
                 logging.error(f"Merging failed for domain {domain_id}: {e}. Skipping.", exc_info=True)
                 validation_failed_domains.add(domain_id)
                 continue

        if not all_features_list:
            logging.error(f"No domain data could be merged successfully for T={temperature or 'average'}. Cannot build feature set.")
            return None

        # Combine features for all successfully processed domains
        final_df = pd.concat(all_features_list, ignore_index=True)
        logging.info(f"Successfully merged data for {len(final_df['domain_id'].unique())} domains for T={temperature or 'average'}.")
        if validation_failed_domains:
             logging.warning(f"Failed validation/merge for {len(validation_failed_domains)} domains for T={temperature or 'average'}.")


        # --- Calculate Derived Features ---
        logging.debug("Calculating derived features...")
        # 1. Protein Size (per domain)
        final_df['protein_size'] = final_df.groupby('domain_id')['resid'].transform('nunique')

        # 2. Normalized Residue ID (per domain)
        def normalize_resid(x):
            min_val = x.min()
            max_val = x.max()
            range_val = max_val - min_val
            return (x - min_val) / range_val if range_val > 0 else 0.0 # Avoid division by zero for single-residue domains
        final_df['normalized_resid'] = final_df.groupby('domain_id')['resid'].transform(normalize_resid)

        # 3. Log RMSF
        if self.add_rmsf_log:
             # Add small epsilon to avoid log(0) issues if RMSF can be exactly zero
             epsilon = 1e-9
             final_df[log_rmsf_col] = np.log(final_df[rmsf_col] + epsilon)

        # --- Encode Categorical Features ---
        logging.debug("Encoding categorical features...")
        # 4. Core/Exterior Encoding
        core_ext_mapping = {"core": 0, "exterior": 1, "unknown": 2} # 'unknown' if SASA failed maybe? Defaulted to core earlier.
        final_df['core_exterior_encoded'] = final_df['core_exterior'].map(core_ext_mapping).fillna(2).astype(int)

        # 5. Secondary Structure (3-state: Helix, Sheet, Coil) Encoding
        def encode_ss3(ss):
            ss = str(ss).upper() # Ensure string and uppercase
            if ss in ['H', 'G', 'I']: return 0 # Helix
            elif ss in ['E', 'B']: return 1 # Sheet
            else: return 2 # Coil/Loop/Other
        final_df['secondary_structure_encoded'] = final_df['dssp'].apply(encode_ss3)

        # 6. Resname Encoding (simple integer mapping)
        unique_resnames = sorted([r for r in final_df['resname'].unique() if r and isinstance(r, str) and len(r) == 3])
        resname_map = {name: i for i, name in enumerate(unique_resnames)}
        final_df['resname_encoded'] = final_df['resname'].map(resname_map).fillna(-1).astype(int) # Use -1 for unknown/non-standard

        # 7. Normalized Phi/Psi Angles (map -180 to 180 -> -1 to 1)
        final_df['phi_norm'] = final_df['phi'] / 180.0
        final_df['psi_norm'] = final_df['psi'] / 180.0
        # Clip values just in case they exceed +/- 180
        final_df[['phi_norm', 'psi_norm']] = final_df[['phi_norm', 'psi_norm']].clip(-1.0, 1.0)

        # --- Final Touches ---
        # Define final column order (adjust as needed)
        core_cols = ['domain_id', 'resid', 'resname', rmsf_col]
        if self.add_rmsf_log: core_cols.append(log_rmsf_col)

        info_cols = ['protein_size', 'normalized_resid', 'chain']
        structure_cols = ['core_exterior', 'relative_accessibility', 'dssp', 'phi', 'psi']
        encoded_cols = ['core_exterior_encoded', 'secondary_structure_encoded', 'resname_encoded', 'phi_norm', 'psi_norm']

        # Get all columns present and order them
        present_cols = final_df.columns.tolist()
        final_order = ([col for col in core_cols if col in present_cols] +
                       [col for col in info_cols if col in present_cols] +
                       [col for col in structure_cols if col in present_cols] +
                       [col for col in encoded_cols if col in present_cols])
        # Add any remaining columns not explicitly listed (shouldn't be many)
        remaining_cols = [col for col in present_cols if col not in final_order]
        final_order.extend(remaining_cols)

        final_df = final_df[final_order]

        # Final check for NaNs in key numerical columns (RMSF, coords should ideally not be NaN)
        nan_check_cols = [rmsf_col, 'protein_size', 'normalized_resid', 'relative_accessibility', 'phi_norm', 'psi_norm']
        if self.add_rmsf_log: nan_check_cols.append(log_rmsf_col)
        for col in nan_check_cols:
             if col in final_df.columns and final_df[col].isnull().any():
                  logging.warning(f"NaN values detected in final feature column '{col}' for T={temperature or 'average'}. Consider filling/investigation.")
                  # Fill with 0 for now? Or mean? Filling with 0 might be safer.
                  final_df[col].fillna(0.0, inplace=True)


        logging.info(f"Finished building features for T={temperature or 'average'}. Final shape: {final_df.shape}")
        return final_df



--- End of File: ./src/mdcath/features/builder.py ---

===== FILE: ./src/mdcath/features/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/pipeline/executor.py =====
# -*- coding: utf-8 -*-
"""
Orchestrates the mdCATH processing pipeline steps.
Includes parallel processing worker function to avoid pickling errors.
"""
import os
import glob
import logging
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from tqdm import tqdm
import importlib
import inspect
import traceback # Import traceback module

# Import package components
from ..config import load_config
from ..io.hdf5_reader import HDF5Reader, HDF5ReaderError
from ..structure.pdb_processor import PDBProcessor
from ..structure.properties import PropertiesCalculator
from ..structure.frame_extractor import extract_and_save_frames
from ..metrics.rmsf import process_domain_rmsf, aggregate_and_average_rmsf, save_rmsf_results
from ..features.builder import FeatureBuilder, FeatureBuilderError
from ..voxel.aposteriori_wrapper import run_aposteriori, VoxelizationError
from ..visualize import plots as viz_plots
from ..utils.parallel import parallel_map
from ..io.writers import save_dataframe_csv

# Import custom exceptions
from ..exceptions import (
    RmsfProcessingError,
    PDBProcessorError,
    PropertiesCalculatorError,
    HDF5ReaderError,
    PipelineExecutionError,
    FeatureBuilderError,
    VoxelizationError,
    ConfigurationError
)

# Define PipelineExecutionError if not defined in exceptions.py
if 'PipelineExecutionError' not in locals():
    class PipelineExecutionError(Exception):
        """Custom exception for pipeline execution errors."""
        pass

# --- Top-level Worker Function for Parallel Processing ---
def _parallel_domain_worker(args_tuple: Tuple[str, Dict, str]) -> Tuple[str, str, Optional[Any], Optional[pd.DataFrame], Optional[str]]:
    """
    Worker function for parallel domain processing. Initializes components internally.

    Args:
        args_tuple (Tuple): Contains (domain_id, config_dict, output_dir)

    Returns:
        Tuple: (domain_id, status, rmsf_result, properties_result, cleaned_pdb_path)
    """
    domain_id, config_dict, output_dir = args_tuple
    status = f"Starting (worker) {domain_id}"
    rmsf_result = None
    properties_result = None
    cleaned_pdb_path = None
    # Configure logging within worker if needed (basic for now)
    # logging.basicConfig(level=logging.INFO) # Or pass level from main config

    try:
        # Initialize components INSIDE the worker to avoid pickling issues
        pdb_processor = PDBProcessor(config_dict.get('processing', {}).get('pdb_cleaning', {}))
        properties_calculator = PropertiesCalculator(config_dict.get('processing', {}).get('properties_calculation', {}))

        input_cfg = config_dict.get('input', {})
        mdcath_folder = input_cfg.get('mdcath_folder')
        h5_path = os.path.join(mdcath_folder, f"mdcath_dataset_{domain_id}.h5")
        reader = HDF5Reader(h5_path)
        status = f"Read HDF5 OK"

        pdb_string = reader.get_pdb_string()
        if not pdb_string: status = "Failed PDB Read"; raise ValueError("Could not read PDB string")

        pdb_out_dir = os.path.join(output_dir, "pdbs")
        os.makedirs(pdb_out_dir, exist_ok=True)
        cleaned_pdb_path = os.path.join(pdb_out_dir, f"{domain_id}.pdb")

        if not pdb_processor.clean_pdb_string(pdb_string, cleaned_pdb_path):
             status = "Failed PDB Clean"; raise PDBProcessorError("PDB cleaning failed")
        status = "PDB Clean OK"

        properties_result = properties_calculator.calculate_properties(cleaned_pdb_path)
        if properties_result is None or properties_result.empty:
             status = "Failed Properties Calc"; raise PropertiesCalculatorError("Property calculation failed")
        status = "Properties Calc OK"

        rmsf_result_tuple = process_domain_rmsf(domain_id, reader, config_dict)
        if rmsf_result_tuple is None:
            status = "Failed RMSF Proc"; raise RmsfProcessingError("RMSF processing failed")

        rmsf_result = (rmsf_result_tuple[0].copy(), rmsf_result_tuple[1])
        status = "RMSF Proc OK"

        status = "Success"

    except (FileNotFoundError, HDF5ReaderError, ValueError) as e:
         status = status if status.startswith("Failed") else "Failed HDF5 Read/Access"
         logging.error(f"Worker error {domain_id} (Read): {e}")
    except PDBProcessorError as e:
         status = status if status.startswith("Failed") else "Failed PDB Clean"
         logging.error(f"Worker error {domain_id} (Clean): {e}")
    except PropertiesCalculatorError as e:
         status = status if status.startswith("Failed") else "Failed Properties Calc"
         logging.error(f"Worker error {domain_id} (Properties): {e}")
    except RmsfProcessingError as e:
         status = status if status.startswith("Failed") else "Failed RMSF Proc"
         logging.error(f"Worker error {domain_id} (RMSF): {e}")
    except Exception as e:
        status = f"Failed Unexpected (Worker): {type(e).__name__}"
        logging.error(f"Unexpected worker error {domain_id}: {e}")
        # Log traceback if needed (might be complex from worker)
        # logging.debug(traceback.format_exc()) # Might not work well here

    return domain_id, status, rmsf_result, properties_result, cleaned_pdb_path


# --- PipelineExecutor Class ---
class PipelineExecutor:
    """ Manages and executes the mdCATH processing pipeline. """
    def __init__(self, config_dict: Dict[str, Any]):
        """ Initializes the executor with a configuration dictionary. """
        try:
            self.config = config_dict
            self.output_dir = self.config.get('output', {}).get('base_dir', './outputs')
            self.num_cores = self.config.get('performance', {}).get('num_cores', 0)
            cpu_count = os.cpu_count()
            if self.num_cores <= 0:
                 self.num_cores = max(1, cpu_count - 2 if cpu_count is not None and cpu_count > 2 else 1)
            else:
                 self.num_cores = self.num_cores
            logging.info(f"Determined number of cores to use: {self.num_cores}")

            # Initialize components that are safe to initialize here (stateless or used sequentially)
            # self.pdb_processor = PDBProcessor(...) # Initializing in worker now
            # self.properties_calculator = PropertiesCalculator(...) # Initializing in worker now

            # Pipeline state
            self.domain_list: List[str] = []
            self.domain_status: Dict[str, str] = {}
            self.all_domain_rmsf_results: Dict[str, Tuple[pd.DataFrame, Dict[str, Dict[str, np.ndarray]]]] = {}
            self.all_domain_properties: Dict[str, Optional[pd.DataFrame]] = {}
            self.cleaned_pdb_paths: Dict[str, Optional[str]] = {}
            self.voxel_output_file: Optional[str] = None
            self.agg_replica_data: Optional[Dict] = None
            self.agg_replica_avg_data: Optional[Dict] = None
            self.agg_overall_avg_data: Optional[pd.DataFrame] = None
            self.all_feature_dfs: Dict[str, pd.DataFrame] = {}

        except Exception as e:
            logging.exception(f"Failed during PipelineExecutor initialization: {e}")
            raise PipelineExecutionError(f"Initialization failed: {e}") from e

    def _get_domain_list(self) -> List[str]:
        """Determines the list of domain IDs to process."""
        # (Keep this method as is)
        input_cfg = self.config.get('input', {})
        configured_domains = input_cfg.get('domain_ids')
        mdcath_folder = input_cfg.get('mdcath_folder')

        if isinstance(configured_domains, list) and configured_domains:
            logging.info(f"Using specified list of {len(configured_domains)} domains.")
            return configured_domains
        elif mdcath_folder and os.path.isdir(mdcath_folder):
            pattern = os.path.join(mdcath_folder, "mdcath_dataset_*.h5")
            logging.debug(f"Searching for HDF5 files with pattern: {pattern}")
            h5_files = glob.glob(pattern)
            if not h5_files:
                 raise PipelineExecutionError(f"No 'mdcath_dataset_*.h5' files found in specified directory: {mdcath_folder}")

            domains = []
            for h5_file in h5_files:
                basename = os.path.basename(h5_file)
                if basename.startswith("mdcath_dataset_") and basename.endswith(".h5"):
                    domain_id = basename[len("mdcath_dataset_"):-len(".h5")]
                    if domain_id: domains.append(domain_id)
                    else: logging.warning(f"Skipping file with potentially empty domain ID: {h5_file}")
                else: logging.warning(f"Skipping file not matching expected pattern: {h5_file}")

            if not domains: raise PipelineExecutionError(f"No valid domains extracted from filenames in: {mdcath_folder}")

            logging.info(f"Found {len(domains)} domains in {mdcath_folder}.")
            return sorted(domains)
        else:
            raise PipelineExecutionError("Config must provide 'input.domain_ids' list or valid 'input.mdcath_folder'.")

    # _process_single_domain is no longer called directly by parallel_map
    # Keep it for potential sequential use or reference if needed
    def _process_single_domain(self, domain_id: str) -> Tuple[str, str, Optional[Any], Optional[pd.DataFrame], Optional[str]]:
         """ Sequential processing for a single domain (used as fallback). """
         # This essentially duplicates the worker function logic
         # Can call the worker function even in sequential mode for consistency
         return _parallel_domain_worker((domain_id, self.config, self.output_dir))

    def _extract_all_frames(self):
        """Extracts frames sequentially after initial processing."""
        # (Keep this method as is)
        frame_cfg = self.config.get('processing', {}).get('frame_selection', {})
        if frame_cfg.get('num_frames', 0) <= 0:
            logging.info("Frame extraction skipped (num_frames <= 0).")
            return

        logging.info("Starting frame extraction...")
        domains_to_process = [did for did, status in self.domain_status.items() if status == "Success"]
        if not domains_to_process:
             logging.warning("No successfully processed domains available for frame extraction.")
             return

        input_cfg = self.config.get('input', {})
        mdcath_folder = input_cfg.get('mdcath_folder')
        num_success = 0
        num_failed = 0

        for domain_id in tqdm(domains_to_process, desc="Extracting Frames", disable=not self.config.get('logging',{}).get('show_progress_bars', True)):
            domain_frames_saved_flag = False
            try:
                 h5_path = os.path.join(mdcath_folder, f"mdcath_dataset_{domain_id}.h5")
                 if not os.path.exists(h5_path):
                      logging.error(f"HDF5 file not found for frame extraction: {h5_path}")
                      num_failed += 1; continue

                 reader = HDF5Reader(h5_path)
                 cleaned_pdb_path = self.cleaned_pdb_paths.get(domain_id)
                 if not cleaned_pdb_path or not os.path.exists(cleaned_pdb_path):
                      logging.warning(f"Skipping frame extraction for {domain_id}: Missing cleaned PDB '{cleaned_pdb_path}'.")
                      num_failed+=1; continue

                 temps = reader.get_available_temperatures()
                 for temp in temps:
                     replicas = reader.get_available_replicas(temp)
                     for rep in replicas:
                          coords_all = reader.get_coordinates(temp, rep, frame_index=-999)
                          if coords_all is None:
                               logging.warning(f"No coordinates found for {domain_id}, T={temp}, R={rep}. Skipping."); continue

                          rmsd_data = reader.get_scalar_traj(temp, rep, 'rmsd')
                          gyration_data = reader.get_scalar_traj(temp, rep, 'gyrationRadius')

                          success = extract_and_save_frames(
                               domain_id, coords_all, cleaned_pdb_path, self.output_dir,
                               self.config, rmsd_data, gyration_data, str(temp), str(rep)
                          )
                          if success: domain_frames_saved_flag = True

                 if domain_frames_saved_flag: num_success += 1
                 else: num_failed += 1

            except Exception as e:
                 logging.error(f"Error during frame extraction loop for {domain_id}: {e}", exc_info=True)
                 num_failed += 1

        logging.info(f"Frame extraction complete. Domains with frames saved: {num_success}, Domains failed: {num_failed}")

    def run(self):
        """Executes the full processing pipeline."""
        try:
            logging.info("Starting mdCATH processing pipeline...")
            logging.info(f"Using output directory: {self.output_dir}")
            os.makedirs(self.output_dir, exist_ok=True)

            # --- Step 1: Get Domain List ---
            self.domain_list = self._get_domain_list()
            if not self.domain_list: logging.warning("No domains found/specified."); return

            # --- Step 2: Initial Domain Processing ---
            logging.info(f"Processing {len(self.domain_list)} domains using up to {self.num_cores} cores...")
            results = []
            show_progress = self.config.get('logging',{}).get('show_progress_bars', True)
            if self.num_cores > 1 and len(self.domain_list) > 1:
                logging.info(f"Using parallel processing with {self.num_cores} cores.")
                try:
                     # Prepare arguments for the worker function
                     worker_args = [(domain_id, self.config, self.output_dir) for domain_id in self.domain_list]
                     results = parallel_map(_parallel_domain_worker, worker_args, # Call the top-level worker
                                            num_cores=self.num_cores, use_progress_bar=show_progress,
                                            desc="Processing Domains")
                except Exception as parallel_e:
                     logging.error(f"Parallel processing failed: {parallel_e}. Falling back to sequential.")
                     # *** ROBUST LOGGING FIX ***
                     if logging.getLogger().isEnabledFor(logging.DEBUG):
                         try: logging.debug("Traceback for parallel processing failure:", exc_info=True)
                         except TypeError: logging.debug(f"Traceback:\n{traceback.format_exc()}") # Use traceback module
                     # Use the sequential instance method as fallback
                     results = [self._process_single_domain(did) for did in tqdm(self.domain_list, desc="Processing (Sequential Fallback)", disable=not show_progress)]
            else:
                logging.info("Using sequential processing.")
                results = [self._process_single_domain(did) for did in tqdm(self.domain_list, desc="Processing Domains", disable=not show_progress)]

            # Collect results (remains the same)
            for result in results:
                 if result and isinstance(result, tuple) and len(result) == 5:
                    domain_id, status, rmsf_res, prop_res, pdb_path = result
                    self.domain_status[domain_id] = status
                    if status == "Success":
                        self.all_domain_rmsf_results[domain_id] = rmsf_res; self.all_domain_properties[domain_id] = prop_res; self.cleaned_pdb_paths[domain_id] = pdb_path
                 else: logging.error(f"Invalid result from domain processing: {result}")

            successful_domains = [d for d, s in self.domain_status.items() if s == "Success"]
            logging.info(f"Initial processing complete. Successful domains: {len(successful_domains)}/{len(self.domain_list)}")
            if not successful_domains:
                 logging.error("No domains processed successfully. Cannot proceed further."); self._generate_status_visualization(); return

            # --- Step 3: Aggregate RMSF ---
            # (Keep as is)
            logging.info("Aggregating RMSF data...")
            try:
                self.agg_replica_data, self.agg_replica_avg_data, self.agg_overall_avg_data = aggregate_and_average_rmsf(self.all_domain_rmsf_results, self.config)
            except Exception as e: logging.exception(f"Error during RMSF aggregation: {e}"); self.agg_replica_data, self.agg_replica_avg_data, self.agg_overall_avg_data = None, None, None


            # --- Step 4: Save RMSF Results ---
            # (Keep as is)
            try: save_rmsf_results(self.output_dir, self.config, self.agg_replica_data, self.agg_replica_avg_data, self.agg_overall_avg_data)
            except Exception as e: logging.exception(f"Error saving RMSF results: {e}")


            # --- Step 5: Build ML Features ---
            # (Keep as is, including robust logging)
            logging.info("Building ML feature sets...")
            try:
                 valid_replica_avg = {k: v for k, v in (self.agg_replica_avg_data or {}).items() if isinstance(v, pd.DataFrame) and not v.empty}
                 valid_overall_avg = self.agg_overall_avg_data if isinstance(self.agg_overall_avg_data, pd.DataFrame) and not self.agg_overall_avg_data.empty else None
                 valid_properties = {k: v for k, v in self.all_domain_properties.items() if isinstance(v, pd.DataFrame) and not v.empty}

                 if (not valid_replica_avg and valid_overall_avg is None) or not valid_properties:
                     if not valid_replica_avg and valid_overall_avg is None: logging.warning("Skipping feature building: No valid RMSF average data.")
                     if not valid_properties: logging.warning("Skipping feature building: No valid structure properties.")
                     self.all_feature_dfs = {}
                 else:
                     feature_builder = FeatureBuilder(self.config.get('processing', {}).get('feature_building', {}), valid_replica_avg, valid_overall_avg, valid_properties)
                     self.all_feature_dfs = {}
                     ml_features_dir = os.path.join(self.output_dir, "ML_features")
                     temps = list(valid_replica_avg.keys())
                     for temp_str in tqdm(temps, desc="Building Temp Features", disable=not show_progress):
                          features_df = feature_builder.build_features(temperature=temp_str)
                          if features_df is not None:
                               self.all_feature_dfs[temp_str] = features_df; save_dataframe_csv(features_df, os.path.join(ml_features_dir, f"final_dataset_temperature_{temp_str}.csv"))
                     overall_features_df = feature_builder.build_features(temperature=None)
                     if overall_features_df is not None:
                          self.all_feature_dfs["average"] = overall_features_df; save_dataframe_csv(overall_features_df, os.path.join(ml_features_dir, "final_dataset_temperature_average.csv"))
            except Exception as e:
                 logging.error(f"Error during feature building: {e}")
                 if logging.getLogger().isEnabledFor(logging.DEBUG):
                     try: logging.debug("Traceback:", exc_info=True)
                     except TypeError: logging.debug(f"Traceback:\n{traceback.format_exc()}")
                 self.all_feature_dfs = {}


            # --- Step 6: Extract Frames ---
            # (Keep as is)
            try: self._extract_all_frames()
            except Exception as e: logging.exception(f"Error during frame extraction: {e}")


            # --- Step 7: Voxelization ---
            # (Keep as is, including robust logging)
            logging.info("Running voxelization...")
            try:
                 cleaned_pdb_dir = os.path.join(self.output_dir, "pdbs")
                 voxel_config = self.config.get('processing', {}).get('voxelization', {})
                 if not os.path.isdir(cleaned_pdb_dir) or not os.listdir(cleaned_pdb_dir): logging.warning("Skipping Voxelization: Cleaned PDB dir empty/missing.")
                 elif not voxel_config.get("enabled", True): logging.info("Voxelization skipped by config.")
                 else:
                     voxel_success = run_aposteriori(voxel_config, self.output_dir, cleaned_pdb_dir)
                     if voxel_success:
                         voxel_output_name = voxel_config.get("output_name", "mdcath_voxelized")
                         potential_path = os.path.join(self.output_dir, "voxelized", f"{voxel_output_name}.hdf5")
                         if os.path.exists(potential_path): self.voxel_output_file = potential_path
                         else: logging.warning(f"Voxelization command finished, but output file not found: {potential_path}")
            except Exception as e:
                 logging.error(f"Unexpected error during voxelization: {e}")
                 if logging.getLogger().isEnabledFor(logging.DEBUG):
                     try: logging.debug("Traceback:", exc_info=True)
                     except TypeError: logging.debug(f"Traceback:\n{traceback.format_exc()}")


            # --- Step 8: Generate Visualizations ---
            # (Keep as is)
            if self.config.get('visualization', {}).get('enabled', True):
                 try: self._generate_visualizations() # Call refactored method
                 except Exception as e:
                     logging.error(f"Error during visualization generation: {e}")
                     if logging.getLogger().isEnabledFor(logging.DEBUG):
                         try: logging.debug("Traceback:", exc_info=True)
                         except TypeError: logging.debug(f"Traceback:\n{traceback.format_exc()}")
            else: logging.info("Visualization generation skipped by configuration.")

            logging.info("Pipeline finished.")

        except Exception as e:
             logging.error(f"An unexpected error occurred during pipeline execution: {e}")
             # Apply robust logging fix here as well
             if logging.getLogger().isEnabledFor(logging.DEBUG):
                 try: logging.debug("Traceback for pipeline execution failure:", exc_info=True)
                 except TypeError: logging.debug(f"Traceback:\n{traceback.format_exc()}")
             raise PipelineExecutionError(f"Pipeline failed unexpectedly: {e}") from e

    # --- run_plot Helper ---
    def run_plot(self, func, *args, **kwargs):
        """ Helper function to run a plotting function with error handling and data validation. """
        arg_name = func.__name__
        logging.debug(f"Attempting to generate plot: {arg_name}")

        # Basic data validation
        all_args_valid = True
        for i, data_arg in enumerate(args):
            arg_desc = f"positional argument {i+1}"
            if data_arg is None: logging.warning(f"Skipping plot {arg_name}: Required {arg_desc} is None."); return
            if isinstance(data_arg, pd.DataFrame) and data_arg.empty: logging.warning(f"Skipping plot {arg_name}: Required DataFrame {arg_desc} is empty."); return
            if isinstance(data_arg, (dict, list, tuple)) and not data_arg: logging.warning(f"Skipping plot {arg_name}: Required {type(data_arg).__name__} {arg_desc} is empty."); return

        if not all_args_valid: return # Exit if validation failed (though loop structure makes this redundant)

        try:
            plot_kwargs = kwargs.copy()
            func_sig = inspect.signature(func)
            func_params = func_sig.parameters

            if 'output_dir' in func_params: plot_kwargs.setdefault('output_dir', self.output_dir)
            if 'viz_config' in func_params: plot_kwargs.setdefault('viz_config', self.config.get('visualization', {}))

            accepted_kwargs = {k: v for k, v in plot_kwargs.items() if k in func_params}
            if any(p.kind == p.VAR_KEYWORD for p in func_params.values()): final_kwargs = plot_kwargs
            else: final_kwargs = accepted_kwargs

            func(*args, **final_kwargs)

        except Exception as e:
            # *** ROBUST LOGGING FIX ***
            error_message = f"Failed to generate plot {func.__name__}: {e}"
            logging.error(error_message)
            if logging.getLogger().isEnabledFor(logging.DEBUG):
                try: logging.debug(f"Traceback for plot failure ({func.__name__}):", exc_info=True)
                except TypeError: logging.debug(f"Traceback for plot failure ({func.__name__}):\n{traceback.format_exc()}")

    # --- _generate_status_visualization Helper ---
    def _generate_status_visualization(self):
         """Helper to generate only the status plot, e.g., on early exit."""
         if self.config.get('visualization', {}).get('enabled', True) and self.domain_status:
             logging.info("Generating processing status visualization...")
             try:
                  avg_feature_df_for_status = self.all_feature_dfs.get("average", pd.DataFrame())
                  replica_avg_data_for_status = self.agg_replica_avg_data if self.agg_replica_avg_data is not None else {}
                  # Call the appropriate function - assuming create_summary_plot handles status
                  self.run_plot(viz_plots.create_summary_plot,
                                replica_avg_data_for_status,
                                avg_feature_df_for_status,
                                self.domain_status)
             except Exception as e:
                  logging.error(f"Failed to generate status plot (via summary): {e}")
                  if logging.getLogger().isEnabledFor(logging.DEBUG):
                      try: logging.debug("Traceback:", exc_info=True)
                      except TypeError: logging.debug(f"Traceback:\n{traceback.format_exc()}")

    # --- _generate_visualizations Method ---
    def _generate_visualizations(self):
          """Generates all configured plots by calling functions from viz_plots."""
          logging.info("Generating visualizations...")

          # --- Prepare Data References ---
          replica_avg_data = self.agg_replica_avg_data if self.agg_replica_avg_data is not None else {}
          avg_feature_df = self.all_feature_dfs.get("average", pd.DataFrame())
          domain_status_data = self.domain_status if self.domain_status is not None else {}
          voxel_output = self.voxel_output_file
          combined_replica_data = self.agg_replica_data if self.agg_replica_data else {}
          overall_avg_data = self.agg_overall_avg_data if self.agg_overall_avg_data is not None else pd.DataFrame()
          feature_dfs_dict = self.all_feature_dfs if self.all_feature_dfs else {} # Pass the whole dict if needed
          pdb_results_placeholder = {} # Need to collect PDB paths during main loop
          domain_results_placeholder = self.domain_status # Use status dict as placeholder for domain results

          # --- Define Plotting Tasks ---
          # Use the function names from the user-provided plots.py
          plotting_tasks = [
              (viz_plots.create_temperature_summary_heatmap, [replica_avg_data], {}),
              (viz_plots.create_temperature_average_summary, [avg_feature_df], {}),
              # Pass both replica and overall avg to the function that creates violin + combined histogram
              (viz_plots.create_rmsf_distribution_plots, [replica_avg_data, overall_avg_data], {}),
              (viz_plots.create_amino_acid_rmsf_plot, [avg_feature_df], {}), # Uses avg_df now
              (viz_plots.create_amino_acid_rmsf_plot_colored, [avg_feature_df], {}),
              (viz_plots.create_replica_variance_plot, [combined_replica_data], {}),
              (viz_plots.create_dssp_rmsf_correlation_plot, [avg_feature_df], {}), # Uses avg_df
              (viz_plots.create_feature_correlation_plot, [avg_feature_df], {}), # Uses avg_df
              # (viz_plots.create_frames_visualization, [pdb_results_placeholder, self.config, domain_results_placeholder], {}), # Requires more data storage
              (viz_plots.create_ml_features_plot, [avg_feature_df], {}), # Uses avg_df
              (viz_plots.create_summary_plot, [replica_avg_data, avg_feature_df, domain_status_data], {}), # Pass avg_df
              (viz_plots.create_additional_ml_features_plot, [avg_feature_df], {}), # Uses avg_df
              # Ensure these new functions exist in plots.py
              (getattr(viz_plots, 'create_rmsf_density_plots', None), [avg_feature_df], {}),
              (getattr(viz_plots, 'create_rmsf_by_aa_ss_density', None), [avg_feature_df], {}),
              (viz_plots.create_voxel_info_plot, [], {'config': self.config, 'voxel_output_file': voxel_output}),
          ]
          # Filter out tasks where the function is None (e.g., if new plots aren't defined)
          plotting_tasks = [task for task in plotting_tasks if task[0] is not None]


          # --- Execute Plotting Tasks ---
          logging.info(f"Executing {len(plotting_tasks)} visualization tasks...")
          for plot_func, data_args, plot_kwargs in tqdm(plotting_tasks, desc="Generating Visualizations", disable=not self.config.get('logging',{}).get('show_progress_bars', True)):
                self.run_plot(plot_func, *data_args, **plot_kwargs)

          logging.info("Visualization generation attempt complete.")
--- End of File: ./src/mdcath/pipeline/executor.py ---

===== FILE: ./src/mdcath/pipeline/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/metrics/__init__.py (Skipped - Detected as non-text) =====

===== FILE: ./src/mdcath/metrics/rmsf.py =====
"""
Module for processing Root Mean Square Fluctuation (RMSF) data.
Includes extraction, validation, averaging, and saving.
"""

import os
import logging
import pandas as pd
import numpy as np
from typing import Dict, Optional, List, Any, Tuple
from mdcath.io.hdf5_reader import HDF5Reader
from mdcath.io.writers import save_dataframe_csv, get_rmsf_output_path

class RmsfProcessingError(Exception):
    """Custom exception for RMSF processing errors."""
    pass

def process_domain_rmsf(domain_id: str,
                        reader: HDF5Reader,
                        config: Dict[str, Any]
                        ) -> Optional[Tuple[pd.DataFrame, Dict[str, Dict[str, np.ndarray]]]]:
    """
    Extracts RMSF data for all temps/replicas and aligns with residue info.
    Standardizes Histidine variants to 'HIS'.

    Args:
        domain_id (str): The domain ID.
        reader (HDF5Reader): Initialized HDF5Reader for the domain.
        config (Dict[str, Any]): Global configuration.

    Returns:
        Optional[Tuple[pd.DataFrame, Dict[str, Dict[str, np.ndarray]]]]:
            - DataFrame with unique 'resid', 'resname' (standardized).
            - Dict[temp_str, Dict[rep_str, np.ndarray]] holding VALIDATED RMSF arrays.
            Returns None if validation fails or no RMSF data found.
    """
    logging.debug(f"Processing RMSF for domain {domain_id}")
    residue_info_df = reader.get_residue_info() # This function already handles decoding bytes
    if residue_info_df is None or residue_info_df.empty:
        logging.error(f"Cannot process RMSF for {domain_id}: Missing or empty residue info.")
        return None

    # *** STANDARDIZE HISTIDINE NAMES HERE ***
    his_variants = ['HSD', 'HSE', 'HSP', 'HID', 'HIE', 'HIP'] # Include common variants
    original_resnames = set(residue_info_df['resname'].unique())
    variants_found = [res for res in his_variants if res in original_resnames]

    if variants_found:
        logging.debug(f"Standardizing Histidine variants {variants_found} to HIS for domain {domain_id}")
        # Create a mapping dictionary
        his_map = {variant: 'HIS' for variant in his_variants}
        # Apply the replacement
        residue_info_df['resname'] = residue_info_df['resname'].replace(his_map)
        standardized_resnames = set(residue_info_df['resname'].unique())
        logging.debug(f"Resnames after standardization for {domain_id}: {standardized_resnames}")
    # --- End Histidine Standardization ---

    num_residues = len(residue_info_df)
    rmsf_data_collected: Dict[str, Dict[str, np.ndarray]] = {}
    found_any_rmsf = False
    temps_to_process = reader.get_available_temperatures() # Use all available temps

    for temp in temps_to_process:
        temp_str = str(temp)
        rmsf_data_collected[temp_str] = {}
        replicas_to_process = reader.get_available_replicas(temp)

        for replica in replicas_to_process:
            replica_str = str(replica)
            rmsf_array = reader.get_rmsf(temp, replica)

            if rmsf_array is None:
                logging.warning(f"RMSF not found/readable for {domain_id}, T={temp_str}, R={replica_str}")
                continue

            # --- CRITICAL VALIDATION ---
            if len(rmsf_array) != num_residues:
                logging.error(f"RMSF length mismatch for {domain_id}, T={temp_str}, R={replica_str}: "
                              f"Expected {num_residues} residues, got {len(rmsf_array)} RMSF values. Skipping this replica.")
                continue # Skip this invalid replica

            # --- Check for NaNs ---
            if np.isnan(rmsf_array).any():
                 logging.warning(f"NaN values found in RMSF data for {domain_id}, T={temp_str}, R={replica_str}. "
                                 f"Attempting to fill with mean of valid values.")
                 if np.isnan(rmsf_array).all():
                      logging.error(f"All RMSF values are NaN for {domain_id}, T={temp_str}, R={replica_str}. Skipping.")
                      continue
                 mean_rmsf = np.nanmean(rmsf_array)
                 rmsf_array[np.isnan(rmsf_array)] = mean_rmsf

            rmsf_data_collected[temp_str][replica_str] = rmsf_array
            found_any_rmsf = True

    if not found_any_rmsf:
        logging.warning(f"No valid RMSF data found for any temp/replica for domain {domain_id}.")
        return None

    # Add domain_id column to residue info for easier aggregation later
    residue_info_df['domain_id'] = domain_id
    residue_info_df = residue_info_df[['domain_id', 'resid', 'resname']] # Reorder

    return residue_info_df, rmsf_data_collected


def aggregate_and_average_rmsf(all_domain_rmsf_results: Dict[str, Tuple[pd.DataFrame, Dict[str, Dict[str, np.ndarray]]]],
                               config: Dict[str, Any]
                               ) -> Tuple[Optional[Dict], Optional[Dict], Optional[pd.DataFrame]]:
    """
    Aggregates RMSF data across all domains, calculates averages, and prepares for saving.

    Args:
        all_domain_rmsf_results (Dict): Output from process_domain_rmsf for multiple domains.
                                        {domain_id: (residue_info_df, rmsf_data_dict)}
        config (Dict[str, Any]): Global configuration.

    Returns:
        Tuple[Optional[Dict], Optional[Dict], Optional[pd.DataFrame]]:
            - Aggregated raw replica data dict: {temp: {replica: combined_df}}
            - Aggregated replica average data dict: {temp: combined_avg_df}
            - Overall temperature average DataFrame across all domains.
            Returns (None, None, None) if input is empty or processing fails.
    """
    if not all_domain_rmsf_results:
        logging.warning("No domain RMSF results provided for aggregation.")
        return None, None, None

    logging.info(f"Aggregating RMSF data for {len(all_domain_rmsf_results)} domains...")

    # Structures to hold combined data
    combined_replica_dfs: Dict[str, Dict[str, List[pd.DataFrame]]] = {} # {temp: {replica: [df1, df2, ...]}}
    temps = set()

    # 1. Combine raw replica data from all domains
    for domain_id, (res_info_df, rmsf_dict) in all_domain_rmsf_results.items():
        if res_info_df is None or rmsf_dict is None: continue

        for temp_str, replica_dict in rmsf_dict.items():
            temps.add(temp_str)
            if temp_str not in combined_replica_dfs:
                combined_replica_dfs[temp_str] = {}

            for replica_str, rmsf_array in replica_dict.items():
                if replica_str not in combined_replica_dfs[temp_str]:
                    combined_replica_dfs[temp_str][replica_str] = []

                # Create DataFrame for this domain/temp/replica
                domain_temp_rep_df = res_info_df.copy()
                rmsf_col_name = f"rmsf_{temp_str}" # Consistent naming
                domain_temp_rep_df[rmsf_col_name] = rmsf_array
                combined_replica_dfs[temp_str][replica_str].append(domain_temp_rep_df)

    if not combined_replica_dfs:
        logging.error("Failed to combine any replica RMSF data.")
        return None, None, None

    # Concatenate DataFrames for each temp/replica
    final_replica_data: Dict[str, Dict[str, pd.DataFrame]] = {} # {temp: {replica: final_df}}
    for temp_str, replica_dict in combined_replica_dfs.items():
        final_replica_data[temp_str] = {}
        for replica_str, df_list in replica_dict.items():
            if df_list:
                 final_replica_data[temp_str][replica_str] = pd.concat(df_list, ignore_index=True)
                 logging.debug(f"Combined replica data for T={temp_str}, R={replica_str}: {len(final_replica_data[temp_str][replica_str])} rows")
            else:
                 logging.warning(f"No dataframes to combine for T={temp_str}, R={replica_str}")


    # 2. Calculate Replica Averages for each temperature
    replica_average_data: Dict[str, pd.DataFrame] = {} # {temp: final_avg_df}
    all_averages_list = [] # List to store average DFs per temp for final overall average

    for temp_str in sorted(list(temps)): # Process in consistent order
        if temp_str not in final_replica_data or not final_replica_data[temp_str]:
            logging.warning(f"No replica data found for T={temp_str} to calculate average.")
            continue

        replica_dfs_for_temp = list(final_replica_data[temp_str].values())
        if not replica_dfs_for_temp:
             logging.warning(f"DataFrame list is empty for T={temp_str} average calculation.")
             continue

        # Use the first replica's DF structure as base, group by domain and residue
        base_df_structure = replica_dfs_for_temp[0][['domain_id', 'resid', 'resname']] # Should be consistent
        all_replica_rmsf = [df[[f"rmsf_{temp_str}"]].rename(columns={f"rmsf_{temp_str}": f"rmsf_rep_{i}"})
                           for i, df in enumerate(replica_dfs_for_temp)]

        # Concatenate along columns (assuming rows align perfectly based on prior processing)
        # This relies heavily on the domain processing having consistent residue info
        # A safer approach might group by (domain_id, resid) but could be slower.
        # Let's assume concatenation works if indices are aligned after pd.concat above.
        # Need to verify index alignment or reset index before merge/concat.

        # Group by approach (safer but potentially slower)
        try:
            all_temp_reps = pd.concat(replica_dfs_for_temp, ignore_index=True)
            grouped = all_temp_reps.groupby(['domain_id', 'resid', 'resname'])
            # Calculate mean RMSF within each group
            avg_rmsf_series = grouped[f"rmsf_{temp_str}"].mean()
            # Convert back to DataFrame
            avg_df_temp = avg_rmsf_series.reset_index()
            # Rename column for consistency? Let's keep rmsf_{temp_str} for now.
            # avg_df_temp = avg_df_temp.rename(columns={f"rmsf_{temp_str}": f"rmsf_avg_{temp_str}"})

            if not avg_df_temp.empty:
                 replica_average_data[temp_str] = avg_df_temp
                 all_averages_list.append(avg_df_temp.rename(columns={f"rmsf_{temp_str}": f"rmsf_avg_at_{temp_str}"})) # Rename for merge
                 logging.info(f"Calculated replica average for T={temp_str}: {len(avg_df_temp)} rows")
            else:
                 logging.warning(f"Replica average calculation resulted in empty DataFrame for T={temp_str}")

        except Exception as e:
             logging.error(f"Failed to calculate replica average for T={temp_str}: {e}", exc_info=True)


    # 3. Calculate Overall Temperature Average
    overall_average_df = None
    if not all_averages_list:
        logging.error("No replica averages available to calculate overall temperature average.")
    else:
        try:
            # Start with the first temperature's average DF
            overall_average_df = all_averages_list[0][['domain_id', 'resid', 'resname', f"rmsf_avg_at_{sorted(list(temps))[0]}"]]
            # Iteratively merge other temperatures
            for i in range(1, len(all_averages_list)):
                temp_str = sorted(list(temps))[i]
                merge_df = all_averages_list[i][['domain_id', 'resid', f"rmsf_avg_at_{temp_str}"]]
                overall_average_df = pd.merge(overall_average_df, merge_df, on=['domain_id', 'resid'], how='outer')

            # Calculate the mean across all 'rmsf_avg_at_TEMP' columns
            rmsf_avg_cols = [col for col in overall_average_df.columns if col.startswith("rmsf_avg_at_")]
            if rmsf_avg_cols:
                 overall_average_df['rmsf_average'] = overall_average_df[rmsf_avg_cols].mean(axis=1, skipna=True)
                 # Optionally remove the per-temp columns after calculating overall average
                 # overall_average_df = overall_average_df.drop(columns=rmsf_avg_cols)
                 logging.info(f"Calculated overall temperature average: {len(overall_average_df)} rows")
            else:
                 logging.error("No average RMSF columns found to calculate overall average.")
                 overall_average_df = None # Reset if calculation failed

        except Exception as e:
            logging.error(f"Failed to calculate overall temperature average: {e}", exc_info=True)
            overall_average_df = None

    return final_replica_data, replica_average_data, overall_average_df


def save_rmsf_results(output_dir: str, config: Dict[str, Any],
                      replica_data: Optional[Dict],
                      replica_avg_data: Optional[Dict],
                      overall_avg_data: Optional[pd.DataFrame]):
    """
    Saves all processed RMSF data to CSV files according to configuration.

    Args:
        output_dir (str): Base output directory.
        config (Dict[str, Any]): Global configuration.
        replica_data (Optional[Dict]): Aggregated raw replica data.
        replica_avg_data (Optional[Dict]): Aggregated replica average data.
        overall_avg_data (Optional[pd.DataFrame]): Overall temperature average data.
    """
    flatten_dirs = config.get("output", {}).get("flatten_rmsf_dirs", True)
    rmsf_base = os.path.join(output_dir, "RMSF")

    # 1. Save Raw Replica Data
    if replica_data:
        logging.info("Saving raw replica RMSF data...")
        for temp_str, replica_dict in replica_data.items():
            for replica_str, df in replica_dict.items():
                 try:
                     path = get_rmsf_output_path(output_dir, 'replica', flatten_dirs,
                                                 temperature=temp_str, replica=replica_str)
                     save_dataframe_csv(df, path)
                 except Exception as e:
                      logging.error(f"Failed to save RMSF for T={temp_str}, R={replica_str}: {e}")
    else:
         logging.warning("No raw replica RMSF data to save.")

    # 2. Save Replica Average Data
    if replica_avg_data:
        logging.info("Saving replica average RMSF data...")
        for temp_str, df in replica_avg_data.items():
            try:
                path = get_rmsf_output_path(output_dir, 'average', flatten_dirs, temperature=temp_str)
                # Ensure column name is consistent as 'rmsf_{temp}' before saving
                df_to_save = df.rename(columns={f"rmsf_{temp_str}": f"rmsf_{temp_str}"}, errors='ignore') # No change needed if correct
                save_dataframe_csv(df_to_save, path)
            except Exception as e:
                logging.error(f"Failed to save replica average RMSF for T={temp_str}: {e}")
    else:
         logging.warning("No replica average RMSF data to save.")

    # 3. Save Overall Average Data
    if overall_avg_data is not None and not overall_avg_data.empty:
        logging.info("Saving overall temperature average RMSF data...")
        try:
             path = get_rmsf_output_path(output_dir, 'average', flatten_dirs) # No temp specified
             # Ensure the column is named 'rmsf_average'
             df_to_save = overall_avg_data[['domain_id', 'resid', 'resname', 'rmsf_average']].copy()
             save_dataframe_csv(df_to_save, path)
        except KeyError:
             logging.error(f"Column 'rmsf_average' not found in overall average DataFrame. Cannot save.")
        except Exception as e:
             logging.error(f"Failed to save overall average RMSF: {e}")
    else:
         logging.warning("No overall average RMSF data to save.")


--- End of File: ./src/mdcath/metrics/rmsf.py ---

===== FILE: ./scripts/check_environment.py =====
#!/usr/bin/env python3
"""
Checks the environment for required external tools (DSSP, Aposteriori).
"""
import shutil
import subprocess
import sys
from platform import python_version

# Try to import optional dependencies for extra checks
try:
    import pdbUtils
    PDBUTILS_OK = True
except ImportError:
    PDBUTILS_OK = False

def check_tool(tool_name):
    """Checks if a tool exists in the system PATH."""
    path = shutil.which(tool_name)
    if path:
        print(f"[ OK ] Found '{tool_name}' at: {path}")
        # Optionally, try running with --version if common
        try:
            result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=5, check=False)
            if result.returncode == 0 and result.stdout:
                 print(f"       Version info: {result.stdout.strip().splitlines()[0]}") # Show first line
            # Handle cases where version might be in stderr or require different flags
            elif result.returncode == 0 and result.stderr:
                 print(f"       Version info: {result.stderr.strip().splitlines()[0]}")
            # Special case for older DSSP/mkdssp that might just print help or fail
            elif tool_name in ['dssp', 'mkdssp'] and result.returncode != 0:
                 print(f"       Could not get version via --version (normal for some DSSP versions).")
            # elif result.returncode !=0 :
            #      print(f"       '{tool_name} --version' exited with code {result.returncode}")

        except Exception:
             print(f"       Could not determine version for {tool_name}.")
        return True
    else:
        print(f"[FAIL] Command '{tool_name}' not found in PATH.")
        return False

def main():
    print("--- Environment Check for mdcath-processor ---")
    print(f"Python version: {python_version()}")

    print("\nChecking external command-line tools:")
    dssp_found = check_tool("dssp")
    mkdssp_found = check_tool("mkdssp")
    aposteriori_found = check_tool("make-frame-dataset") # Aposteriori's command

    print("\nChecking optional Python libraries:")
    if PDBUTILS_OK:
        print("[ OK ] Found 'pdbUtils' library.")
    else:
        print("[WARN] Optional 'pdbUtils' library not found. PDB cleaning will use fallback.")

    print("\nSummary:")
    final_status_ok = True
    if not (dssp_found or mkdssp_found):
        print("\n[ERROR] DSSP executable ('dssp' or 'mkdssp') is required but not found in PATH.")
        print("        Please install DSSP (e.g., 'conda install -c salilab dssp') and ensure it's accessible.")
        final_status_ok = False

    # Check Aposteriori installation (assuming voxelization is generally enabled)
    # Could add a config check here if needed
    if not aposteriori_found:
        print("\n[ERROR] Aposteriori ('make-frame-dataset' command) is required but not found.")
        print("        Please install Aposteriori ('pip install aposteriori').")
        final_status_ok = False

    if final_status_ok:
         print("\nRequired external tools seem to be available.")
         if not PDBUTILS_OK:
             print("Note: Using fallback PDB cleaning due to missing 'pdbUtils'. Install 'pdbUtils' for potentially better results.")
         print("Environment check passed.")
         sys.exit(0)
    else:
         print("\nEnvironment check failed due to missing required external tools.")
         sys.exit(1)


if __name__ == "__main__":
    main()

--- End of File: ./scripts/check_environment.py ---

===== FILE: ./README.md =====
# 🧪 mdCATH Dataset Processor

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)

A comprehensive Python package for processing the mdCATH protein dynamics dataset. It extracts simulation data, cleans structures, calculates metrics and properties, builds ML-ready feature sets, performs voxelization, and generates high-quality visualizations.

## Features

*   Reads mdCATH HDF5 files
*   Cleans PDB structures (using pdbUtils or internal fallback)
*   Calculates DSSP, SASA, Core/Exterior classification, Phi/Psi angles
*   Extracts and averages RMSF data
*   Generates ML-ready CSV feature files per temperature and overall average
*   Voxelizes cleaned structures using `aposteriori`
*   Creates publishable quality visualizations
*   Configurable pipeline execution
*   Parallel processing support

## Installation

```bash
# 1. Clone the repository (if you haven't already)
# git clone <repository-url>
# cd mdcath-processor

# 2. Create and activate a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`

# 3. Install dependencies
pip install -r requirements.txt

# 4. Install external tools:
#    - DSSP: Install `dssp` or `mkdssp` and ensure it's in your system PATH.
#      (e.g., using conda: `conda install -c salilab dssp`)
#    - Aposteriori: Install `aposteriori`
#      pip install aposteriori

# 5. Install the package itself (editable mode recommended for development)
pip install -e .
```

## Usage

The main entry point is `main.py`, driven by a configuration file (`config/default_config.yaml`).

```bash
# Run the full pipeline using the default configuration
mdprocess

# Specify a different configuration file
mdprocess --config path/to/your/config.yaml

# Specify a different output directory
mdprocess --output-dir /path/to/outputs

# Process only a subset of domains
mdprocess --domains 1a02F00 16pkA02

# Adjust the number of parallel cores
mdprocess --num-cores 8
```

See `config/default_config.yaml` for available configuration options.

## Output Structure

The processed data and visualizations will be saved in the specified output directory (default: `./outputs/`):

```
outputs/
├── logs/
├── ML_features/
├── pdbs/
├── frames/
├── RMSF/
├── voxelized/
└── visualizations/
```
(Refer to the full project specification for details on the content of each folder).

## Contributing

[Details on how to contribute - e.g., reporting issues, submitting pull requests]

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

--- End of File: ./README.md ---

===== FILE: ./requirements.txt =====
# Core scientific libraries
h5py>=3.1.0
numpy>=1.26.4 # Adjusted based on modern practice
pandas>=1.3.0 # Adjusted based on modern practice
scikit-learn>=1.0 # For KMeans used in frame selection

# Bioinformatics / Structure handling
biopython>=1.79 # Adjusted based on modern practice
pdbUtils>=2.0 # Assuming a version, adjust if needed

# Configuration and Utilities
PyYAML>=6.0 # Adjusted based on modern practice
tqdm>=4.60.0

# Plotting
matplotlib>=3.5.0 # Adjusted based on modern practice
seaborn>=0.11.0
statsmodels>=0.14.0

# Note: External dependencies must be installed separately:
# - dssp or mkdssp (and in PATH)
# - aposteriori (pip install aposteriori)

--- End of File: ./requirements.txt ---

===== FILE: ./pyproject.toml =====
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "mdcath-processor"
version = "0.1.1" # Incremented version
authors = [
  { name="Your Name", email="your.email@example.com" },
]
description = "Processor for mdCATH dataset to generate ML features and visualizations."
readme = "README.md"
license = { file="LICENSE" }
requires-python = ">=3.9"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Intended Audience :: Science/Research",
    "Topic :: Scientific/Engineering :: Bio-Informatics",
    "Development Status :: 3 - Alpha",
]
dependencies = [
    # Copied from requirements.txt - Keep these in sync!
    "h5py>=3.1.0",
    "numpy>=1.26.4", # Updated version
    "pandas>=1.3.0",
    "scikit-learn>=1.0",
    "biopython>=1.79",
    "pdbUtils>=2.0", # Specify correct version if known
    "PyYAML>=6.0",
    "tqdm>=4.60.0",
    "matplotlib>=3.5.0",
    "seaborn>=0.11.0",
    "statsmodels>=0.14.0", # Added dependency
]

[project.urls]
"Homepage" = "https://github.com/your_username/mdcath-processor" # Add your repo URL
"Bug Tracker" = "https://github.com/your_username/mdcath-processor/issues" # Add your repo URL

[project.scripts]
mdprocess = "mdcath.cli:main"

[tool.setuptools.packages.find]
where = ["src"]
--- End of File: ./pyproject.toml ---

===== FILE: ./main.py =====
#!/usr/bin/env python3
"""
Main entry point for mdCATH dataset processing pipeline.
"""
import sys
import logging
import mdcath.cli
# from mdcath.utils.logging_config import setup_logging # Removed initial setup call

# Basic configuration just to catch early errors before full setup
logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')

def run_pipeline():
    """Parses arguments and runs the main pipeline executor."""
    try:
        # cli.main() handles argument parsing, config loading, FULL logging setup, and executor run
        mdcath.cli.main()
        logging.info("mdCATH processing pipeline completed successfully.") # This should use the configured logger
    except SystemExit:
         # Allow SystemExit (e.g., from argparse --help) to pass through without logging failure
         pass
    except Exception as e:
        # Catch broad exceptions here as a last resort
        # Use the root logger, which *should* be configured by cli.main if it got that far
        # If cli.main failed very early, basicConfig might be used.
        logging.critical(f"Pipeline execution failed at the top level: {e}", exc_info=True)
        sys.exit(1) # Exit with a non-zero code to indicate failure

if __name__ == "__main__":
    run_pipeline()
--- End of File: ./main.py ---


=======================================
Preview of Output Files (First 10 Lines)
(Looking in './outputs' directory, ignoring binary files)
=======================================

Found output directory: ./outputs
---------------------------------------------------------
===== FILE: ./outputs/ML_features/final_dataset_temperature_320.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_320,rmsf_log_320,protein_size,normalized_resid,chain,core_exterior,relative_accessibility,dssp,phi,psi,core_exterior_encoded,secondary_structure_encoded,resname_encoded,phi_norm,psi_norm
16pkA02,199,TYR,0.50295705,-0.6872505,208,0.0,A,exterior,0.5017039851248393,C,360.0,-37.8,1,2,18,1.0,-0.21
16pkA02,200,PHE,0.44642836,-0.80647635,208,0.004830917874396135,A,core,0.10618583169133497,H,-71.7,-30.7,0,0,13,-0.3983333333333334,-0.17055555555555554
16pkA02,201,ALA,0.37455827,-0.982008,208,0.00966183574879227,A,exterior,0.6301889716649842,H,-65.9,-38.6,1,0,0,-0.36611111111111116,-0.21444444444444447
16pkA02,202,LYS,0.32780004,-1.1153514,208,0.014492753623188406,A,exterior,0.4713354887007335,H,-61.5,-48.7,1,0,11,-0.3416666666666667,-0.27055555555555555
16pkA02,203,VAL,0.3622904,-1.0153091,208,0.01932367149758454,A,core,0.0,H,-66.6,-27.8,0,0,19,-0.37,-0.15444444444444444
16pkA02,204,LEU,0.34021345,-1.0781821,208,0.024154589371980676,A,exterior,0.23727558413154473,H,-91.3,-1.5,1,0,10,-0.5072222222222222,-0.008333333333333333
16pkA02,205,GLY,0.24910352,-1.3898867,208,0.028985507246376812,A,exterior,0.4301768320902102,S,-92.5,-104.6,1,2,7,-0.5138888888888888,-0.5811111111111111
16pkA02,206,ASN,0.18635844,-1.6800834,208,0.033816425120772944,A,exterior,0.7963738892637345,C,-136.1,67.1,1,2,2,-0.7561111111111111,0.37277777777777776
16pkA02,207,PRO,0.13207284,-2.0244017,208,0.03864734299516908,A,core,0.050876255489561005,C,-64.3,158.0,0,2,14,-0.3572222222222222,0.8777777777777778

--- End of Preview: ./outputs/ML_features/final_dataset_temperature_320.csv ---

===== FILE: ./outputs/ML_features/final_dataset_temperature_average.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_average,rmsf_log,protein_size,normalized_resid,chain,core_exterior,relative_accessibility,dssp,phi,psi,core_exterior_encoded,secondary_structure_encoded,resname_encoded,phi_norm,psi_norm,rmsf_avg_at_320,rmsf_avg_at_348,rmsf_avg_at_379,rmsf_avg_at_413,rmsf_avg_at_450
16pkA02,199,TYR,1.0373558,0.036674958,208,0.0,A,exterior,0.5017039851248393,C,360.0,-37.8,1,2,18,1.0,-0.21,0.50295705,0.80185825,1.5658125,1.0156492,1.300502
16pkA02,200,PHE,0.94003165,-0.061841737,208,0.004830917874396135,A,core,0.10618583169133497,H,-71.7,-30.7,0,0,13,-0.3983333333333334,-0.17055555555555554,0.44642836,0.7277818,1.4438989,0.90346014,1.1785886
16pkA02,201,ALA,0.8620101,-0.14848827,208,0.00966183574879227,A,exterior,0.6301889716649842,H,-65.9,-38.6,1,0,0,-0.36611111111111116,-0.21444444444444447,0.37455827,0.70131874,1.341158,0.8250831,1.0679324
16pkA02,202,LYS,0.7659291,-0.26666567,208,0.014492753623188406,A,exterior,0.4713354887007335,H,-61.5,-48.7,1,0,11,-0.3416666666666667,-0.27055555555555555,0.32780004,0.6089414,1.2206743,0.7227322,0.9494976
16pkA02,203,VAL,0.71202695,-0.3396395,208,0.01932367149758454,A,core,0.0,H,-66.6,-27.8,0,0,19,-0.37,-0.15444444444444444,0.3622904,0.5542306,1.1831158,0.63409096,0.82640696
16pkA02,204,LEU,0.64697456,-0.4354483,208,0.024154589371980676,A,exterior,0.23727558413154473,H,-91.3,-1.5,1,0,10,-0.5072222222222222,-0.008333333333333333,0.34021345,0.49447808,1.1054609,0.5722263,0.722494
16pkA02,205,GLY,0.53998363,-0.6162165,208,0.028985507246376812,A,exterior,0.4301768320902102,S,-92.5,-104.6,1,2,7,-0.5138888888888888,-0.5811111111111111,0.24910352,0.3678082,0.9934122,0.46935278,0.6202416
16pkA02,206,ASN,0.44120312,-0.81824994,208,0.033816425120772944,A,exterior,0.7963738892637345,C,-136.1,67.1,1,2,2,-0.7561111111111111,0.37277777777777776,0.18635844,0.23854928,0.9113768,0.35910138,0.51062953
16pkA02,207,PRO,0.37508807,-0.98059446,208,0.03864734299516908,A,core,0.050876255489561005,C,-64.3,158.0,0,2,14,-0.3572222222222222,0.8777777777777778,0.13207284,0.15936087,0.8605076,0.2872625,0.43623668

--- End of Preview: ./outputs/ML_features/final_dataset_temperature_average.csv ---

===== FILE: ./outputs/ML_features/final_dataset_temperature_379.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_379,rmsf_log_379,protein_size,normalized_resid,chain,core_exterior,relative_accessibility,dssp,phi,psi,core_exterior_encoded,secondary_structure_encoded,resname_encoded,phi_norm,psi_norm
16pkA02,199,TYR,1.5658125,0.44840488,208,0.0,A,exterior,0.5017039851248393,C,360.0,-37.8,1,2,18,1.0,-0.21
16pkA02,200,PHE,1.4438989,0.36734706,208,0.004830917874396135,A,core,0.10618583169133497,H,-71.7,-30.7,0,0,13,-0.3983333333333334,-0.17055555555555554
16pkA02,201,ALA,1.341158,0.29353347,208,0.00966183574879227,A,exterior,0.6301889716649842,H,-65.9,-38.6,1,0,0,-0.36611111111111116,-0.21444444444444447
16pkA02,202,LYS,1.2206743,0.19940339,208,0.014492753623188406,A,exterior,0.4713354887007335,H,-61.5,-48.7,1,0,11,-0.3416666666666667,-0.27055555555555555
16pkA02,203,VAL,1.1831158,0.16815151,208,0.01932367149758454,A,core,0.0,H,-66.6,-27.8,0,0,19,-0.37,-0.15444444444444444
16pkA02,204,LEU,1.1054609,0.10026234,208,0.024154589371980676,A,exterior,0.23727558413154473,H,-91.3,-1.5,1,0,10,-0.5072222222222222,-0.008333333333333333
16pkA02,205,GLY,0.9934122,-0.006609599,208,0.028985507246376812,A,exterior,0.4301768320902102,S,-92.5,-104.6,1,2,7,-0.5138888888888888,-0.5811111111111111
16pkA02,206,ASN,0.9113768,-0.09279888,208,0.033816425120772944,A,exterior,0.7963738892637345,C,-136.1,67.1,1,2,2,-0.7561111111111111,0.37277777777777776
16pkA02,207,PRO,0.8605076,-0.15023282,208,0.03864734299516908,A,core,0.050876255489561005,C,-64.3,158.0,0,2,14,-0.3572222222222222,0.8777777777777778

--- End of Preview: ./outputs/ML_features/final_dataset_temperature_379.csv ---

===== FILE: ./outputs/ML_features/final_dataset_temperature_348.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_348,rmsf_log_348,protein_size,normalized_resid,chain,core_exterior,relative_accessibility,dssp,phi,psi,core_exterior_encoded,secondary_structure_encoded,resname_encoded,phi_norm,psi_norm
16pkA02,199,TYR,0.80185825,-0.22082345,208,0.0,A,exterior,0.5017039851248393,C,360.0,-37.8,1,2,18,1.0,-0.21
16pkA02,200,PHE,0.7277818,-0.31775406,208,0.004830917874396135,A,core,0.10618583169133497,H,-71.7,-30.7,0,0,13,-0.3983333333333334,-0.17055555555555554
16pkA02,201,ALA,0.70131874,-0.3547928,208,0.00966183574879227,A,exterior,0.6301889716649842,H,-65.9,-38.6,1,0,0,-0.36611111111111116,-0.21444444444444447
16pkA02,202,LYS,0.6089414,-0.49603328,208,0.014492753623188406,A,exterior,0.4713354887007335,H,-61.5,-48.7,1,0,11,-0.3416666666666667,-0.27055555555555555
16pkA02,203,VAL,0.5542306,-0.5901745,208,0.01932367149758454,A,core,0.0,H,-66.6,-27.8,0,0,19,-0.37,-0.15444444444444444
16pkA02,204,LEU,0.49447808,-0.7042525,208,0.024154589371980676,A,exterior,0.23727558413154473,H,-91.3,-1.5,1,0,10,-0.5072222222222222,-0.008333333333333333
16pkA02,205,GLY,0.3678082,-1.0001937,208,0.028985507246376812,A,exterior,0.4301768320902102,S,-92.5,-104.6,1,2,7,-0.5138888888888888,-0.5811111111111111
16pkA02,206,ASN,0.23854928,-1.4331794,208,0.033816425120772944,A,exterior,0.7963738892637345,C,-136.1,67.1,1,2,2,-0.7561111111111111,0.37277777777777776
16pkA02,207,PRO,0.15936087,-1.8365841,208,0.03864734299516908,A,core,0.050876255489561005,C,-64.3,158.0,0,2,14,-0.3572222222222222,0.8777777777777778

--- End of Preview: ./outputs/ML_features/final_dataset_temperature_348.csv ---

===== FILE: ./outputs/ML_features/final_dataset_temperature_450.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_450,rmsf_log_450,protein_size,normalized_resid,chain,core_exterior,relative_accessibility,dssp,phi,psi,core_exterior_encoded,secondary_structure_encoded,resname_encoded,phi_norm,psi_norm
16pkA02,199,TYR,1.300502,0.2627503,208,0.0,A,exterior,0.5017039851248393,C,360.0,-37.8,1,2,18,1.0,-0.21
16pkA02,200,PHE,1.1785886,0.16431764,208,0.004830917874396135,A,core,0.10618583169133497,H,-71.7,-30.7,0,0,13,-0.3983333333333334,-0.17055555555555554
16pkA02,201,ALA,1.0679324,0.06572442,208,0.00966183574879227,A,exterior,0.6301889716649842,H,-65.9,-38.6,1,0,0,-0.36611111111111116,-0.21444444444444447
16pkA02,202,LYS,0.9494976,-0.051822297,208,0.014492753623188406,A,exterior,0.4713354887007335,H,-61.5,-48.7,1,0,11,-0.3416666666666667,-0.27055555555555555
16pkA02,203,VAL,0.82640696,-0.19066794,208,0.01932367149758454,A,core,0.0,H,-66.6,-27.8,0,0,19,-0.37,-0.15444444444444444
16pkA02,204,LEU,0.722494,-0.32504618,208,0.024154589371980676,A,exterior,0.23727558413154473,H,-91.3,-1.5,1,0,10,-0.5072222222222222,-0.008333333333333333
16pkA02,205,GLY,0.6202416,-0.47764623,208,0.028985507246376812,A,exterior,0.4301768320902102,S,-92.5,-104.6,1,2,7,-0.5138888888888888,-0.5811111111111111
16pkA02,206,ASN,0.51062953,-0.6721109,208,0.033816425120772944,A,exterior,0.7963738892637345,C,-136.1,67.1,1,2,2,-0.7561111111111111,0.37277777777777776
16pkA02,207,PRO,0.43623668,-0.82957035,208,0.03864734299516908,A,core,0.050876255489561005,C,-64.3,158.0,0,2,14,-0.3572222222222222,0.8777777777777778

--- End of Preview: ./outputs/ML_features/final_dataset_temperature_450.csv ---

===== FILE: ./outputs/ML_features/final_dataset_temperature_413.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_413,rmsf_log_413,protein_size,normalized_resid,chain,core_exterior,relative_accessibility,dssp,phi,psi,core_exterior_encoded,secondary_structure_encoded,resname_encoded,phi_norm,psi_norm
16pkA02,199,TYR,1.0156492,0.015528013,208,0.0,A,exterior,0.5017039851248393,C,360.0,-37.8,1,2,18,1.0,-0.21
16pkA02,200,PHE,0.90346014,-0.10152329,208,0.004830917874396135,A,core,0.10618583169133497,H,-71.7,-30.7,0,0,13,-0.3983333333333334,-0.17055555555555554
16pkA02,201,ALA,0.8250831,-0.1922712,208,0.00966183574879227,A,exterior,0.6301889716649842,H,-65.9,-38.6,1,0,0,-0.36611111111111116,-0.21444444444444447
16pkA02,202,LYS,0.7227322,-0.32471654,208,0.014492753623188406,A,exterior,0.4713354887007335,H,-61.5,-48.7,1,0,11,-0.3416666666666667,-0.27055555555555555
16pkA02,203,VAL,0.63409096,-0.45556286,208,0.01932367149758454,A,core,0.0,H,-66.6,-27.8,0,0,19,-0.37,-0.15444444444444444
16pkA02,204,LEU,0.5722263,-0.55822074,208,0.024154589371980676,A,exterior,0.23727558413154473,H,-91.3,-1.5,1,0,10,-0.5072222222222222,-0.008333333333333333
16pkA02,205,GLY,0.46935278,-0.7564006,208,0.028985507246376812,A,exterior,0.4301768320902102,S,-92.5,-104.6,1,2,7,-0.5138888888888888,-0.5811111111111111
16pkA02,206,ASN,0.35910138,-1.0241505,208,0.033816425120772944,A,exterior,0.7963738892637345,C,-136.1,67.1,1,2,2,-0.7561111111111111,0.37277777777777776
16pkA02,207,PRO,0.2872625,-1.2473588,208,0.03864734299516908,A,core,0.050876255489561005,C,-64.3,158.0,0,2,14,-0.3572222222222222,0.8777777777777778

--- End of Preview: ./outputs/ML_features/final_dataset_temperature_413.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature450.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_450
1a02F00,140,ARG,1.4358152
1a02F00,141,ARG,1.2744541
1a02F00,142,ILE,1.1265574
1a02F00,143,ARG,1.0099715
1a02F00,144,ARG,0.9081847
1a02F00,145,GLU,0.8102415
1a02F00,146,ARG,0.77996707
1a02F00,147,ASN,0.7524421
1a02F00,148,LYS,0.75123334

--- End of Preview: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature450.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature320.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_320
1a02F00,140,ARG,1.2454878
1a02F00,141,ARG,1.1503527
1a02F00,142,ILE,0.9713822
1a02F00,143,ARG,0.88364774
1a02F00,144,ARG,0.87333214
1a02F00,145,GLU,0.81107813
1a02F00,146,ARG,0.81622756
1a02F00,147,ASN,0.74003917
1a02F00,148,LYS,0.6513919

--- End of Preview: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature320.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature413.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_413
1a02F00,140,ARG,1.4717748
1a02F00,141,ARG,1.3274008
1a02F00,142,ILE,1.2008092
1a02F00,143,ARG,1.0682749
1a02F00,144,ARG,0.9799862
1a02F00,145,GLU,0.87632394
1a02F00,146,ARG,0.89807016
1a02F00,147,ASN,0.88627636
1a02F00,148,LYS,0.892927

--- End of Preview: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature413.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature348.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_348
1a02F00,140,ARG,1.1155102
1a02F00,141,ARG,1.1234472
1a02F00,142,ILE,1.0818914
1a02F00,143,ARG,0.9480402
1a02F00,144,ARG,0.8493563
1a02F00,145,GLU,0.801841
1a02F00,146,ARG,0.8203058
1a02F00,147,ASN,0.82727623
1a02F00,148,LYS,0.721029

--- End of Preview: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature348.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature379.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_379
1a02F00,140,ARG,1.3483015
1a02F00,141,ARG,1.2063309
1a02F00,142,ILE,1.0590104
1a02F00,143,ARG,0.995455
1a02F00,144,ARG,0.908797
1a02F00,145,GLU,0.8401951
1a02F00,146,ARG,0.80026555
1a02F00,147,ASN,0.8096612
1a02F00,148,LYS,0.787525

--- End of Preview: ./outputs/RMSF/replicas/replica_2/rmsf_replica2_temperature379.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature320.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_320
1a02F00,140,ARG,1.2817839
1a02F00,141,ARG,1.1480439
1a02F00,142,ILE,1.2196033
1a02F00,143,ARG,1.168149
1a02F00,144,ARG,1.0673517
1a02F00,145,GLU,1.1154531
1a02F00,146,ARG,1.1085513
1a02F00,147,ASN,1.0731494
1a02F00,148,LYS,1.0370867

--- End of Preview: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature320.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature413.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_413
1a02F00,140,ARG,1.497703
1a02F00,141,ARG,1.3020372
1a02F00,142,ILE,1.1175009
1a02F00,143,ARG,0.9397426
1a02F00,144,ARG,0.7905788
1a02F00,145,GLU,0.67115253
1a02F00,146,ARG,0.5593437
1a02F00,147,ASN,0.6437585
1a02F00,148,LYS,0.70599425

--- End of Preview: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature413.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature379.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_379
1a02F00,140,ARG,1.1203154
1a02F00,141,ARG,0.9764169
1a02F00,142,ILE,0.80038136
1a02F00,143,ARG,0.8490753
1a02F00,144,ARG,0.85044754
1a02F00,145,GLU,0.6937541
1a02F00,146,ARG,0.6799402
1a02F00,147,ASN,0.73019505
1a02F00,148,LYS,0.63617295

--- End of Preview: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature379.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature348.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_348
1a02F00,140,ARG,1.2450223
1a02F00,141,ARG,1.1191647
1a02F00,142,ILE,1.0976031
1a02F00,143,ARG,1.0333469
1a02F00,144,ARG,0.9626813
1a02F00,145,GLU,0.87042046
1a02F00,146,ARG,0.8643918
1a02F00,147,ASN,0.7965814
1a02F00,148,LYS,0.7522541

--- End of Preview: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature348.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature450.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_450
1a02F00,140,ARG,1.6130577
1a02F00,141,ARG,1.4880637
1a02F00,142,ILE,1.3673811
1a02F00,143,ARG,1.2330419
1a02F00,144,ARG,1.1511005
1a02F00,145,GLU,1.055495
1a02F00,146,ARG,1.0056403
1a02F00,147,ASN,0.93653464
1a02F00,148,LYS,0.85119766

--- End of Preview: ./outputs/RMSF/replicas/replica_4/rmsf_replica4_temperature450.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature348.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_348
1a02F00,140,ARG,1.4136652
1a02F00,141,ARG,1.2298706
1a02F00,142,ILE,1.1203903
1a02F00,143,ARG,0.91970927
1a02F00,144,ARG,0.9102142
1a02F00,145,GLU,0.8322756
1a02F00,146,ARG,0.85446346
1a02F00,147,ASN,0.91084605
1a02F00,148,LYS,0.8513837

--- End of Preview: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature348.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature379.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_379
1a02F00,140,ARG,1.5595698
1a02F00,141,ARG,1.4415306
1a02F00,142,ILE,1.2706891
1a02F00,143,ARG,1.193445
1a02F00,144,ARG,1.1306113
1a02F00,145,GLU,1.0790689
1a02F00,146,ARG,1.0506817
1a02F00,147,ASN,1.0153743
1a02F00,148,LYS,0.9771671

--- End of Preview: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature379.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature413.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_413
1a02F00,140,ARG,1.326726
1a02F00,141,ARG,1.1562042
1a02F00,142,ILE,1.0098339
1a02F00,143,ARG,0.84797853
1a02F00,144,ARG,0.8022058
1a02F00,145,GLU,0.7602433
1a02F00,146,ARG,0.81178796
1a02F00,147,ASN,0.8335515
1a02F00,148,LYS,0.8066539

--- End of Preview: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature413.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature320.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_320
1a02F00,140,ARG,1.4892555
1a02F00,141,ARG,1.3377619
1a02F00,142,ILE,1.2658004
1a02F00,143,ARG,1.2484705
1a02F00,144,ARG,1.2105318
1a02F00,145,GLU,1.1763273
1a02F00,146,ARG,1.1883858
1a02F00,147,ASN,1.2583191
1a02F00,148,LYS,1.3047016

--- End of Preview: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature320.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature450.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_450
1a02F00,140,ARG,1.6197017
1a02F00,141,ARG,1.4491435
1a02F00,142,ILE,1.3006016
1a02F00,143,ARG,1.1609293
1a02F00,144,ARG,1.0347358
1a02F00,145,GLU,0.9325392
1a02F00,146,ARG,0.88351625
1a02F00,147,ASN,0.83957374
1a02F00,148,LYS,0.8019758

--- End of Preview: ./outputs/RMSF/replicas/replica_0/rmsf_replica0_temperature450.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature348.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_348
1a02F00,140,ARG,1.2627746
1a02F00,141,ARG,1.1204895
1a02F00,142,ILE,1.0167179
1a02F00,143,ARG,0.88645005
1a02F00,144,ARG,0.8030989
1a02F00,145,GLU,0.73298407
1a02F00,146,ARG,0.67856854
1a02F00,147,ASN,0.75269574
1a02F00,148,LYS,0.7710494

--- End of Preview: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature348.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature413.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_413
1a02F00,140,ARG,1.52004
1a02F00,141,ARG,1.339702
1a02F00,142,ILE,1.1846218
1a02F00,143,ARG,1.0579318
1a02F00,144,ARG,0.9535355
1a02F00,145,GLU,0.8424857
1a02F00,146,ARG,0.8474073
1a02F00,147,ASN,0.8399177
1a02F00,148,LYS,0.79528415

--- End of Preview: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature413.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature450.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_450
1a02F00,140,ARG,1.5987595
1a02F00,141,ARG,1.4505792
1a02F00,142,ILE,1.2916604
1a02F00,143,ARG,1.1535153
1a02F00,144,ARG,1.0607121
1a02F00,145,GLU,0.9679756
1a02F00,146,ARG,0.92190367
1a02F00,147,ASN,0.90764093
1a02F00,148,LYS,0.91017646

--- End of Preview: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature450.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature320.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_320
1a02F00,140,ARG,0.985484
1a02F00,141,ARG,0.869053
1a02F00,142,ILE,0.77601635
1a02F00,143,ARG,0.83328027
1a02F00,144,ARG,0.8023455
1a02F00,145,GLU,0.86390454
1a02F00,146,ARG,0.8319781
1a02F00,147,ASN,0.9175232
1a02F00,148,LYS,0.84193236

--- End of Preview: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature320.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature379.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_379
1a02F00,140,ARG,1.4677473
1a02F00,141,ARG,1.328752
1a02F00,142,ILE,1.1845002
1a02F00,143,ARG,1.0515709
1a02F00,144,ARG,1.0190299
1a02F00,145,GLU,0.92949045
1a02F00,146,ARG,0.8960116
1a02F00,147,ASN,0.85469925
1a02F00,148,LYS,0.79912335

--- End of Preview: ./outputs/RMSF/replicas/replica_1/rmsf_replica1_temperature379.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature450.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_450
1a02F00,140,ARG,1.503264
1a02F00,141,ARG,1.3511001
1a02F00,142,ILE,1.2149286
1a02F00,143,ARG,1.052324
1a02F00,144,ARG,0.96118987
1a02F00,145,GLU,0.862865
1a02F00,146,ARG,0.8545662
1a02F00,147,ASN,0.86698693
1a02F00,148,LYS,0.8489278

--- End of Preview: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature450.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature413.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_413
1a02F00,140,ARG,1.7088464
1a02F00,141,ARG,1.5599184
1a02F00,142,ILE,1.4144981
1a02F00,143,ARG,1.2491521
1a02F00,144,ARG,1.139499
1a02F00,145,GLU,1.0115646
1a02F00,146,ARG,0.97736806
1a02F00,147,ASN,0.9297871
1a02F00,148,LYS,0.89519817

--- End of Preview: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature413.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature320.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_320
1a02F00,140,ARG,1.587804
1a02F00,141,ARG,1.4460127
1a02F00,142,ILE,1.3345162
1a02F00,143,ARG,1.2456694
1a02F00,144,ARG,1.1207356
1a02F00,145,GLU,1.0337538
1a02F00,146,ARG,0.9209151
1a02F00,147,ASN,0.8699527
1a02F00,148,LYS,0.75888497

--- End of Preview: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature320.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature348.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_348
1a02F00,140,ARG,1.3467407
1a02F00,141,ARG,1.1742688
1a02F00,142,ILE,1.046751
1a02F00,143,ARG,0.8777708
1a02F00,144,ARG,0.77434254
1a02F00,145,GLU,0.6368366
1a02F00,146,ARG,0.5977424
1a02F00,147,ASN,0.6163674
1a02F00,148,LYS,0.62640876

--- End of Preview: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature348.csv ---

===== FILE: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature379.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_379
1a02F00,140,ARG,1.2324042
1a02F00,141,ARG,1.0714487
1a02F00,142,ILE,0.93857217
1a02F00,143,ARG,0.85995346
1a02F00,144,ARG,0.79134035
1a02F00,145,GLU,0.7030264
1a02F00,146,ARG,0.72894955
1a02F00,147,ASN,0.7142074
1a02F00,148,LYS,0.7077183

--- End of Preview: ./outputs/RMSF/replicas/replica_3/rmsf_replica3_temperature379.csv ---

===== FILE: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature348.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_348
16pkA02,199,TYR,0.80185825
16pkA02,200,PHE,0.7277818
16pkA02,201,ALA,0.70131874
16pkA02,202,LYS,0.6089414
16pkA02,203,VAL,0.5542306
16pkA02,204,LEU,0.49447808
16pkA02,205,GLY,0.3678082
16pkA02,206,ASN,0.23854928
16pkA02,207,PRO,0.15936087

--- End of Preview: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature348.csv ---

===== FILE: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature320.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_320
16pkA02,199,TYR,0.50295705
16pkA02,200,PHE,0.44642836
16pkA02,201,ALA,0.37455827
16pkA02,202,LYS,0.32780004
16pkA02,203,VAL,0.3622904
16pkA02,204,LEU,0.34021345
16pkA02,205,GLY,0.24910352
16pkA02,206,ASN,0.18635844
16pkA02,207,PRO,0.13207284

--- End of Preview: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature320.csv ---

===== FILE: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature413.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_413
16pkA02,199,TYR,1.0156492
16pkA02,200,PHE,0.90346014
16pkA02,201,ALA,0.8250831
16pkA02,202,LYS,0.7227322
16pkA02,203,VAL,0.63409096
16pkA02,204,LEU,0.5722263
16pkA02,205,GLY,0.46935278
16pkA02,206,ASN,0.35910138
16pkA02,207,PRO,0.2872625

--- End of Preview: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature413.csv ---

===== FILE: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature379.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_379
16pkA02,199,TYR,1.5658125
16pkA02,200,PHE,1.4438989
16pkA02,201,ALA,1.341158
16pkA02,202,LYS,1.2206743
16pkA02,203,VAL,1.1831158
16pkA02,204,LEU,1.1054609
16pkA02,205,GLY,0.9934122
16pkA02,206,ASN,0.9113768
16pkA02,207,PRO,0.8605076

--- End of Preview: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature379.csv ---

===== FILE: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature450.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_450
16pkA02,199,TYR,1.300502
16pkA02,200,PHE,1.1785886
16pkA02,201,ALA,1.0679324
16pkA02,202,LYS,0.9494976
16pkA02,203,VAL,0.82640696
16pkA02,204,LEU,0.722494
16pkA02,205,GLY,0.6202416
16pkA02,206,ASN,0.51062953
16pkA02,207,PRO,0.43623668

--- End of Preview: ./outputs/RMSF/replica_average/rmsf_replica_average_temperature450.csv ---

===== FILE: ./outputs/RMSF/replica_average/rmsf_all_temperatures_all_replicas.csv (First 10 lines) =====
domain_id,resid,resname,rmsf_average
16pkA02,199,TYR,1.0373558
16pkA02,200,PHE,0.94003165
16pkA02,201,ALA,0.8620101
16pkA02,202,LYS,0.7659291
16pkA02,203,VAL,0.71202695
16pkA02,204,LEU,0.64697456
16pkA02,205,GLY,0.53998363
16pkA02,206,ASN,0.44120312
16pkA02,207,PRO,0.37508807

--- End of Preview: ./outputs/RMSF/replica_average/rmsf_all_temperatures_all_replicas.csv ---

===== FILE: ./outputs/pdbs/16pkA02.pdb (First 10 lines) =====
CRYST1    1.000    1.000    1.000  90.00  90.00  90.00 P 1           1
ATOM      1 CAY  TYR A 199       3.248 -19.246  11.624  0.00  0.00           C  
ATOM      2 HY1  TYR A 199       4.166 -18.932  11.870  0.00  0.00           H  
ATOM      3 HY2  TYR A 199       3.298 -19.863  10.839  0.00  0.00           H  
ATOM      4 HY3  TYR A 199       2.829 -19.715  12.401  0.00  0.00           H  
ATOM      5 CY   TYR A 199       2.688 -18.454  11.381  0.00  0.00           C  
ATOM      6 OY   TYR A 199       1.771 -18.767  11.135  0.00  0.00           O  
ATOM      7 N    TYR A 199       3.241 -17.625  11.468  1.00  0.00           N  
ATOM      8 HN   TYR A 199       4.158 -17.312  11.714  0.00  0.00           H  
ATOM      9 CA   TYR A 199       2.427 -16.474  11.114  1.00  0.00           C  

--- End of Preview: ./outputs/pdbs/16pkA02.pdb ---

===== FILE: ./outputs/pdbs/1a02F00.pdb (First 10 lines) =====
CRYST1    1.000    1.000    1.000  90.00  90.00  90.00 P 1           1
ATOM      1 CAY  ARG A 140      16.299   1.661 -33.895  0.00  0.00           C  
ATOM      2 HY1  ARG A 140      17.097   2.095 -33.477  0.00  0.00           H  
ATOM      3 HY2  ARG A 140      15.484   1.847 -33.347  0.00  0.00           H  
ATOM      4 HY3  ARG A 140      16.171   2.006 -34.825  0.00  0.00           H  
ATOM      5 CY   ARG A 140      16.448   0.673 -33.933  0.00  0.00           C  
ATOM      6 OY   ARG A 140      15.650   0.239 -34.350  0.00  0.00           O  
ATOM      7 N    ARG A 140      17.343   0.463 -33.539  1.00  0.00           N  
ATOM      8 HN   ARG A 140      18.141   0.897 -33.121  0.00  0.00           H  
ATOM      9 CA   ARG A 140      17.565  -1.011 -33.595  1.00  0.00           C  

--- End of Preview: ./outputs/pdbs/1a02F00.pdb ---

===== FILE: ./outputs/voxelized/mdcath_voxelized.hdf5 (Skipped - Detected as non-text) =====

===== FILE: ./outputs/logs/pipeline.log (First 10 lines) =====
2025-03-30 18:56:16,779 - root - INFO - Determined number of cores to use: 14
2025-03-30 18:56:16,779 - root - INFO - Starting mdCATH processing pipeline...
2025-03-30 18:56:16,779 - root - INFO - Using output directory: ./outputs
2025-03-30 18:56:16,779 - root - INFO - Using specified list of 2 domains.
2025-03-30 18:56:16,779 - root - INFO - Processing 2 domains using up to 14 cores...
2025-03-30 18:56:16,779 - root - INFO - Using parallel processing with 14 cores.
2025-03-30 18:56:16,779 - root - INFO - Running in parallel with 2 cores (chunksize=1).
2025-03-30 18:56:16,787 - root - INFO - Using pdbUtils for PDB cleaning.
2025-03-30 18:56:16,787 - root - INFO - Using pdbUtils for PDB cleaning.
2025-03-30 18:56:16,787 - root - INFO - Found DSSP executable 'dssp' at: /usr/bin/dssp

--- End of Preview: ./outputs/logs/pipeline.log ---

===== FILE: ./outputs/visualizations/rmsf_violin_plot.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/amino_acid_rmsf_violin_plot.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/replica_variance_plot.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/temperature_summary_heatmap.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/amino_acid_rmsf_colored.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/pipeline_summary_report.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/rmsf_density_plots.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/feature_correlation_plot.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/temperature_average_summary.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/ml_features_analysis.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/dssp_rmsf_correlation_plot.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/rmsf_histogram_separated.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/ml_features_additional_analysis.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/visualizations/voxelization_info.png (Skipped - Detected as non-text) =====

===== FILE: ./outputs/frames/450/replica_2/1a02F00_frame_190.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      30.120  -2.660  31.550  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      29.510  -1.770  31.810  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      31.120  -2.200  31.410  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      30.270  -3.280  32.460  0.00  0.00       H  
ATOM      5 CY   ARG A 140      29.510  -3.200  30.290  0.00  0.00       C  
ATOM      6 OY   ARG A 140      30.130  -3.250  29.250  0.00  0.00       O  
ATOM      7 N    ARG A 140      28.290  -3.470  30.600  1.00  0.00       N  
ATOM      8 HN   ARG A 140      28.010  -3.290  31.530  0.00  0.00       H  
ATOM      9 CA   ARG A 140      27.400  -4.070  29.700  1.00  0.00       C  
ATOM     10 HA   ARG A 140      27.690  -3.980  28.660  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_2/1a02F00_frame_190.pdb ---

===== FILE: ./outputs/frames/450/replica_2/16pkA02_frame_241.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      -8.060 100.810  45.970  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      -8.230 100.060  46.770  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      -7.330 100.500  45.200  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      -9.010 100.960  45.420  0.00  0.00       H  
ATOM      5 CY   TYR A 199      -7.720 102.110  46.470  0.00  0.00       C  
ATOM      6 OY   TYR A 199      -8.170 102.580  47.520  0.00  0.00       O  
ATOM      7 N    TYR A 199      -6.960 102.890  45.680  1.00  0.00       N  
ATOM      8 HN   TYR A 199      -6.790 102.530  44.770  0.00  0.00       H  
ATOM      9 CA   TYR A 199      -6.400 104.200  45.980  1.00  0.00       C  
ATOM     10 HA   TYR A 199      -5.980 104.280  46.970  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_2/16pkA02_frame_241.pdb ---

===== FILE: ./outputs/frames/450/replica_4/16pkA02_frame_365.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     -13.390 -54.780 -66.260  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     -14.390 -54.650 -66.730  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     -13.530 -55.600 -65.530  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     -12.540 -54.950 -66.950  0.00  0.00       H  
ATOM      5 CY   TYR A 199     -13.230 -53.660 -65.370  0.00  0.00       C  
ATOM      6 OY   TYR A 199     -14.130 -53.500 -64.520  0.00  0.00       O  
ATOM      7 N    TYR A 199     -12.220 -52.800 -65.500  1.00  0.00       N  
ATOM      8 HN   TYR A 199     -11.490 -53.160 -66.090  0.00  0.00       H  
ATOM      9 CA   TYR A 199     -12.050 -51.620 -64.710  1.00  0.00       C  
ATOM     10 HA   TYR A 199     -12.420 -51.750 -63.700  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_4/16pkA02_frame_365.pdb ---

===== FILE: ./outputs/frames/450/replica_4/1a02F00_frame_318.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     -35.620 174.090 113.680  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     -34.620 173.630 113.810  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     -36.240 173.820 114.570  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     -35.630 175.180 113.460  0.00  0.00       H  
ATOM      5 CY   ARG A 140     -36.470 173.400 112.640  0.00  0.00       C  
ATOM      6 OY   ARG A 140     -37.710 173.640 112.500  0.00  0.00       O  
ATOM      7 N    ARG A 140     -35.820 172.460 112.110  1.00  0.00       N  
ATOM      8 HN   ARG A 140     -34.820 172.470 112.100  0.00  0.00       H  
ATOM      9 CA   ARG A 140     -36.370 171.530 111.200  1.00  0.00       C  
ATOM     10 HA   ARG A 140     -37.020 171.970 110.450  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_4/1a02F00_frame_318.pdb ---

===== FILE: ./outputs/frames/450/replica_0/1a02F00_frame_313.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      69.290 -80.870 134.810  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      68.990 -79.980 134.220  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      68.520 -81.020 135.600  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      69.280 -81.730 134.110  0.00  0.00       H  
ATOM      5 CY   ARG A 140      70.640 -80.900 135.460  0.00  0.00       C  
ATOM      6 OY   ARG A 140      70.950 -80.020 136.310  0.00  0.00       O  
ATOM      7 N    ARG A 140      71.580 -81.800 135.080  1.00  0.00       N  
ATOM      8 HN   ARG A 140      71.380 -82.470 134.370  0.00  0.00       H  
ATOM      9 CA   ARG A 140      73.000 -81.740 135.410  1.00  0.00       C  
ATOM     10 HA   ARG A 140      73.080 -81.700 136.480  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_0/1a02F00_frame_313.pdb ---

===== FILE: ./outputs/frames/450/replica_0/16pkA02_frame_225.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     172.740 -86.760 -82.160  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     172.050 -85.930 -82.410  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     172.140 -87.650 -81.860  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     173.270 -87.190 -83.040  0.00  0.00       H  
ATOM      5 CY   TYR A 199     173.600 -86.190 -81.100  0.00  0.00       C  
ATOM      6 OY   TYR A 199     173.910 -86.840 -80.160  0.00  0.00       O  
ATOM      7 N    TYR A 199     174.050 -84.970 -81.350  1.00  0.00       N  
ATOM      8 HN   TYR A 199     173.740 -84.440 -82.140  0.00  0.00       H  
ATOM      9 CA   TYR A 199     174.960 -84.230 -80.420  1.00  0.00       C  
ATOM     10 HA   TYR A 199     175.570 -84.930 -79.850  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_0/16pkA02_frame_225.pdb ---

===== FILE: ./outputs/frames/450/replica_1/16pkA02_frame_367.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     191.580  40.710 -98.420  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     192.240  41.610 -98.440  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     191.380  40.640 -97.330  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     190.570  40.920 -98.830  0.00  0.00       H  
ATOM      5 CY   TYR A 199     192.320  39.590 -99.020  0.00  0.00       C  
ATOM      6 OY   TYR A 199     191.820  38.640 -99.710  0.00  0.00       O  
ATOM      7 N    TYR A 199     193.610  39.560 -98.770  1.00  0.00       N  
ATOM      8 HN   TYR A 199     194.110  40.350 -98.420  0.00  0.00       H  
ATOM      9 CA   TYR A 199     194.460  38.430 -99.150  1.00  0.00       C  
ATOM     10 HA   TYR A 199     194.000  37.750 -99.840  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_1/16pkA02_frame_367.pdb ---

===== FILE: ./outputs/frames/450/replica_1/1a02F00_frame_365.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      11.310   2.340 -65.620  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      12.300   2.040 -65.210  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      10.430   2.160 -64.970  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      11.360   3.380 -66.010  0.00  0.00       H  
ATOM      5 CY   ARG A 140      10.970   1.580 -66.860  0.00  0.00       C  
ATOM      6 OY   ARG A 140       9.930   1.880 -67.500  0.00  0.00       O  
ATOM      7 N    ARG A 140      11.820   0.610 -67.300  1.00  0.00       N  
ATOM      8 HN   ARG A 140      12.540   0.520 -66.620  0.00  0.00       H  
ATOM      9 CA   ARG A 140      11.760  -0.220 -68.440  1.00  0.00       C  
ATOM     10 HA   ARG A 140      11.690   0.380 -69.330  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_1/1a02F00_frame_365.pdb ---

===== FILE: ./outputs/frames/450/replica_3/1a02F00_frame_215.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     -52.130  71.050 -64.550  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     -51.620  71.670 -65.320  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     -52.750  70.290 -65.070  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     -51.180  70.600 -64.200  0.00  0.00       H  
ATOM      5 CY   ARG A 140     -52.930  71.850 -63.570  0.00  0.00       C  
ATOM      6 OY   ARG A 140     -53.010  73.070 -63.630  0.00  0.00       O  
ATOM      7 N    ARG A 140     -53.480  71.140 -62.610  1.00  0.00       N  
ATOM      8 HN   ARG A 140     -53.470  70.150 -62.690  0.00  0.00       H  
ATOM      9 CA   ARG A 140     -54.180  71.660 -61.470  1.00  0.00       C  
ATOM     10 HA   ARG A 140     -53.920  72.700 -61.400  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_3/1a02F00_frame_215.pdb ---

===== FILE: ./outputs/frames/450/replica_3/16pkA02_frame_228.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     143.230 140.610  20.650  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     143.820 141.150  21.420  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     143.550 140.740  19.600  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     143.180 139.530  20.920  0.00  0.00       H  
ATOM      5 CY   TYR A 199     141.830 141.140  20.730  0.00  0.00       C  
ATOM      6 OY   TYR A 199     141.070 140.460  21.370  0.00  0.00       O  
ATOM      7 N    TYR A 199     141.470 142.260  20.050  1.00  0.00       N  
ATOM      8 HN   TYR A 199     142.000 142.770  19.380  0.00  0.00       H  
ATOM      9 CA   TYR A 199     140.060 142.520  19.970  1.00  0.00       C  
ATOM     10 HA   TYR A 199     139.460 141.630  19.900  1.00  0.00       H  

--- End of Preview: ./outputs/frames/450/replica_3/16pkA02_frame_228.pdb ---

===== FILE: ./outputs/frames/379/replica_2/16pkA02_frame_117.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      20.940 -23.800  94.810  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      20.110 -24.410  94.400  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      20.410 -23.430  95.710  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      21.870 -24.350  95.060  0.00  0.00       H  
ATOM      5 CY   TYR A 199      21.270 -22.730  93.850  0.00  0.00       C  
ATOM      6 OY   TYR A 199      22.430 -22.590  93.290  0.00  0.00       O  
ATOM      7 N    TYR A 199      20.340 -21.800  93.680  1.00  0.00       N  
ATOM      8 HN   TYR A 199      19.640 -21.890  94.380  0.00  0.00       H  
ATOM      9 CA   TYR A 199      20.480 -20.520  93.050  1.00  0.00       C  
ATOM     10 HA   TYR A 199      20.740 -20.680  92.010  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_2/16pkA02_frame_117.pdb ---

===== FILE: ./outputs/frames/379/replica_2/1a02F00_frame_296.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     118.560  38.420 186.810  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     118.840  37.580 187.470  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     117.470  38.290 186.650  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     118.860  39.450 187.090  0.00  0.00       H  
ATOM      5 CY   ARG A 140     119.210  38.060 185.490  0.00  0.00       C  
ATOM      6 OY   ARG A 140     120.300  37.520 185.640  0.00  0.00       O  
ATOM      7 N    ARG A 140     118.640  38.310 184.330  1.00  0.00       N  
ATOM      8 HN   ARG A 140     117.720  38.690 184.240  0.00  0.00       H  
ATOM      9 CA   ARG A 140     119.300  38.040 183.120  1.00  0.00       C  
ATOM     10 HA   ARG A 140     120.380  38.120 183.190  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_2/1a02F00_frame_296.pdb ---

===== FILE: ./outputs/frames/379/replica_4/1a02F00_frame_24.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      76.150  14.490  74.310  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      75.120  14.800  74.060  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      76.280  14.510  75.410  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      76.390  13.460  73.940  0.00  0.00       H  
ATOM      5 CY   ARG A 140      77.090  15.400  73.580  0.00  0.00       C  
ATOM      6 OY   ARG A 140      77.080  16.600  73.670  0.00  0.00       O  
ATOM      7 N    ARG A 140      77.960  14.810  72.840  1.00  0.00       N  
ATOM      8 HN   ARG A 140      77.930  13.810  72.840  0.00  0.00       H  
ATOM      9 CA   ARG A 140      79.110  15.460  72.130  1.00  0.00       C  
ATOM     10 HA   ARG A 140      79.500  16.200  72.820  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_4/1a02F00_frame_24.pdb ---

===== FILE: ./outputs/frames/379/replica_4/16pkA02_frame_420.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     -41.290 -49.090 -15.060  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     -42.070 -48.320 -14.860  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     -40.570 -48.660 -15.790  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     -40.570 -49.220 -14.220  0.00  0.00       H  
ATOM      5 CY   TYR A 199     -41.950 -50.270 -15.470  0.00  0.00       C  
ATOM      6 OY   TYR A 199     -42.140 -50.330 -16.630  0.00  0.00       O  
ATOM      7 N    TYR A 199     -42.460 -51.130 -14.500  1.00  0.00       N  
ATOM      8 HN   TYR A 199     -42.360 -50.930 -13.530  0.00  0.00       H  
ATOM      9 CA   TYR A 199     -43.190 -52.440 -14.720  1.00  0.00       C  
ATOM     10 HA   TYR A 199     -43.350 -52.620 -15.780  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_4/16pkA02_frame_420.pdb ---

===== FILE: ./outputs/frames/379/replica_0/16pkA02_frame_399.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      56.810 -18.830 161.920  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      56.160 -19.080 162.780  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      56.940 -17.720 161.990  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      57.810 -19.310 161.930  0.00  0.00       H  
ATOM      5 CY   TYR A 199      56.120 -19.130 160.680  0.00  0.00       C  
ATOM      6 OY   TYR A 199      55.020 -18.640 160.460  0.00  0.00       O  
ATOM      7 N    TYR A 199      56.690 -20.000 159.860  1.00  0.00       N  
ATOM      8 HN   TYR A 199      57.450 -20.550 160.190  0.00  0.00       H  
ATOM      9 CA   TYR A 199      56.290 -20.220 158.550  1.00  0.00       C  
ATOM     10 HA   TYR A 199      55.700 -19.420 158.140  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_0/16pkA02_frame_399.pdb ---

===== FILE: ./outputs/frames/379/replica_0/1a02F00_frame_173.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     180.830  33.820 -34.670  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     181.870  34.180 -34.600  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     180.240  34.560 -34.070  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     180.560  33.640 -35.730  0.00  0.00       H  
ATOM      5 CY   ARG A 140     180.740  32.510 -33.970  0.00  0.00       C  
ATOM      6 OY   ARG A 140     180.750  31.430 -34.610  0.00  0.00       O  
ATOM      7 N    ARG A 140     180.850  32.600 -32.600  1.00  0.00       N  
ATOM      8 HN   ARG A 140     180.780  33.470 -32.110  0.00  0.00       H  
ATOM      9 CA   ARG A 140     181.050  31.410 -31.780  1.00  0.00       C  
ATOM     10 HA   ARG A 140     181.770  30.760 -32.270  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_0/1a02F00_frame_173.pdb ---

===== FILE: ./outputs/frames/379/replica_1/1a02F00_frame_221.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     114.280-134.180 101.640  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     113.720-134.380 100.700  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     114.600-135.100 102.190  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     115.320-133.840 101.460  0.00  0.00       H  
ATOM      5 CY   ARG A 140     113.570-133.220 102.460  0.00  0.00       C  
ATOM      6 OY   ARG A 140     113.100-132.260 101.940  0.00  0.00       O  
ATOM      7 N    ARG A 140     113.330-133.400 103.790  1.00  0.00       N  
ATOM      8 HN   ARG A 140     113.540-134.330 104.070  0.00  0.00       H  
ATOM      9 CA   ARG A 140     112.750-132.380 104.660  1.00  0.00       C  
ATOM     10 HA   ARG A 140     112.270-131.580 104.110  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_1/1a02F00_frame_221.pdb ---

===== FILE: ./outputs/frames/379/replica_1/16pkA02_frame_392.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      73.200 -98.060 -40.160  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      74.010 -98.790 -39.980  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      73.570 -97.030 -39.990  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      72.690 -98.250 -41.120  0.00  0.00       H  
ATOM      5 CY   TYR A 199      71.990 -98.280 -39.150  0.00  0.00       C  
ATOM      6 OY   TYR A 199      70.800 -98.180 -39.400  0.00  0.00       O  
ATOM      7 N    TYR A 199      72.500 -98.790 -38.040  1.00  0.00       N  
ATOM      8 HN   TYR A 199      73.460 -98.960 -37.840  0.00  0.00       H  
ATOM      9 CA   TYR A 199      71.490 -99.230 -36.990  1.00  0.00       C  
ATOM     10 HA   TYR A 199      70.630 -99.600 -37.520  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_1/16pkA02_frame_392.pdb ---

===== FILE: ./outputs/frames/379/replica_3/16pkA02_frame_283.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      77.040  -8.300  26.110  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      75.970  -8.540  25.890  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      77.100  -7.770  27.080  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      77.590  -9.260  26.220  0.00  0.00       H  
ATOM      5 CY   TYR A 199      77.710  -7.470  25.160  0.00  0.00       C  
ATOM      6 OY   TYR A 199      78.590  -7.950  24.430  0.00  0.00       O  
ATOM      7 N    TYR A 199      77.360  -6.150  25.010  1.00  0.00       N  
ATOM      8 HN   TYR A 199      76.630  -5.700  25.520  0.00  0.00       H  
ATOM      9 CA   TYR A 199      77.970  -5.240  23.980  1.00  0.00       C  
ATOM     10 HA   TYR A 199      78.420  -5.910  23.250  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_3/16pkA02_frame_283.pdb ---

===== FILE: ./outputs/frames/379/replica_3/1a02F00_frame_32.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      45.630 -11.200  92.560  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      45.710 -10.130  92.270  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      45.650 -11.170  93.670  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      44.680 -11.440  92.030  0.00  0.00       H  
ATOM      5 CY   ARG A 140      46.800 -11.990  92.070  0.00  0.00       C  
ATOM      6 OY   ARG A 140      47.360 -11.630  91.070  0.00  0.00       O  
ATOM      7 N    ARG A 140      46.880 -13.210  92.660  1.00  0.00       N  
ATOM      8 HN   ARG A 140      46.360 -13.420  93.480  0.00  0.00       H  
ATOM      9 CA   ARG A 140      47.700 -14.280  92.220  1.00  0.00       C  
ATOM     10 HA   ARG A 140      48.660 -14.080  91.780  1.00  0.00       H  

--- End of Preview: ./outputs/frames/379/replica_3/1a02F00_frame_32.pdb ---

===== FILE: ./outputs/frames/348/replica_2/16pkA02_frame_298.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     143.600 -31.530  64.830  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     143.750 -30.940  63.900  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     142.810 -31.050  65.440  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     143.360 -32.600  64.650  0.00  0.00       H  
ATOM      5 CY   TYR A 199     144.870 -31.560  65.500  0.00  0.00       C  
ATOM      6 OY   TYR A 199     145.180 -32.280  66.430  0.00  0.00       O  
ATOM      7 N    TYR A 199     145.670 -30.580  65.120  1.00  0.00       N  
ATOM      8 HN   TYR A 199     145.370 -29.800  64.570  0.00  0.00       H  
ATOM      9 CA   TYR A 199     147.050 -30.580  65.510  1.00  0.00       C  
ATOM     10 HA   TYR A 199     147.490 -31.570  65.580  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_2/16pkA02_frame_298.pdb ---

===== FILE: ./outputs/frames/348/replica_2/1a02F00_frame_153.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      62.030  46.380  29.660  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      62.400  45.550  29.030  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      62.400  46.240  30.700  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      62.500  47.280  29.200  0.00  0.00       H  
ATOM      5 CY   ARG A 140      60.570  46.560  29.620  0.00  0.00       C  
ATOM      6 OY   ARG A 140      59.880  46.100  28.690  0.00  0.00       O  
ATOM      7 N    ARG A 140      60.010  47.180  30.620  1.00  0.00       N  
ATOM      8 HN   ARG A 140      60.630  47.650  31.240  0.00  0.00       H  
ATOM      9 CA   ARG A 140      58.610  47.460  30.970  1.00  0.00       C  
ATOM     10 HA   ARG A 140      58.710  47.610  32.030  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_2/1a02F00_frame_153.pdb ---

===== FILE: ./outputs/frames/348/replica_4/16pkA02_frame_347.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      97.090  24.060  64.940  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      97.690  23.220  65.350  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      96.970  24.000  63.830  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      96.120  24.070  65.490  0.00  0.00       H  
ATOM      5 CY   TYR A 199      97.970  25.230  64.950  0.00  0.00       C  
ATOM      6 OY   TYR A 199      98.050  26.080  64.050  0.00  0.00       O  
ATOM      7 N    TYR A 199      98.700  25.410  66.040  1.00  0.00       N  
ATOM      8 HN   TYR A 199      98.690  24.770  66.800  0.00  0.00       H  
ATOM      9 CA   TYR A 199      99.690  26.500  66.140  1.00  0.00       C  
ATOM     10 HA   TYR A 199     100.040  26.570  67.160  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_4/16pkA02_frame_347.pdb ---

===== FILE: ./outputs/frames/348/replica_4/1a02F00_frame_172.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     104.920  -3.200  84.420  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     103.880  -2.810  84.480  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     104.950  -4.060  85.130  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     105.100  -3.440  83.350  0.00  0.00       H  
ATOM      5 CY   ARG A 140     105.840  -2.130  84.860  0.00  0.00       C  
ATOM      6 OY   ARG A 140     106.270  -2.050  85.990  0.00  0.00       O  
ATOM      7 N    ARG A 140     106.260  -1.300  83.940  1.00  0.00       N  
ATOM      8 HN   ARG A 140     105.990  -1.470  82.990  0.00  0.00       H  
ATOM      9 CA   ARG A 140     107.030   0.010  84.260  1.00  0.00       C  
ATOM     10 HA   ARG A 140     107.800  -0.260  84.960  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_4/1a02F00_frame_172.pdb ---

===== FILE: ./outputs/frames/348/replica_0/1a02F00_frame_303.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      89.320 107.460  66.970  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      90.170 107.190  66.290  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      88.430 106.960  66.540  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      89.530 107.180  68.020  0.00  0.00       H  
ATOM      5 CY   ARG A 140      89.070 108.850  66.960  0.00  0.00       C  
ATOM      6 OY   ARG A 140      89.670 109.620  67.740  0.00  0.00       O  
ATOM      7 N    ARG A 140      88.030 109.370  66.260  1.00  0.00       N  
ATOM      8 HN   ARG A 140      87.390 108.790  65.770  0.00  0.00       H  
ATOM      9 CA   ARG A 140      87.790 110.810  66.190  1.00  0.00       C  
ATOM     10 HA   ARG A 140      88.690 111.410  66.190  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_0/1a02F00_frame_303.pdb ---

===== FILE: ./outputs/frames/348/replica_0/16pkA02_frame_65.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     118.340  32.240 -13.070  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     119.150  32.320 -12.320  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     117.940  33.280 -13.170  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     118.560  31.860 -14.090  0.00  0.00       H  
ATOM      5 CY   TYR A 199     117.230  31.320 -12.570  0.00  0.00       C  
ATOM      6 OY   TYR A 199     116.030  31.710 -12.580  0.00  0.00       O  
ATOM      7 N    TYR A 199     117.560  30.180 -12.070  1.00  0.00       N  
ATOM      8 HN   TYR A 199     118.480  29.820 -12.180  0.00  0.00       H  
ATOM      9 CA   TYR A 199     116.660  29.370 -11.230  1.00  0.00       C  
ATOM     10 HA   TYR A 199     115.990  30.000 -10.670  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_0/16pkA02_frame_65.pdb ---

===== FILE: ./outputs/frames/348/replica_1/16pkA02_frame_48.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      76.220  49.700 -21.240  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      75.340  50.300 -20.930  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      76.670  49.310 -20.310  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      75.800  49.030 -22.020  0.00  0.00       H  
ATOM      5 CY   TYR A 199      77.000  50.760 -21.980  0.00  0.00       C  
ATOM      6 OY   TYR A 199      76.570  51.480 -22.860  0.00  0.00       O  
ATOM      7 N    TYR A 199      78.250  50.870 -21.540  1.00  0.00       N  
ATOM      8 HN   TYR A 199      78.760  50.330 -20.870  0.00  0.00       H  
ATOM      9 CA   TYR A 199      78.990  52.000 -22.020  1.00  0.00       C  
ATOM     10 HA   TYR A 199      78.360  52.870 -21.960  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_1/16pkA02_frame_48.pdb ---

===== FILE: ./outputs/frames/348/replica_1/1a02F00_frame_371.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      79.800 -11.960 144.510  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      80.800 -11.490 144.550  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      79.930 -12.880 145.110  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      79.020 -11.330 144.990  0.00  0.00       H  
ATOM      5 CY   ARG A 140      79.410 -12.240 143.030  0.00  0.00       C  
ATOM      6 OY   ARG A 140      78.390 -11.740 142.630  0.00  0.00       O  
ATOM      7 N    ARG A 140      80.280 -12.970 142.290  1.00  0.00       N  
ATOM      8 HN   ARG A 140      81.070 -13.420 142.700  0.00  0.00       H  
ATOM      9 CA   ARG A 140      80.020 -13.300 140.890  1.00  0.00       C  
ATOM     10 HA   ARG A 140      79.170 -12.800 140.460  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_1/1a02F00_frame_371.pdb ---

===== FILE: ./outputs/frames/348/replica_3/16pkA02_frame_133.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      63.030 -17.710  41.330  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      62.450 -17.410  42.230  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      63.970 -18.210  41.630  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      62.400 -18.380  40.700  0.00  0.00       H  
ATOM      5 CY   TYR A 199      63.550 -16.570  40.490  0.00  0.00       C  
ATOM      6 OY   TYR A 199      64.630 -16.680  39.890  0.00  0.00       O  
ATOM      7 N    TYR A 199      62.780 -15.510  40.480  1.00  0.00       N  
ATOM      8 HN   TYR A 199      62.000 -15.460  41.100  0.00  0.00       H  
ATOM      9 CA   TYR A 199      63.020 -14.310  39.630  1.00  0.00       C  
ATOM     10 HA   TYR A 199      63.880 -14.460  39.000  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_3/16pkA02_frame_133.pdb ---

===== FILE: ./outputs/frames/348/replica_3/1a02F00_frame_253.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     153.780   3.490 -33.040  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     154.620   3.990 -33.560  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     152.970   4.210 -32.780  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     153.270   2.740 -33.680  0.00  0.00       H  
ATOM      5 CY   ARG A 140     154.150   2.800 -31.780  0.00  0.00       C  
ATOM      6 OY   ARG A 140     155.240   2.270 -31.600  0.00  0.00       O  
ATOM      7 N    ARG A 140     153.220   2.920 -30.790  1.00  0.00       N  
ATOM      8 HN   ARG A 140     152.420   3.440 -31.090  0.00  0.00       H  
ATOM      9 CA   ARG A 140     153.480   2.630 -29.370  1.00  0.00       C  
ATOM     10 HA   ARG A 140     154.500   2.960 -29.250  1.00  0.00       H  

--- End of Preview: ./outputs/frames/348/replica_3/1a02F00_frame_253.pdb ---

===== FILE: ./outputs/frames/413/replica_2/16pkA02_frame_199.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      43.300  10.300  62.640  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      44.210   9.660  62.620  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      43.320  11.070  63.430  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      43.100  10.810  61.670  0.00  0.00       H  
ATOM      5 CY   TYR A 199      42.080   9.420  63.020  0.00  0.00       C  
ATOM      6 OY   TYR A 199      41.410   9.720  63.950  0.00  0.00       O  
ATOM      7 N    TYR A 199      41.830   8.350  62.220  1.00  0.00       N  
ATOM      8 HN   TYR A 199      42.320   8.260  61.360  0.00  0.00       H  
ATOM      9 CA   TYR A 199      40.660   7.510  62.220  1.00  0.00       C  
ATOM     10 HA   TYR A 199      39.990   7.770  63.020  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_2/16pkA02_frame_199.pdb ---

===== FILE: ./outputs/frames/413/replica_2/1a02F00_frame_172.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     183.280   4.720 205.110  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     184.260   4.750 204.590  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     182.760   3.840 204.660  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     183.430   4.540 206.200  0.00  0.00       H  
ATOM      5 CY   ARG A 140     182.460   5.920 204.830  0.00  0.00       C  
ATOM      6 OY   ARG A 140     182.530   6.520 203.730  0.00  0.00       O  
ATOM      7 N    ARG A 140     181.670   6.340 205.830  1.00  0.00       N  
ATOM      8 HN   ARG A 140     181.850   5.870 206.690  0.00  0.00       H  
ATOM      9 CA   ARG A 140     180.790   7.440 205.540  1.00  0.00       C  
ATOM     10 HA   ARG A 140     180.030   7.090 204.860  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_2/1a02F00_frame_172.pdb ---

===== FILE: ./outputs/frames/413/replica_4/1a02F00_frame_349.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140       0.130 -41.070   6.620  0.00  0.00       C  
ATOM      2 HY1  ARG A 140       0.370 -40.060   6.230  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      -0.080 -41.700   5.720  0.00  0.00       H  
ATOM      4 HY3  ARG A 140       0.990 -41.570   7.110  0.00  0.00       H  
ATOM      5 CY   ARG A 140      -1.030 -41.120   7.470  0.00  0.00       C  
ATOM      6 OY   ARG A 140      -0.940 -41.910   8.430  0.00  0.00       O  
ATOM      7 N    ARG A 140      -2.090 -40.380   7.190  1.00  0.00       N  
ATOM      8 HN   ARG A 140      -1.920 -39.640   6.540  0.00  0.00       H  
ATOM      9 CA   ARG A 140      -3.450 -40.440   7.680  1.00  0.00       C  
ATOM     10 HA   ARG A 140      -4.010 -39.680   7.140  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_4/1a02F00_frame_349.pdb ---

===== FILE: ./outputs/frames/413/replica_4/16pkA02_frame_289.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      46.100 126.260  95.560  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      46.740 126.310  94.650  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      45.150 126.820  95.390  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      45.780 125.220  95.760  0.00  0.00       H  
ATOM      5 CY   TYR A 199      46.770 126.920  96.710  0.00  0.00       C  
ATOM      6 OY   TYR A 199      47.370 127.930  96.560  0.00  0.00       O  
ATOM      7 N    TYR A 199      46.430 126.380  97.890  1.00  0.00       N  
ATOM      8 HN   TYR A 199      45.820 125.600  98.010  0.00  0.00       H  
ATOM      9 CA   TYR A 199      46.650 127.140  99.120  1.00  0.00       C  
ATOM     10 HA   TYR A 199      46.560 128.210  98.970  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_4/16pkA02_frame_289.pdb ---

===== FILE: ./outputs/frames/413/replica_0/16pkA02_frame_311.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199    -143.890 108.270  46.380  0.00  0.00       C  
ATOM      2 HY1  TYR A 199    -143.260 108.060  45.480  0.00  0.00       H  
ATOM      3 HY2  TYR A 199    -144.410 109.250  46.360  0.00  0.00       H  
ATOM      4 HY3  TYR A 199    -144.610 107.430  46.460  0.00  0.00       H  
ATOM      5 CY   TYR A 199    -143.060 108.020  47.640  0.00  0.00       C  
ATOM      6 OY   TYR A 199    -143.190 108.740  48.620  0.00  0.00       O  
ATOM      7 N    TYR A 199    -142.090 107.070  47.530  1.00  0.00       N  
ATOM      8 HN   TYR A 199    -142.100 106.420  46.780  0.00  0.00       H  
ATOM      9 CA   TYR A 199    -141.180 106.950  48.700  1.00  0.00       C  
ATOM     10 HA   TYR A 199    -141.680 107.480  49.500  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_0/16pkA02_frame_311.pdb ---

===== FILE: ./outputs/frames/413/replica_0/1a02F00_frame_120.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      -0.940  71.530  19.520  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      -1.750  71.760  20.230  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      -1.340  71.530  18.480  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      -0.240  72.390  19.590  0.00  0.00       H  
ATOM      5 CY   ARG A 140      -0.140  70.340  19.850  0.00  0.00       C  
ATOM      6 OY   ARG A 140      -0.340  69.720  20.870  0.00  0.00       O  
ATOM      7 N    ARG A 140       0.950  70.230  19.110  1.00  0.00       N  
ATOM      8 HN   ARG A 140       1.190  70.940  18.450  0.00  0.00       H  
ATOM      9 CA   ARG A 140       2.000  69.230  19.400  1.00  0.00       C  
ATOM     10 HA   ARG A 140       1.690  68.380  19.980  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_0/1a02F00_frame_120.pdb ---

===== FILE: ./outputs/frames/413/replica_1/1a02F00_frame_225.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     125.970  52.200  78.890  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     126.160  52.920  79.720  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     125.340  52.700  78.120  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     126.850  51.800  78.340  0.00  0.00       H  
ATOM      5 CY   ARG A 140     125.390  51.040  79.470  0.00  0.00       C  
ATOM      6 OY   ARG A 140     125.830  49.920  79.150  0.00  0.00       O  
ATOM      7 N    ARG A 140     124.470  51.290  80.400  1.00  0.00       N  
ATOM      8 HN   ARG A 140     124.370  52.260  80.620  0.00  0.00       H  
ATOM      9 CA   ARG A 140     123.790  50.250  81.230  1.00  0.00       C  
ATOM     10 HA   ARG A 140     124.590  49.520  81.280  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_1/1a02F00_frame_225.pdb ---

===== FILE: ./outputs/frames/413/replica_1/16pkA02_frame_381.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     164.060 108.590  -1.500  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     163.960 107.480  -1.420  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     164.140 108.730  -2.590  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     163.070 109.010  -1.190  0.00  0.00       H  
ATOM      5 CY   TYR A 199     165.190 109.210  -0.780  0.00  0.00       C  
ATOM      6 OY   TYR A 199     165.070 110.290  -0.170  0.00  0.00       O  
ATOM      7 N    TYR A 199     166.400 108.600  -0.980  1.00  0.00       N  
ATOM      8 HN   TYR A 199     166.310 107.740  -1.490  0.00  0.00       H  
ATOM      9 CA   TYR A 199     167.610 108.910  -0.430  1.00  0.00       C  
ATOM     10 HA   TYR A 199     167.420 109.140   0.610  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_1/16pkA02_frame_381.pdb ---

===== FILE: ./outputs/frames/413/replica_3/16pkA02_frame_297.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      55.050  49.520  88.000  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      54.230  50.060  87.470  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      56.050  49.980  87.900  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      54.780  49.440  89.070  0.00  0.00       H  
ATOM      5 CY   TYR A 199      54.950  48.150  87.470  0.00  0.00       C  
ATOM      6 OY   TYR A 199      54.160  47.390  87.930  0.00  0.00       O  
ATOM      7 N    TYR A 199      55.850  47.680  86.610  1.00  0.00       N  
ATOM      8 HN   TYR A 199      56.700  48.130  86.340  0.00  0.00       H  
ATOM      9 CA   TYR A 199      56.040  46.430  85.960  1.00  0.00       C  
ATOM     10 HA   TYR A 199      55.340  46.250  85.150  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_3/16pkA02_frame_297.pdb ---

===== FILE: ./outputs/frames/413/replica_3/1a02F00_frame_205.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     101.530 -10.240 120.630  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     101.230 -10.140 121.690  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     100.650 -10.170 119.950  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     102.230  -9.390 120.500  0.00  0.00       H  
ATOM      5 CY   ARG A 140     102.380 -11.460 120.410  0.00  0.00       C  
ATOM      6 OY   ARG A 140     102.160 -12.520 121.070  0.00  0.00       O  
ATOM      7 N    ARG A 140     103.210 -11.400 119.440  1.00  0.00       N  
ATOM      8 HN   ARG A 140     103.390 -10.600 118.870  0.00  0.00       H  
ATOM      9 CA   ARG A 140     103.940 -12.550 119.110  1.00  0.00       C  
ATOM     10 HA   ARG A 140     103.300 -13.400 118.900  1.00  0.00       H  

--- End of Preview: ./outputs/frames/413/replica_3/1a02F00_frame_205.pdb ---

===== FILE: ./outputs/frames/320/replica_2/1a02F00_frame_374.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     286.830 -50.280 214.800  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     287.270 -49.840 213.880  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     287.510 -50.110 215.660  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     285.930 -49.740 215.160  0.00  0.00       H  
ATOM      5 CY   ARG A 140     286.530 -51.700 214.580  0.00  0.00       C  
ATOM      6 OY   ARG A 140     285.400 -52.140 214.420  0.00  0.00       O  
ATOM      7 N    ARG A 140     287.590 -52.540 214.380  1.00  0.00       N  
ATOM      8 HN   ARG A 140     288.500 -52.120 214.450  0.00  0.00       H  
ATOM      9 CA   ARG A 140     287.540 -53.880 214.090  1.00  0.00       C  
ATOM     10 HA   ARG A 140     286.530 -54.250 214.150  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_2/1a02F00_frame_374.pdb ---

===== FILE: ./outputs/frames/320/replica_2/16pkA02_frame_120.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      24.400 -27.830  -6.620  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      25.190 -27.240  -6.100  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      24.170 -27.270  -7.550  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      24.930 -28.770  -6.880  0.00  0.00       H  
ATOM      5 CY   TYR A 199      23.230 -28.080  -5.830  0.00  0.00       C  
ATOM      6 OY   TYR A 199      22.070 -28.160  -6.230  0.00  0.00       O  
ATOM      7 N    TYR A 199      23.530 -28.200  -4.570  1.00  0.00       N  
ATOM      8 HN   TYR A 199      24.370 -27.940  -4.100  0.00  0.00       H  
ATOM      9 CA   TYR A 199      22.500 -28.420  -3.510  1.00  0.00       C  
ATOM     10 HA   TYR A 199      21.600 -28.960  -3.800  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_2/16pkA02_frame_120.pdb ---

===== FILE: ./outputs/frames/320/replica_4/16pkA02_frame_344.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      18.070 -23.130-100.900  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      17.260 -22.480-100.530  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      18.170 -24.100-100.360  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      18.010 -23.350-101.990  0.00  0.00       H  
ATOM      5 CY   TYR A 199      19.410 -22.450-100.770  0.00  0.00       C  
ATOM      6 OY   TYR A 199      20.060 -22.240-101.770  0.00  0.00       O  
ATOM      7 N    TYR A 199      19.830 -22.170 -99.510  1.00  0.00       N  
ATOM      8 HN   TYR A 199      19.150 -22.410 -98.820  0.00  0.00       H  
ATOM      9 CA   TYR A 199      21.040 -21.550 -99.240  1.00  0.00       C  
ATOM     10 HA   TYR A 199      21.650 -21.640-100.130  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_4/16pkA02_frame_344.pdb ---

===== FILE: ./outputs/frames/320/replica_4/1a02F00_frame_114.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      81.240 -99.430  87.220  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      81.200-100.530  87.400  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      81.060 -98.990  88.230  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      82.220 -99.020  86.900  0.00  0.00       H  
ATOM      5 CY   ARG A 140      80.200 -98.970  86.330  0.00  0.00       C  
ATOM      6 OY   ARG A 140      79.590 -99.800  85.620  0.00  0.00       O  
ATOM      7 N    ARG A 140      80.030 -97.650  86.200  1.00  0.00       N  
ATOM      8 HN   ARG A 140      80.510 -96.980  86.770  0.00  0.00       H  
ATOM      9 CA   ARG A 140      79.080 -97.190  85.310  1.00  0.00       C  
ATOM     10 HA   ARG A 140      78.150 -97.730  85.360  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_4/1a02F00_frame_114.pdb ---

===== FILE: ./outputs/frames/320/replica_0/1a02F00_frame_279.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     121.290   2.230 121.940  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     121.750   3.180 122.280  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     122.140   1.590 121.620  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     120.660   1.780 122.740  0.00  0.00       H  
ATOM      5 CY   ARG A 140     120.480   2.430 120.710  0.00  0.00       C  
ATOM      6 OY   ARG A 140     120.800   1.830 119.680  0.00  0.00       O  
ATOM      7 N    ARG A 140     119.430   3.280 120.840  1.00  0.00       N  
ATOM      8 HN   ARG A 140     119.020   3.580 121.700  0.00  0.00       H  
ATOM      9 CA   ARG A 140     118.590   3.440 119.620  1.00  0.00       C  
ATOM     10 HA   ARG A 140     118.740   2.570 118.990  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_0/1a02F00_frame_279.pdb ---

===== FILE: ./outputs/frames/320/replica_0/16pkA02_frame_360.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     -33.460  -7.260 -53.370  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     -33.440  -6.570 -52.500  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     -32.970  -8.200 -53.050  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     -34.490  -7.630 -53.560  0.00  0.00       H  
ATOM      5 CY   TYR A 199     -32.970  -6.490 -54.530  0.00  0.00       C  
ATOM      6 OY   TYR A 199     -32.560  -5.320 -54.450  0.00  0.00       O  
ATOM      7 N    TYR A 199     -33.040  -7.160 -55.740  1.00  0.00       N  
ATOM      8 HN   TYR A 199     -33.460  -8.060 -55.680  0.00  0.00       H  
ATOM      9 CA   TYR A 199     -32.440  -6.840 -57.010  1.00  0.00       C  
ATOM     10 HA   TYR A 199     -31.470  -6.400 -56.840  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_0/16pkA02_frame_360.pdb ---

===== FILE: ./outputs/frames/320/replica_1/16pkA02_frame_216.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199     135.970 -47.220  25.520  0.00  0.00       C  
ATOM      2 HY1  TYR A 199     135.260 -46.400  25.780  0.00  0.00       H  
ATOM      3 HY2  TYR A 199     136.760 -47.360  26.290  0.00  0.00       H  
ATOM      4 HY3  TYR A 199     135.280 -48.090  25.480  0.00  0.00       H  
ATOM      5 CY   TYR A 199     136.740 -47.020  24.210  0.00  0.00       C  
ATOM      6 OY   TYR A 199     137.970 -46.930  24.190  0.00  0.00       O  
ATOM      7 N    TYR A 199     136.080 -46.750  23.060  1.00  0.00       N  
ATOM      8 HN   TYR A 199     135.090 -46.790  23.140  0.00  0.00       H  
ATOM      9 CA   TYR A 199     136.680 -46.350  21.850  1.00  0.00       C  
ATOM     10 HA   TYR A 199     137.640 -46.780  21.580  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_1/16pkA02_frame_216.pdb ---

===== FILE: ./outputs/frames/320/replica_1/1a02F00_frame_229.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140     -91.780 160.330  72.500  0.00  0.00       C  
ATOM      2 HY1  ARG A 140     -92.430 159.760  73.200  0.00  0.00       H  
ATOM      3 HY2  ARG A 140     -92.160 160.240  71.460  0.00  0.00       H  
ATOM      4 HY3  ARG A 140     -91.690 161.390  72.810  0.00  0.00       H  
ATOM      5 CY   ARG A 140     -90.410 159.710  72.590  0.00  0.00       C  
ATOM      6 OY   ARG A 140     -90.270 158.660  73.190  0.00  0.00       O  
ATOM      7 N    ARG A 140     -89.460 160.390  71.960  1.00  0.00       N  
ATOM      8 HN   ARG A 140     -89.730 161.260  71.570  0.00  0.00       H  
ATOM      9 CA   ARG A 140     -88.100 159.900  71.620  1.00  0.00       C  
ATOM     10 HA   ARG A 140     -87.540 159.500  72.460  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_1/1a02F00_frame_229.pdb ---

===== FILE: ./outputs/frames/320/replica_3/16pkA02_frame_272.pdb (First 10 lines) =====
ATOM      1 CAY  TYR A 199      60.410  54.630  35.110  0.00  0.00       C  
ATOM      2 HY1  TYR A 199      59.390  54.600  34.690  0.00  0.00       H  
ATOM      3 HY2  TYR A 199      60.340  55.450  35.860  0.00  0.00       H  
ATOM      4 HY3  TYR A 199      60.610  53.650  35.580  0.00  0.00       H  
ATOM      5 CY   TYR A 199      61.510  54.780  34.160  0.00  0.00       C  
ATOM      6 OY   TYR A 199      61.550  54.240  33.070  0.00  0.00       O  
ATOM      7 N    TYR A 199      62.570  55.360  34.700  1.00  0.00       N  
ATOM      8 HN   TYR A 199      62.560  55.670  35.650  0.00  0.00       H  
ATOM      9 CA   TYR A 199      63.870  55.360  34.100  1.00  0.00       C  
ATOM     10 HA   TYR A 199      63.740  55.660  33.070  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_3/16pkA02_frame_272.pdb ---

===== FILE: ./outputs/frames/320/replica_3/1a02F00_frame_341.pdb (First 10 lines) =====
ATOM      1 CAY  ARG A 140      72.260  86.850   1.680  0.00  0.00       C  
ATOM      2 HY1  ARG A 140      72.560  86.380   0.720  0.00  0.00       H  
ATOM      3 HY2  ARG A 140      72.120  86.050   2.440  0.00  0.00       H  
ATOM      4 HY3  ARG A 140      72.950  87.660   2.020  0.00  0.00       H  
ATOM      5 CY   ARG A 140      70.880  87.330   1.390  0.00  0.00       C  
ATOM      6 OY   ARG A 140      69.970  87.260   2.210  0.00  0.00       O  
ATOM      7 N    ARG A 140      70.720  87.900   0.220  1.00  0.00       N  
ATOM      8 HN   ARG A 140      71.490  88.130  -0.370  0.00  0.00       H  
ATOM      9 CA   ARG A 140      69.470  88.240  -0.360  1.00  0.00       C  
ATOM     10 HA   ARG A 140      69.010  88.760   0.470  1.00  0.00       H  

--- End of Preview: ./outputs/frames/320/replica_3/1a02F00_frame_341.pdb ---


=======================================
Context Generation Complete.
=======================================

Apostreriri information: 
Skip to content
Navigation Menu
wells-wood-research
aposteriori

Type / to search
Code
Issues
5
Pull requests
2
Actions
Projects
Security
Insights
Owner avatar
aposteriori
Public
wells-wood-research/aposteriori
Go to file
t
Name		
LunaPrauDrLeucine
LunaPrau
and
DrLeucine
Update README.md
552d00f
 · 
10 months ago
.github/workflows
Fix artifact name wiht python version
2 years ago
img
Improve readme + fix authors
2 years ago
src/aposteriori
Increase version number
11 months ago
tests
Recreate accidentally deleted tests
11 months ago
.gitignore
Adds new dev dependencies.
5 years ago
Dockerfile
#86 Move requirements as dev only
2 years ago
LICENSE
Initial commit
5 years ago
MANIFEST.in
#22 Remove DNN + Add package modules
5 years ago
README.md
Update README.md
10 months ago
dev-requirements.txt
Remove hardcoded numpy version
2 years ago
make_wheels.sh
Update ampal + add python releases
2 years ago
pyproject.toml
Increase version number
11 months ago
setup.py
Increase version number
11 months ago
Repository files navigation
README
MIT license

Protein Structures Voxelisation for Deep Learning

CI

aposteriori is a library for the voxelization of protein structures for protein design. It uses conventional PDB files to create fixed discretized areas of space called "frames". The atoms belonging to the side-chain of the residues are removed so to allow a Deep Learning classifier to determine the identity of the frames based solely on the protein backbone structure.


Installation
PyPI
pip install aposteriori
Manual Install
Change directory to the aposteriori folder if you have not done so already:

git clone https://github.com/wells-wood-research/aposteriori/tree/master
cd aposteriori/
Install aposteriori

pip install .
Creating a Dataset
There are two ways to create a dataset using aposteriori: through the Python API in aposteriori.make_frame_dataset or using the command line tool make-frame-dataset that installs along side the module:

make-frame-dataset /path/to/folder
If you want to try out an example, run:

make-frame-dataset tests/testing_files/pdb_files/
Check the make-frame-dataset help page for more details on its usage:

Usage: make-frame-dataset [OPTIONS] STRUCTURE_FILE_FOLDER

  Creates a dataset of voxelized amino acid frames.

  A frame refers to a region of space around an amino acid. For every
  residue in the input structure(s), a cube of space around the region (with
  an edge length equal to `--frame_edge_length`, default 12 Å), will be
  mapped to discrete space, with a defined number of voxels per edge (equal
  to `--voxels-per-side`, default = 21).

  Basic Usage:

  `make-frame-dataset $path_to_folder_with_pdb/`

  eg. `make-frame-dataset tests/testing_files/pdb_files/`

  This command will make a tiny dataset in the current directory
  `test_dataset.hdf5`, containing all residues of the structures in the
  folder.

  Globs can be used to define the structure files to be processed. `make-
  frame-dataset pdb_files/**/*.pdb` would include all `.pdb` files in all
  subdirectories of the `pdb_files` directory.

  You can process gzipped pdb files, but the program assumes that the format
  of the file name is similar to `1mkk.pdb.gz`. If you have more complex
  requirements than this, we recommend using this library directly from
  Python rather than through this CLI.

  The hdf5 object itself is like a Python dict. The structure is simple:
  
    └─[pdb_code] Contains a number of subgroups, one for each chain.
      └─[chain_id] Contains a number of subgroups, one for each residue.
        └─[residue_id] voxels_per_side^3 array of ints, representing element number.
          └─.attrs['label'] Three-letter code for the residue.
          └─.attrs['encoded_residue'] One-hot encoding of the residue.
    └─.attrs['make_frame_dataset_ver']: str - Version used to produce the dataset.
    └─.attrs['frame_dims']: t.Tuple[int, int, int, int] - Dimentsions of the frame.
    └─.attrs['atom_encoder']: t.List[str] - Lables used for the encoding (eg, ["C", "N", "O"]).
    └─.attrs['encode_cb']: bool - Whether a Cb atom was added at the avg position of (-0.741287356, -0.53937931, -1.224287356).
    └─.attrs['atom_filter_fn']: str - Function used to filter the atoms in the frame.
    └─.attrs['residue_encoder']: t.List[str] - Ordered list of residues corresponding to the encoding used.
    └─.attrs['frame_edge_length']: float - Length of the frame in Angstroms (A)
    └─.attrs['voxels_as_gaussian']: bool - Whether the voxels are encoded as a floating point of a gaussian (True) or boolean (False)

  So hdf5['1ctf']['A']['58'] would be an array for the voxelized.

Options:
Options:
  -o, --output-folder PATH        Path to folder where output will be written.
                                  Default = `.`

  -n, --name TEXT                 Name used for the dataset file, the `.hdf5`
                                  extension does not need to be included as it
                                  will be appended. Default = `frame_dataset`

  -e, --extension TEXT            Extension of structure files to be included.
                                  Default = `.pdb`.

  --pieces-filter-file PATH       Path to a Pieces format file used to filter
                                  the dataset to specific chains inspecific
                                  files. All other PDB files included in the
                                  input will be ignored.

  --frame-edge-length FLOAT       Edge length of the cube of space around each
                                  residue that will be voxelized. Default =
                                  12.0 Angstroms.

  --voxels-per-side INTEGER       The number of voxels per side of the frame.
                                  This will give a final cube of `voxels-per-
                                  side`^3. Default = 21.

  -p, --processes INTEGER         Number of processes to be used to create the
                                  dataset. Default = 1.

  -z, --is_pdb_gzipped            If True, this flag indicates that the
                                  structure files are gzipped. Default =
                                  False.

  -r, --recursive                 If True, all files in all subfolders will be
                                  processed.

  -v, --verbose                   Sets the verbosity of the output, use `-v`
                                  for low level output or `-vv` for even more
                                  information.

  -cb, --encode_cb BOOLEAN        Encode the Cb at an average position
                                  (-0.741287356, -0.53937931, -1.224287356) in
                                  the aligned frame, even for Glycine
                                  residues. Default = True

  -ae, --atom_encoder [CNO|CNOCB|CNOCBCA]
                                  Encodes atoms in different channels,
                                  depending on atom types. Default is CNO,
                                  other options are ´CNOCB´ and `CNOCBCA` to
                                  encode the Cb or Cb and Ca in different
                                  channels respectively.  [required]

  -d, --download_file PATH        Path to csv file with PDB codes to be
                                  voxelised. The biological assembly will be
                                  used for download. PDB codes will be
                                  downloaded the /pdb/ folder.

  -g, --voxels_as_gaussian BOOLEAN
                                  Boolean - whether to encode voxels as
                                  gaussians (True) or voxels (False). The
                                  gaussian representation uses the
                                  wanderwaal's radius of each atom using the
                                  formula e^(-x^2) where x is Vx - x)^2 + (Vy
                                  - y)^2) + (Vz - z)^2)/ r^2 and  (Vx, Vy, Vz)
                                  is the position of the voxel in space. (x,
                                  y, z) is the position of the atom in space,
                                  r is the Van der Waal’s radius of the atom.
                                  They are then normalized to add up to 1.

  -b, --blacklist_csv PATH        Path to csv file with structures to be
                                  removed.

  -comp, --compression_gzip BOOLEAN
                                  Whether to comrpess the dataset with gzip
                                  compression.

  -vas, --voxelise_all_states BOOLEAN
                                  Whether to voxelise only the first state of
                                  the NMR structure (False) or all of them
                                  (True).

  -rot, --tag_rotamers BOOLEAN    Whether to tag rotamer information to the
                                  frame (True) or not (False).

  --help                          Show this message and exit.
Example 1: Create a Dataset Using Biological Units of Proteins
Ideally, if you are trying to solve the Inverse Protein Folding Problem , you should use Biological Units as they are the minimal functional part of a protein. This prevents having solvent-exposed hydrophobic residues as training data.

Download the dataset:

ftp://ftp.ebi.ac.uk/pub/databases/pdb/data/biounit/PDB (European Server) Alternative servers are available here (https://www.wwpdb.org/ftp/pdb-ftp-sites)
To read more about biological units: https://pdbj.org/help/about-aubu and https://pdb101.rcsb.org/learn/guide-to-understanding-pdb-data/biological-assemblies

Once the dataset is downloaded, you will have a directory with sub-directory containig the gzipped PDB structures (ie. your Protein Data Bank Files).

To voxelize the structures into frames, run:

make-frame-dataset /path/to/biounits/  -e .pdb1.gz 
If everything went well, you should be seeing the number of structures that will be voxelised and a list of default parameters, to which you will press "y " to proceed.

Example 2: Create a Dataset Using Biological Units of Proteins and PISCES
PISCES (Protein Sequence Culling Server) is a curated subset of protein structures. Each file contains a list of structures with parameters such as resolution, percentage identity and R-Values.

Aposteriori supports filtering with a PISCES file as such:

make-frame-dataset /path/to/biounits/  -e .pdb1.gz --pieces-filter-file
 path/to/pisces/cullpdb_pc90_res1.6_R0.25_d190114_chains8082
If everything went well, you should be seeing the number of structures that will be voxelised and a list of default parameters, to which you will press "y " to proceed.

Development
The easiest way to install a development version of aposteriori is using Conda:

Conda
Create the environment:

conda create -n aposteriori python=3.8
Activate it and clone the repository:

conda activate aposteriori
git clone https://github.com/wells-wood-research/aposteriori.git
cd aposteriori/
Install dependencies:

pip install -r dev-requirements.txt
Install aposteriori:

pip install .
Check that aposteriori works

 make-frame-dataset --help
Make sure you test your install:

pytest tests/
Pip (only)
Alternatively you can install the repository with pip:

git clone https://github.com/wells-wood-research/aposteriori.git
cd aposteriori/
pip install -r dev-requirements.txt
Install aposteriori:

pip install .
About
DNN based protein design.

Resources
 Readme
License
 MIT license
 Activity
 Custom properties
Stars
 7 stars
Watchers
 1 watching
Forks
 2 forks
Report repository
Releases
No releases published
Packages
No packages published
Contributors
5
@DrLeucine
@ChrisWellsWood
@dependabot[bot]
@sunal1996
@LunaPrau
Languages
Python
98.8%
 
Other
1.2%
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Docs
Contact
Manage cookies
Do not share my personal information
wells-wood-research/aposteriori: DNN based protein design. 


Additionally the original paper infomration: 
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 1
www.nature.com/scientificdata
mdCATH: A Large-Scale MD
Dataset for Data-Driven
Computational Biophysics
Antonio Mirarchi 1,5, ToniGiorgino 2,5 ✉ & Gianni De Fabritiis1,3,4 ✉
Recent advancements in protein structure determination are revolutionizing our understanding of
proteins. Still, a signifcant gap remains in the availability of comprehensive datasets that focus on the
dynamics of proteins, which are crucial for understanding protein function, folding, and interactions.
To address this critical gap, we introduce mdCATH, a dataset generated through an extensive set of
all-atom molecular dynamics simulations of a diverse and representative collection of protein domains.
This dataset comprises all-atom systems for 5,398 domains, modeled with a state-of-the-art classical
force feld, and simulated in fve replicates each at fve temperatures from 320K to 450K. The mdCATH
dataset records coordinates and forces every 1ns, for over 62ms of accumulated simulation time,
efectively capturing the dynamics of the various classes of domains and providing a unique resource for
proteome-wide statistical analyses of protein unfolding thermodynamics and kinetics. We outline the
dataset structure and showcase its potential through four easily reproducible case studies, highlighting
its capabilities in advancing protein science.
Background and Summary
Proteins, the building blocks of life, are central to nearly all biological processes, and understanding their structure and dynamics is crucial for advancements in felds ranging from biochemistry to pharmaceuticals. Te convergence of advanced computational methods and biophysical techniques has led to unprecedented insights into
molecular structures and functions of proteins. Molecular dynamics (MD), for example, is a compute-intensive
technique that attempts to model the dynamics of biological macromolecules in realistic environments, ofen at
all-atom resolution, based on empirical force-felds whose quality has been improving over decades1–3
. Machine
learning, especially through the development of neural network potentials (NNPs), has the potential to further
enhance computational protein research by enabling more accurate predictions and simulations of behaviors4–6
.
However, the lack of comprehensive datasets capturing the dynamic behaviors of proteins remains a signifcant
challenge7
. Such datasets are vital for training machine learning models that can predict protein folding, functions, and interactions — ofen dynamic and transient processes, yet critical for understanding how macromolecules work, interact, and how they might be targeted. High-quality datasets are thus pivotal in advancing our
comprehension of these complex phenomena. In recent years, eforts have been made to provide MD datasets,
especially for key targets in drug discovery. Notable databases include GPCRmd8
, a platform dedicated to the
study of G-protein-coupled receptors (GPCRs) dynamics, and SCOV2-MD9
 as well as BioExcel-CV1910, both
showcasing the power of collaborative MD databases in the context of COVID-19 research. However, these initiatives are limited by their focus on specifc proteome subsets, leaving a gap in comprehensive proteome-wide
dynamic datasets. Previous projects such as MoDEL11, Dynameomics12 and ATLAS13, and the MDDB14 and
MDRepo15 initiatives have been introduced to provide dynamics datasets encompassing a broader range of proteins, ofen in a single replica and at room temperature, but the computational cost of MD has generally limited
databases in terms of coverage breadth and timescales.
Here, we introduce mdCATH, a dataset focused on providing extensive all-atom MD-derived dynamics for
most protein domains in the CATH classifcation system16. mdCATH features simulations of 5,398 domains at
1
Computational Science Laboratory, Universitat Pompeu Fabra, Barcelona Biomedical Research Park (PRBB), Carrer
Dr. Aiguader 88, Barcelona, 08003, Spain. 2
Biophysics Institute, National Research Council (CNR-IBF), Via Celoria
26, Milan, 20133, Italy. 3Institució Catalana de Recerca i Estudis Avançats (ICREA), Passeig Lluis Companys 23,
Barcelona, 08010, Spain. 4
Acellera Labs, Doctor Trueta 183, Barcelona, 08005, Spain. 5
These authors contributed
equally: Antonio Mirarchi, Toni Giorgino. ✉e-mail: toni.giorgino@cnr.it; g.defabritiis@gmail.com
Data Descriptor
OPEN
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 2
www.nature.com/scientificdata/ www.nature.com/scientificdata
fve diferent temperatures, each in fve replicas, therefore ofering statistically relevant large-scale insights into
protein structure dynamics under a multiplicity of conditions. Tis extensive and homogeneously-collected
dataset of all-atom molecular dynamics simulations flls a critical void in the available molecular datasets by
ofering a rich, diverse, and physiologically relevant array of protein domain dynamics, enabling systematic,
proteome-wide studies into protein thermodynamics, folding, and kinetics. It is possible to exploit mdCATH for
learning data-driven (e.g. neural network-based) potentials17, also thanks to the inclusion, unique to our knowledge, of instantaneous forces derived from a state-of-the-art all-atom force feld. We hope that the mdCATH
dataset will facilitate improvements in the design and refnement of biomolecular force felds.
Dataset Requirements
Our goal is to take a step forward in creating a proteome-wide molecular dynamics dataset for advancing drug
discovery and enabling researchers to explore the dynamic behaviors of diverse protein targets. We built the
mdCATH dataset to meet the following design features:
•	 Comprehensive coverage of structural features. mdCATH provides molecular dynamics information across
5,398 protein domains from the CATH classifcation system. Tis extensive coverage ensures a broad representation of the proteome, making the dataset valuable for a wide range of research applications in drug
discovery.
•	 MD-derived coordinates and forces. Te dataset includes both coordinates and forces from simulated trajectories. Te presence of forces is a unique feature in this dataset, which enables training force-based machine
learning potentials.
•	 Wide conformational space sampling. mdCATH features multiple replicas at diferent temperatures, capturing
a variety of conformations, including higher energy states encountered in molecular dynamics simulations.
Tis ensures that the potential functions trained on this dataset produce accurate results across all relevant
conformations.
•	 High quality data. To ensure the highest accuracy, mdCATH utilizes state-of-the-art force felds, code, and
computational resources. Te accuracy of the dataset directly impacts the performance of models trained on
it, making the use of the most accurate level of theory practical a priority.
•	 Derived metadata. Te dataset includes pre-computed information such as root-mean-square deviation
(RMSD), root-mean-square fuctuation (RMSF), secondary structure composition, and so on.
•	 Reproducibility. Reproducibility is ensured by including the PDB and PSF fles in the dataset. Additionally, the
data is stored in the efcient HDF5 binary data format, facilitating easy access and manipulation of the dataset
for further research and model training.
Methods
We built the dataset on the basis of the domain defnitions provided by the CATH database18–20. CATH, a publicly available resource maintained by the Orengo group, provides a set of domains clustered by general architecture according to the class, architecture, topology, and homologous superfamily hierarchy16. We started from
14,433 non-homologous domains at the S20 (20%) homology level in CATH release 4.2.0. We then restricted
the selection to the subset of 13,470 domains between 50 and 500 amino acids, to focus on globular structures.
Next, we excluded all the structures whose backbone was non-contiguous, e.g. due to unresolved regions in the
original experimental structures; we also excluded sequences containing non-standard amino acids (also absent
from CATH model fles). Te inclusion criteria lef 5,883 residues for further processing.
All the domain structures have been prepared with a standard protonation protocol at pH 7 including charge
state assignments, proton placement and H-bond network optimization21. Peptide chains were capped with
Fig. 1 Exclusion criteria and the resulting number of domains at each step, starting from the 14,433 domains in
the S20 homology set of CATH release 4.2.0, and ending with 5,398 domains included in the mdCATH dataset
presented in this work.
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 3
www.nature.com/scientificdata/ www.nature.com/scientificdata
Field Size Type Unit Description
Domain ID/
chain N string Chain ID
element N string Chemical element
pdb 1 string PDB fle used for simulation
psf 1 string Topology fle used for simulation
pdbProteinAtoms 1 string PDB fle with the N reported atoms
resid N integer Residue number
resname N string Residue name
z N integer Atomic number
.numResidues 1 integer Number of residues (attribute)
320/ Group for the 320K simulations
0/ Data of the frst replica
coords F×N×3 foat Å Atom coordinates
forces F×N×3 foat kcal/mol/Å Forces
dssp F×R string DSSP secondary str. assignments
gyrationRadius F double nm Gyration radius
rmsd F foat nm Root-mean square deviation w.r.t. begin
rmsf R foat nm Cα root-mean-square fuctuation
box 3×3 foat nm Simulation unit cell
.numFrames 1 integer Number of frames for this replica (attribute)
1/ Second replica
…
348/
…
Table 1. Hierarchical organization of the data felds in the mdCATH dataset, with units and description. Te
following groups and felds are provided in an HDF5 fle for each simulated CATH domain. Key: N, number of
atoms; R, number of residues; F, trajectory length in frames (1 frame corresponds to 1ns of simulated time).
Fig. 2 (a) Distribution of the number of atoms per domain, revealing a variation of nearly an order of
magnitude in the atom counts across systems. (b) Distribution of the total number of residues per domain,
showing a broad peak of around 100 residues and a long tail of up to 500 residues (cut-of size). (c) Distribution
of trajectory lengths, peaking at 500ns. (d) Distribution of root mean square deviation (RMSD) of the protein’s
heavy atoms between the frst and the last frame of each trajectory.
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 4
www.nature.com/scientificdata/ www.nature.com/scientificdata
acetylated and N-methylated termini. Te systems were solvated in cubic boxes of TIP3P water with at least
9Å of padding on each side, neutralized, and ionized with Na+ and Cl− ions at 0.150M concentration. Systems
whose resulting solvation cubic box was larger than (100Å)3
 were discarded. Te fnal dataset includes 5,398
accepted domains, as illustrated in Fig. 1. HTMD version 1.16 was used for all the building steps22,23.
All systems were parameterized with the CHARMM22* forcefeld1
. Long-range electrostatic forces were
treated with the particle-mesh Ewald (PME) summation24, with an integration timestep of 4 fs enabled by the
hydrogen mass repartitioning scheme of 4 amu per H atom25. Te simulations were performed with ACEMD26
on GPUGRID.net distributed network27.
Each system thus obtained was subjected to a pre-equilibration phase for 20ns with a time-step of 4 fs in the
NPT ensemble at 1 atm and 300K utilizing the Montecarlo barostat. Harmonic restraints were applied to the
protein’s carbon α atoms (1.0 kcal/mol/Å) and heavy atoms (0.1 kcal/mol/Å) to maintain them close to their
initial positions during the frst half (10ns) of equilibration. Te second half of equilibration (10ns to 20ns) was
performed without restraints. No restraints were used during the subsequent production phase.
Te fnal confguration of each system was used as a starting point for 25 production simulations, spawning
runs at fve temperatures in geometric progression (320K, 348K, 379K, 413K, 450K), each in fve replicas. Te
production simulations were performed in the NVT ensemble using Langevin thermostat for integration and a
0.1 ps−1
 relaxation time. Te use of the constant-volume ensemble sidesteps issues with the poor reproduction
of the water phase and pressure by TIP3P28,29. Bonds involving hydrogen atoms were constrained at the equilibrium length with the M-shake algorithm30 with a tolerance of 10−5
. Atom positions and forces acting on each
atom were recorded every 1ns and made available as part of the dataset as described below. A sampling rate of
Domains 5,398
Trajectories 134,950
Total sampled time 62.6ms
Total atoms 11,671,592
Total amino acids 740,813
Avg. traj. length 464ns
Avg. system size 2,162 atoms
Avg. domain length 137 AAs
Total fle size 3.3 TB
Table 2. Descriptive statistics of the mdCATH dataset.
Fig. 3 Relation between the radius of gyration and the number of residues in α or β secondary structure
elements for six of the mdCATH domains simulated. Each point represents a frame, taken between 0 and 500 ns
(blue to yellow) at 1 ns intervals, from the frst replica of a run at 320 K.
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 5
www.nature.com/scientificdata/ www.nature.com/scientificdata
1ns bounds the tractable kinetics, enabling the resolution of the dynamics of relatively slow degrees of freedom
such as conformational changes, but not faster motions (e.g. solvent-exposed side-chain rotations). For both
NPT and NVT simulations, a 9Å cutof was applied for PME, while van der Waals interactions used a cutof of
9Å and a switching distance of 7.5Å. Analysis of the trajectories was conducted using the HTMD library23, in
order to include potentially useful pre-computed metadata. Secondary structure assignments have been computed for each frame and residue using the implementation of the DSSP algorithm in moleculekit version 1.8.32,
encoded following the customary 8-class codes31.
Data Records
Te mdCATH dataset makes the trajectories available under a CC BY 4.0 license. It is available at HuggingFace32.
It is possible to (1) download individual domain files from HuggingFace via a browser; (2) retrieve them
via the HuggingFace dataset API (Listing 2); (3) visualize them interactively (without downloading) on the
PlayMolecule website (see the “Code Availability” section); (4) download them from PlayMolecule in XTC
format.
Organization. Te dataset is provided as a set of fles in the Hierarchical Data Format, version 5 (HDF5).
HDF5 allows the efcient storage and random access of heterogeneous data felds and arrays organized in a
flesystem-like hierarchy. For the sake of simplicity, all of the data related to a given domain were collected into
an individual HDF5 fle. Te dataset provided is structured into felds that describe snapshots of molecular simulation trajectories and derived quantities as shown in Table 1. Te root group of each fle in the dataset is the
domain ID, which aggregates felds such as chain, element, resid, resname, and z, each a vector of length
N, representing the number of protein atoms. Te pdb and psf strings hold, respectively, the verbatim PDB
fle used for the simulation (with solvent) and its topology in CHARMM/XPLOR protein structure fle (PSF)
format; pdbProteinAtoms holds a PDB of the N solute atoms used for analysis. Data on the dynamics are
organized hierarchically: fve groups at the top-most level named according to the temperature; each temperature group includes fve groups for each of the replicas; fnally, each replica holds felds for atomic coordinates,
forces, simulation box, as well as pre-computed derived quantities such as secondary structure assignments,
instantaneous gyration radius, root-mean-square deviation, and fuctuations. Coordinates and forces are stored
as three-dimensional arrays, their axes running along frames, atoms, and spatial dimensions. DSSP secondary
structure assignments are provided per residue and frame following the standard 8-letter codes.
Size. At the production cut-of date, we collected 134,950 trajectories for 5,398 domains, which were included
in the dataset. Figure 2a and 2b show the distribution of system sizes that made it to the production simulation
phase in terms of the number of solute atoms and the number of amino acids. Due to the distributed nature of
the computing network, the length of the simulations varies (independently from system size), the majority of
Fig. 4 Relation between the radius of gyration and the number of residues in α or β secondary structure
elements for domain 5sicI00 (subtilisin inhibitor-like, a 2-layer α-β sandwich of 106 amino acids) at
increasing temperatures. Destabilization is seen at 413K, and at 450K complete unfolding occurs within 100ns
(last two panels, fxed scale and full view respectively). Axes and legend are as in Fig. 3.
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 6
www.nature.com/scientificdata/ www.nature.com/scientificdata
trajectories being 500ns long (average 464ns, standard deviation 76ns; Fig. 2c). Te total simulated time is over
62ms. Te full dataset size is over 3 TB. Further aggregate statistics are reported in Table 2.
Technical Validation
We perform several statistical analyses of the dataset to validate its content.
Validation of temperature denaturation. As a frst validation of the dataset, we examined the correlation between the amount of secondary structure and the radius of gyration, which was assumed to be a proxy for
Fig. 5 Relationship between the fraction of time each residue spends in α or β secondary structure elements
and its root mean squared fuctuation (RMSF) across actin-binding protein T-fmbrin domain (5j8eA00),
cytochrome Bc1 complex domain (2a06B02), and HUP superfamily domain (2xryA01). Each point
indicates a residue, colored by its position along the sequence, from purple (N-terminal) to red (C-terminal).
Te relationship is presented in two temperature conditions, 320K (lef) and 450K (right; note the diferent
RMSF scale). At 320K, secondary structure presence exhibits a bimodal distribution, weakly correlated with
RMSF. Bimodality disappears at 450K, showing a continuum in the participation to structure elements, which is
roughly inversely correlated to the corresponding fuctuations.
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 7
www.nature.com/scientificdata/ www.nature.com/scientificdata
domain compactness. Te fraction of amino acids that are in helical or β-strand confgurations, represented by
the DSSP codes G, H, I, E, and B, is used to defne the amount of secondary structure. Tis will be referred to as
“α+β” for simplicity. Figure 3 shows the results for six domains at 320K (only one replica is shown for clarity).
Te radius of gyration and the fraction of sequence in secondary structure elements naturally depend on the
domain architecture. At 320K the domains are generally stable, and both values exhibit fuctuations around mean
values but no systematic drif nor marked correlations, with the possible exception of 1w9rA00, which undergoes a transition compacting its radius of gyration from 2.4nm to 1.8nm.
We then validated whether the relationship holds at increasing temperatures. Figure 4 shows the relation between the radius of gyration and the fraction of sequence in secondary structure elements for a specific domain, subtilisin inhibitor-like, a 2-layer α-β sandwich of 106 amino acids (CATH-Gene3D entry
G3DSA:3.30.350.10), at increasing temperatures. Between 320K and 379K, the dynamics appear essentially
unchanged, namely both quantities fuctuate randomly and uncorrelated within the 500 ns of sampled time.
Fig. 6 Distribution of protein secondary structures content–helical (top), strand (lef), and coil/turn (right)–
organized by CATH domain class and temperature. It represents data taken from the fnal snapshot of all
replicas across all domains, illustrating how the proportions of helical and strand structures shif toward coil
content as temperatures increase.
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 8
www.nature.com/scientificdata/ www.nature.com/scientificdata
Some destabilization starts to appear at 413K: the fraction of α/β structure is unchanged, while the radius of
gyration has a marked increase beyond the 1.4 nm threshold. At 450K the system unfolds: the amount of secondary structure drops below 30%, and the radius of gyration grows beyond 1.5nm within 100ns.
Fluctuation-unfolding cooperativity. We further validated the dataset by assessing the fuctuation of
residues in relation to secondary structure and temperatures. Figure 5 displays, for each residue, the fraction of
time spent in an α or β secondary structure element compared to the root mean squared fuctuation (RMSF) of
the same residue. Te structure-fuctuation relationships are shown for three domains taken as examples, namely
5j8eA00 (actin-binding protein, T-fmbrin, domain 1; mainly α), 2a06B02 (cytochrome Bc1 complex, chain
A, domain 1; α-β), and 2xryA01 (HUP superfamily, 6-strand sheet Rossmann fold), in rows, each shown at
low (320K, lef column) and high temperature (450K, right column). A clear inverse relationship between local
structure and fuctuation emerges which supports that the dataset is well constructed.
Class-wise thermodynamics of denaturation. It is possible to combine the annotations and metadata
provided by the CATH database to cross-reference dynamic data with protein classifcation. For example, we
can leverage CATH metadata by conditioning the analysis on the top-most classifcation level of CATH (Class),
defned in terms of the general architectural organization of the domain: mainly α, mainly β, α-β, few secondary
structures, and special.
Figure 6 illustrates the construction of probability distribution for various domains, conditioned using domain
class annotations. Tis fgure uses ternary plots to show the distribution of protein secondary structures — helical
(top), strand (lef), and coil/turn (right) content — on a plane. Tese plots are based on data from the last snapshot of all replicas across all domains, categorized by temperature and domain type. Te plots clearly show a shif
in the fractions of helical and strand structures toward coil content at temperatures of 413K and 450K. Notably,
the strand content shows greater resistance to thermal denaturation compared to the helical content.
Kinetics of secondary structure loss. As a last example, we show how it is possible to combine the annotations and metadata provided by the CATH database to extract proteome-wide kinetic data. Supplementary
Figure S1 analyzes the conservation of α/β structure in time as a function of temperature for the four classes
(mdCATH has no representative of the “special” class). Each panel reports time on the horizontal axis and the
fraction of residues in secondary structure elements, normalized so the initial value is one, on the vertical axis.
Values for 50 domains per class and replicas are aggregated and displayed as distributions. Diferent cooperativity
regimes emerge for the four classes (Kolmogorov-Smirnov tests for all distribution pairs at 400ns: p≪10−6
).
Fig. 7 Example of an mdCATH trajectory loaded in the PlayMolecule platform.
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 9
www.nature.com/scientificdata/ www.nature.com/scientificdata
Mainly β domains appear to be the most stable, losing structure only at 450K. Mainly α domains exhibit a partial
loss of structure at 413K; interestingly, at 450K their transition to a low-secondary structure state is, on average, abrupt (∼100 ns). Mixed α-β domains have an intermediate behaviour showing aspects of both. Lastly, as
expected, the few secondary structures class is pretty much difuse and heterogeneous.
Usage Notes
An ad-hoc class, torch_geometric.data.Dataset, has been integrated into TorchMD-Net33 to streamline the use of the mdCATH dataset, providing precise control over the protein domain selection and advanced
fltering options for trajectories. Listing 1 shows a self-contained code demonstrating how to use the mdCATH
data loader in TorchMD-Net for model training, highlighting how additional dataset arguments can be used to
focus on specifc cases of interest. Future dataset releases will include additional simulations at 300 K to expand
coverage around room-temperature conditions.
Listing 1. Importing mdCATH as a training set in TorchMD-NET.
Listing 2. Example of how to download an mdCATH HDF5 fle using the HuggingFace API.
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 1 0
www.nature.com/scientificdata/ www.nature.com/scientificdata
Code availability
Companion code to load the HDF5 fles in VMD34 for interactive inspection and analysis, to import them in
HTMD molecular analysis library23, and to convert them to standard molecular fle formats (PDB and XTC)
is provided at https://github.com/compsciencelab/mdCATH. In addition to HuggingFace, the full dataset is
also available in the PlayMolecule.org interactive viewer at https://open.playmolecule.org/mdcath, both for
visualization and for further processing via the PlayMolecule platform21,35 (Fig. 7). All the scripts used to generate
and analyze the mdCATH dataset are also available at https://github.com/compsciencelab/mdCATH.
Received: 23 July 2024; Accepted: 15 November 2024;
Published: xx xx xxxx
References
1. Piana, S., Lindorff-Larsen, K. & Shaw, D. E. How Robust Are Protein Folding Simulations with Respect to Force Field
Parameterization? Biophysical Journal 100, L47–L49, https://doi.org/10.1016/j.bpj.2011.03.051 (2011).
2. MacKerell, A. D. et al. All-Atom Empirical Potential for Molecular Modeling and Dynamics Studies of Proteins. Te Journal of
Physical Chemistry B 102, 3586–3616, https://doi.org/10.1021/jp973084f (1998).
3. Piana, S., Robustelli, P., Tan, D., Chen, S. & Shaw, D. E. Development of a Force Field for the Simulation of Single-Chain Proteins and
Protein–Protein Complexes. Journal of Chemical Teory and Computation 16, 2494–2507, https://doi.org/10.1021/acs.jctc.9b00251
(2020).
4. Anand, N. & Achim, T. Protein structure and sequence generation with equivariant denoising difusion probabilistic models. arXiv
preprint arXiv:2205.15019 (2022).
5. Mosalaganti, S. et al. Ai-based structure prediction empowers integrative structural analysis of human nuclear pores. Science 376,
eabm9506 (2022).
6. Isert, C., Atz, K. & Schneider, G. Structure-based drug design with geometric deep learning. Current Opinion in Structural Biology
79, 102548 (2023).
7. Vander Meersche, Y., Cretin, G., de Brevern, A. G., Gelly, J.-C. & Galochkina, T. Medusa: prediction of protein fexibility from
sequence. Journal of molecular biology 433, 166882 (2021).
8. Rodrguez-Espigares, I. et al. Gpcrmd uncovers the dynamics of the 3d-gpcrome. Nature Methods 17, 777–787 (2020).
9. Torrens-Fontanals, M. et al. SCoV2-MD: a database for the dynamics of the SARS-CoV-2 proteome and variant impact predictions.
Nucleic Acids Research 50, D858–D866, https://doi.org/10.1093/nar/gkab977 (2022).
10. Beltrán, D., Hospital, A., Gelp, J. L. & Orozco, M. A new paradigm for molecular dynamics databases: the covid-19 database, the
legacy of a titanic community efort. Nucleic Acids Research 52, D393–D403 (2024).
11. Meyer, T. et al. MoDEL (Molecular Dynamics Extended Library): a database of atomistic molecular dynamics trajectories. Structure
(London, England: 1993) 18, 1399–1409, https://doi.org/10.1016/j.str.2010.07.013 (2010).
12. van der Kamp, M. W. et al. Dynameomics: A comprehensive database of protein dynamics. Structure 18, 423–435, https://doi.
org/10.1016/j.str.2010.01.012 (2010).
13. Vander Meersche, Y., Cretin, G., Gheeraert, A., Gelly, J.-C. & Galochkina, T. Atlas: protein fexibility description from atomistic
molecular dynamics simulations. Nucleic Acids Research 52, D384–D392 (2024).
14. Amaro, R. et al. Te need to implement fair principles in biomolecular simulations (2024).
15. Roy, A. et al. Mdrepo – an open environment for data warehousing and knowledge discovery from molecular dynamics simulations.
bioRxiv https://doi.org/10.1101/2024.07.11.602903 (2024).
16. Sillitoe, I. et al. CATH: increased structural coverage of functional space. Nucleic Acids Research 49, D266–D273, https://doi.
org/10.1093/nar/gkaa1079 (2021).
17. Mirarchi, A., Peláez, R. P., Simeon, G. & De Fabritiis, G. AMARO: All heavy-atom transferable neural network potentials of protein
thermodynamics. J. Chem. Theory Comput. https://doi.org/10.1021/acs.jctc.4c01239. Preprint available at https://arxiv.org/
abs/2409.17852 (2024).
18. Sillitoe, I. et al. CATH: expanding the horizons of structure-based functional annotations for genome sequences. Nucleic acids
research 47, D280–D284 (2019).
19. Pearl, F. M. et al. Te CATH database: an extended protein family resource for structural and functional genomics. Nucleic acids
research 31, 452–455 (2003).
20. Orengo, C. A. et al. CATH–a hierarchic classifcation of protein domain structures. Structure 5, 1093–1109 (1997).
21. Martínez-Rosell, G., Giorgino, T. & De Fabritiis, G. PlayMolecule ProteinPrepare: A Web Application for Protein Preparation for
Molecular Dynamics Simulations. Journal of Chemical Information and Modeling 57, 1511–1516, https://doi.org/10.1021/acs.
jcim.7b00190 (2017).
22. Doerr, S., Giorgino, T., Martínez-Rosell, G., Damas, J. M. & De Fabritiis, G. High-Throughput Automated Preparation and
Simulation of Membrane Proteins with HTMD. Journal of Chemical Theory and Computation 13, 4003–4011, https://doi.
org/10.1021/acs.jctc.7b00480 (2017).
23. Doerr, S., Harvey, M. J., Noé, F. & De Fabritiis, G. HTMD: High-Troughput Molecular Dynamics for Molecular Discovery. Journal
of Chemical Teory and Computation 12, 1845–1852, https://doi.org/10.1021/acs.jctc.6b00049 (2016).
24. Darden, T., York, D. & Pedersen, L. Particle mesh Ewald: An N log(N) method for Ewald sums in large systems. Te Journal of
Chemical Physics 98, 10089–10092, https://doi.org/10.1063/1.464397 (1993).
25. Feenstra, K. A., Hess, B. & Berendsen, H. J. C. Improving efciency of large time-scale molecular dynamics simulations of hydrogenrich systems. Journal of Computational Chemistry 20, 786–798, https://doi.org/10.1002/(SICI)1096-987X(199906)20:8<786::AIDJCC5>3.0.CO;2-B (1999).
26. Harvey, M. J., Giupponi, G. & Fabritiis, G. D. Acemd: accelerating biomolecular dynamics in the microsecond time scale. Journal of
chemical theory and computation 5, 1632–1639 (2009).
27. Buch, I., Harvey, M. J., Giorgino, T., Anderson, D. P. & De Fabritiis, G. High-throughput all-atom molecular dynamics simulations
using distributed computing. Journal of Chemical Information and Modeling 50, 397–403, https://doi.org/10.1021/ci900455r (2010).
28. Quoika, P. K. & Zacharias, M. Liquid–Vapor Coexistence and Spontaneous Evaporation at Atmospheric Pressure of Common Rigid
Tree-Point Water Models in Molecular Simulations. Te Journal of Physical Chemistry B 128, 2457–2468, https://doi.org/10.1021/
acs.jpcb.3c08183 (2024).
29. Vega, C., Abascal, J. L. F., Conde, M. M. & Aragones, J. L. What ice can teach us about water interactions: a critical comparison of the
performance of diferent water models. Faraday Discussions 141, 251–276, https://doi.org/10.1039/B805531A (2008).
30. Kräutler, V. & van Gunsteren, W. F. & Hünenberger, P. H. A fast SHAKE algorithm to solve distance constraint equations for small
molecules in molecular dynamics simulations. Journal of Computational Chemistry 22, 501–508, https://doi.org/10.1002/1096-
987X(20010415)22:5<501::AID-JCC1021>3.0.CO;2-V (2001).
31. Kabsch, W. & Sander, C. Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded and geometrical
features. Biopolymers 22, 2577–2637, https://doi.org/10.1002/bip.360221211 (1983).
32. Mirarchi, A., Giorgino, T. & Fabritiis, G. D. mdCATH (Revision 2393a6d) https://doi.org/10.57967/hf/3201 (2024).
Scientific Data | (2024) 11:1299 | https://doi.org/10.1038/s41597-024-04140-z 1 1
www.nature.com/scientificdata/ www.nature.com/scientificdata
33. Pelaez, R. P. et al. Torchmd-net 2.0: Fast neural network potentials for molecular simulations. Journal of Chemical Teory and
Computation, (2024).
34. Humphrey, W., Dalke, A. & Schulten, K. VMD: Visual molecular dynamics. Journal of Molecular Graphics 14, 33–38, https://doi.
org/10.1016/0263-7855(96)00018-5 (1996).
35. Torrens-Fontanals, M., Tourlas, P., Doerr, S. & De Fabritiis, G. PlayMolecule Viewer: A Toolkit for the Visualization of Molecules
and Other Data. Journal of Chemical Information and Modeling 64, 584–589, https://doi.org/10.1021/acs.jcim.3c01776 (2024).
Acknowledgements
AM is fnancially supported by Generalitat de Catalunya’s Agency for Management of University and Research
Grants (AGAUR) PhD grant FI-1-00278 and PID2020-116564GB-I00 has been funded by MCIN / AEI /
https://doi.org/10.13039/501100011033. TG acknowledges fnancial support from the Spoke 7 of the National
Centre for HPC, Big Data and Quantum Computing (Centro Nazionale 01 – CN0000013), funded by the
European Union–NextGenerationEU, Mission 4, Component 2, Investment line 1.4, CUP B93C22000620006;
from the PRIN 2022 (BioCat4BioPol) from the Ministero dell’Università e Ricerca, funded by the European
Union–NextGenerationEU, Mission 4 Component C2, CUP B53D23015140006; and from the project InvAtInvecchiamento Attivo e in Salute (FOE 2022) CUP B53C22010140001. We thank the volunteers of GPUGRID.net
for donating computing time for the simulations. Research reported in this publication was partially supported
by the National Institute of General Medical Sciences (NIGMS) of the National Institutes of Health under award
number R01GM140090. Te content is solely the responsibility of the authors and does not necessarily represent
the ofcial views of the National Institutes of Health.
Author contributions
G.D.F.: design and project lead. T.G.: generation of the MD data. AM: conversion of MD trajectories into HDF5
datasets. A.M., T.G. and G.D.F.: data analysis and writing-up of the manuscript.
Competing interests
Te authors declare no competing interests.
Additional information
Supplementary information Te online version contains supplementary material available at https://doi.org/
10.1038/s41597-024-04140-z.
Correspondence and requests for materials should be addressed to T.G. or G.D.F.
Reprints and permissions information is available at www.nature.com/reprints.
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and
institutional afliations.
Open Access This article is licensed under a Creative Commons Attribution 4.0 International
License, which permits use, sharing, adaptation, distribution and reproduction in any medium or
format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. Te images or other third party material in this
article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the
material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the
copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
© Te Author(s) 2024